{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Topic Coherence Measures\n",
    "\n",
    "Topic coherence represents the overall topics’ interpretability and is used to assess the topics’ quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Topic modeling aims to explain a collection of documents as a mixture of topics. Each topic is a distribution over words, and each document is a distribution over topics. The goal of topic modeling is to find the topics and their distributions over words and documents.\n",
    "\n",
    "It is based on the assumption that:\n",
    "\n",
    "-  A text (document) is composed of several topics.\n",
    "-  A topic is composed of several words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3ec11",
   "metadata": {},
   "source": [
    "## Evaluating Topics\n",
    "\n",
    "Topic modeling algorithms rely on statistical inference to find the topics. However, the quality of the topics is not directly observable. Therefore, we need to evaluate the topics to assess their quality.\n",
    "\n",
    "Mathematically optimal topics are not necessarily interpretable. Therefore, we need to evaluate the topics based on their interpretability.\n",
    "\n",
    "For example, a topic modeling algorithm can find the following topics:\n",
    "\n",
    "-  Topic 1: `['cat', 'dog', 'home', 'pet']`\n",
    "-  Topic 2: `['super', 'brick', 'number']`\n",
    "\n",
    "The first topic is more interpretable than the second topic to humans. However, the second topic is mathematically as optimal as the first topic.\n",
    "\n",
    "When we’re looking for data understanding, the topics created are meant to be interpreted by humans. Therefore, we need to evaluate the topics based on their interpretability.\n",
    "\n",
    "Topic coherence is a measure of interpretability. It is used to evaluate the topics’ quality. It tries to represent the degree of semantic similarity between high scoring words in a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03890fc",
   "metadata": {},
   "source": [
    "## Topic Coherence\n",
    "\n",
    "Usually, when we talk about coherence, it refers to the charateristic of cooperative and consistent behavior. For example, a group of people can be coherent if they have the same opinion about something.\n",
    "\n",
    "What a topic coherence measure assesses is how well a topic is supported by a text corpus. It uses statistics and probabilities drawn from the text corpus to measure the coherence of a topic, especially focusing on the word's context.\n",
    "\n",
    "Topic coherence depends not only on the words in a topic but also on the reference corpus.\n",
    "\n",
    "![Topic Coherence](../figs/intro_nlp/topic_coherence/topic_coherence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a0cd9",
   "metadata": {},
   "source": [
    "Röder, M. et al in `Exploring the Space of Topic Coherence Measures` proposes a general structure for topic coherence measures. It consists of three components:\n",
    "\n",
    "It's a composition of different independent components, each one doing a different task, that is combined in a sequential pipeline.\n",
    "\n",
    "The topic coherence measure is a pipeline that receives the topics and the reference corpus as inputs and outputs a single value representing the overall coherence of the topics.\n",
    "\n",
    "![](../figs/intro_nlp/topic_coherence/topic_coherence_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed850a8",
   "metadata": {},
   "source": [
    "### Segmentation\n",
    "\n",
    "The segmentation module is responsible for creating pairs of word subsets that will be used to compute the coherence of a topic.\n",
    "\n",
    "Considering $W={w_1, w_2, …, w_n}$ as the top-n most important words of a topic $t$, the application of a segmentation$ $S$ results in a set of subset pairs from $W$.\n",
    "\n",
    "$$ S = \\{(W^{\\prime}, W^*), W^{\\prime}, W^* \\subseteq W\\} $$\n",
    "\n",
    "To simplify things, we can understand the segmentation as the step where we choose how we want to mix the words in a topic to evaluate them posteriorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b835048",
   "metadata": {},
   "source": [
    "For example, the segmentation S-one-one, says that we need to make word pairs of different words. If $W={w_1, w_2, …, w_n}$, then the segmentation S-one-one will result in the following pairs:\n",
    "\n",
    "$$ S = \\{(w_1, w_2), (w_1, w_3), …, (w_1, w_n), (w_2, w_3), …, (w_2, w_n), …, (w_{n-1}, w_n)\\} $$\n",
    "\n",
    "So, by using this technique, we’re saying that to compute the final coherence score, our model is interested in the relationship between each word in the topic and the other words in the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd96229",
   "metadata": {},
   "source": [
    "Another example is the segmentation S-one-all, which says that we need to make pairs of each word with all other words. Applying it to $W$ will result in the following pairs:\n",
    "\n",
    "$$ S = \\{(\\{w_1\\}, \\{w_2, w_3, …, w_n\\}), (\\{w_2\\}, \\{w_1, w_3, …, w_n\\}), …, (\\{w_n\\}, \\{w_1, w_2, …, w_{n-1}\\})\\} $$\n",
    "\n",
    "So, by using this technique, we’re saying that to compute the final coherence score, our model is interested in the relationship between each word in the topic and all the other words in the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c526efd",
   "metadata": {},
   "source": [
    "### Probability Calculation\n",
    "\n",
    "Coherence metrics use probabilities drawn from the textual corpus. The probability calculation module is responsible for calculating the probabilities of the word subsets generated by the segmentation module.\n",
    "\n",
    "For example, let's say we’re interested in two different probabilities:\n",
    "\n",
    "-  $P(w)$: the probability of a word $w$ in the corpus.\n",
    "-  $P(w_1, w_2)$: the probability of a word pair $(w_1, w_2)$ in the corpus.\n",
    "\n",
    "Different techniques will estimate these probabilities in different ways. For example, the probability $P(w)$ can be estimated by counting the number of documents that $w$ appears and dividing it by the total number of documents in the corpus.\n",
    "\n",
    "$$ P(w) = \\frac{count(w)}{N} $$\n",
    "\n",
    "The probability $P(w_1, w_2)$ can be estimated by counting the number of documents that both $w_1$ and $w_2$ appear and dividing it by the total number of documents in the corpus.\n",
    "\n",
    "$$ P(w_1, w_2) = \\frac{count(w_1, w_2)}{N} $$\n",
    "\n",
    "Another example is to use sentence-level probabilities. In this case, the probability $P(w)$ is estimated by counting the number of sentences that $w$ appears and dividing it by the total number of sentences in the corpus.\n",
    "\n",
    "These probabilities are the fundamental building blocks of the coherence metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cae3bc",
   "metadata": {},
   "source": [
    "### Confirmation Measure\n",
    "\n",
    "The confirmation measure module is the core of the coherence metrics. It is responsible for calculating the confirmation of a word subset.\n",
    "\n",
    "The confirmation measure is calculated by comparing the probability of the word subset $S$ with the probabilities of the words in the subset. It computes how well the subset $W^*$ supports the words in the subset $W^{\\prime}$.\n",
    "\n",
    "That is, it tries to measure how well two subsets of words are related to each other by comparing the probabilities calculated from the corpus.\n",
    "\n",
    "If the words in $W^{\\prime}$ are more likely to appear together with the words in $W^*$, then the confirmation measure will be high. Otherwise, it will be low.\n",
    "\n",
    "For example, let's say we have the following word subsets:\n",
    "\n",
    "-  $W^{\\prime} = \\{w_1, w_2\\}$\n",
    "-  $W^* = \\{w_3, w_4\\}$\n",
    "\n",
    "The confirmation measure will be high if the words $w_1$ and $w_2$ are more likely to appear together with the words $w_3$ and $w_4$ in the reference corpus.\n",
    "\n",
    "![](../figs/intro_nlp/topic_coherence/confirmation_measure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811a4fe",
   "metadata": {},
   "source": [
    "The confirmation measure is applied to each one of the pairs created in the segmentation step, yielding a confirmation score for each pair.\n",
    "\n",
    "There are two different types of confirmation measures: direct and indirect.\n",
    "\n",
    "#### Direct Confirmation Measure\n",
    "\n",
    "The direct confirmation measure is the simplest one. It compares the probabilities of the word subsets $W^{\\prime}$ and $W^*$.\n",
    "\n",
    "$$ CM(S) = \\frac{P(W^{\\prime}, W^*)}{P(W^{\\prime})P(W^*)} $$\n",
    "\n",
    "or, using logarithms:\n",
    "\n",
    "$$ CM(S) = log \\frac{P(W^{\\prime}, W^*) + \\epsilon}{P(W^{\\prime})P(W^*) + \\epsilon} $$\n",
    "\n",
    "where $\\epsilon$ is a small constant to avoid undefined values for logarithms.\n",
    "\n",
    "#### Indirect Confirmation Measure\n",
    "\n",
    "The indirect confirmation measure is a more complex one. It computes a direct confirmation measure for each word in the subsets $W^{\\prime}$ with all other words in the subset $W$, yielding a vector of confirmation scores for each word in $W^{\\prime}$.\n",
    "\n",
    "$$ \\vec{v}(W^{\\prime}) = \\left\\{\\sum_{w \\in W^{\\prime}} CM(w_i, w_j)\\right\\}_{j=1,2, …, |W|} $$\n",
    "\n",
    "where $|W|$ is the number of words in $W$.\n",
    "\n",
    "The same is done for the subset $W^*$.\n",
    "\n",
    "Then, the indirect confirmation measure is the similarity between the two vectors.\n",
    "\n",
    "$$ CM(S) = sim(\\vec{v}(W^{\\prime}), \\vec{v}(W^*)) $$\n",
    "\n",
    "where $sim$ is a similarity function, such as cosine similarity.\n",
    "\n",
    "![](../figs/intro_nlp/topic_coherence/indirect_confirmation_measure.png)\n",
    "\n",
    "The idea behind this indirect confirmation measure is that it tries to capture some relationships that are not captured by the direct confirmation measure. \n",
    "\n",
    "For example, the words ‘cats’ and ‘dogs’ may never appear together in our dataset, but they might appear frequently with the words ‘toys’, ‘pets’, and ‘cute’. In this case, the direct confirmation measure will not be able to capture the relationship between ‘cats’ and ‘dogs’ because they never appear together. However, the indirect confirmation measure will be able to capture this relationship because it will be able to see that ‘cats’ and ‘dogs’ appear frequently with the words ‘toys’, ‘pets’, and ‘cute’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da823cf",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "The aggregation module is responsible for aggregating the confirmation scores of the pairs generated in the previous step. It computes the final coherence score for the topic. \n",
    "\n",
    "There are different types of aggregation techniques: mean, median, geometric mean, and so on.\n",
    "\n",
    "![](../figs/intro_nlp/topic_coherence/aggregation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bfb17",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Measuring the coherence metrics follows the following steps:\n",
    "\n",
    "- We have a topic $T$ that we want to measure the coherence of.\n",
    "- We choose a reference corpus $C$.\n",
    "- The top-n most important words in the topic $T$ are extracted, yielding a word subset $W$.\n",
    "- $W$ is segmented into pairs of words, yielding a set of word subsets $S$.\n",
    "- Using the reference corpus $C$, we calculate the probabilities of the word subsets $S$.\n",
    "- With the segmented word subsets $S$ and the probabilities calculated from the reference corpus $C$, we calculate the confirmation measure for each pair of words in $S$.\n",
    "- All the confirmation scores are aggregated into a single coherence score for the topic $T$.\n",
    "\n",
    "![](../figs/intro_nlp/topic_coherence/putting_everything_together.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e071a",
   "metadata": {},
   "source": [
    "If we have more than one topic, we can repeat the same process for each topic and use the average coherence score as a measure of the quality of the topic model.\n",
    "\n",
    "![](../figs/intro_nlp/topic_coherence/multiple_topics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52458df",
   "metadata": {},
   "source": [
    "The Gensim library provides a class that implements the four most famous coherence models: u_mass, c_v, c_uci, c_npmi.\n",
    "\n",
    "![](../figs/intro_nlp/topic_coherence/gensim_coherence_models.png)\n",
    "\n",
    "C_NPMI uses the following steps:\n",
    "\n",
    "- Segmentation: S-one-one (one word in each subset)\n",
    "- Probability estimation: the probabilities are calculated over a sliding window of size 10.\n",
    "- Confirmation measure: the confirmation measure is the Normalized Pointwise Mutual Information (NPMI).\n",
    "\n",
    "$$ NPMI(W^{\\prime}, W^*) = \\frac{log \\frac{P(W^{\\prime}, W^*) + \\epsilon}{P(W^{\\prime})P(W^*) + \\epsilon}}{-log (P(W^{\\prime}, W^*) + \\epsilon)} $$\n",
    "\n",
    "where $\\epsilon$ is a small constant to avoid undefined values for logarithms.\n",
    "- Aggregation: the aggregation is the mean of the confirmation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af564400",
   "metadata": {},
   "source": [
    "C_V uses the following steps:\n",
    "\n",
    "- Segmentation: S-one-set, the confirmaion measure is calculated for pairs of words that are in the same subset.\n",
    "- Probability estimation: the probabilities are calculated over a sliding window of size 110.\n",
    "- Confirmation measure: indirect confirmation measure, the similarity function is cosine similarity.\n",
    "- Aggregation: the aggregation is the mean of the confirmation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d9cac",
   "metadata": {},
   "source": [
    "## Topic Coherence in Practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f52dde",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
