{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Word Segmentation and Association\n",
    "\n",
    "![](../figs/intro_nlp/words/entelecheia_associaltion_vs_segmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f9e1e",
   "metadata": {},
   "source": [
    "## Word Segmentation\n",
    "\n",
    "- **Word segmentation** is the task of splitting a string of characters into words.\n",
    "- Word segmentation is important for a machine to understand the meaning of a sentence.\n",
    "- In English, we can split a string of characters into words by spaces.\n",
    "- However, in languages like Chinese and Janpanese, there is no space between words.  \n",
    "- Even in English, there are some cases where no space is used between words.\n",
    "- Humans can easily segment a string of characters into words, even though there is no space between words.\n",
    "- For example, we can easily segment the string of characters `Ilikechocolate` into words `I like chocolate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7cf33",
   "metadata": {},
   "source": [
    "## Why should we segment words?\n",
    "\n",
    "There are many applications that require word segmentation, even in English.\n",
    "\n",
    "- Normalizing English compound nouns that are variably written for search engines.\n",
    "  - For example, `ice cream` and `ice-cream` should be segmented into `icecream`.\n",
    "- Word segmentation for compounds: Both orginal words and split words should be in the dictionary.\n",
    "- Typing errors may be corrected by word segmentation.\n",
    "- Conversion errors: During conversion, some spaces may be lost.\n",
    "- OCR errors: OCRed text may contain errors.\n",
    "- Keyword extraction from URL addresses, domain names, table column description or programming variables that are written without spaces.\n",
    "- For password analysis, the extraction of terms from passwords can be required.\n",
    "- Automatic CamelCasing of programming variables.\n",
    "- Speech recognition: Speech recognition systems may not properly recognize spaces between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de54549",
   "metadata": {},
   "source": [
    "## Generating segment variants\n",
    "\n",
    "We can generate all possible segment variants of a string of characters. Each distinct segment variant is called a **composition**.\n",
    "\n",
    "- En a string of length $n$, there are $n-1$ possible positions to split the string.\n",
    "- Each of the $n-1$ positions can be used as word boundary.\n",
    "- Therefore, there are $2^{n-1}$ possible compositions.\n",
    "\n",
    "The compositions have to be evaluated to find the best segmentation.\n",
    "\n",
    "- The best segmentation is the one that has the highest probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf09dec",
   "metadata": {},
   "source": [
    "### Naive Recursive Algorithm\n",
    "\n",
    "- The naive recursive algorithm is to generate all possible compositions and evaluate them.\n",
    "- The time complexity of the naive recursive algorithm is $O(2^n)$.\n",
    "- The naive recursive algorithm is not efficient for long strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d279eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def segment_naive(string):\n",
    "    if not string:\n",
    "        return []\n",
    "    else:\n",
    "        return [[string]] + [\n",
    "            [string[:i]] + rest\n",
    "            for i in range(1, len(string))\n",
    "            for rest in segment_naive(string[i:])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdac8abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['isit'],\n",
      " ['i', 'sit'],\n",
      " ['i', 's', 'it'],\n",
      " ['i', 's', 'i', 't'],\n",
      " ['i', 'si', 't'],\n",
      " ['is', 'it'],\n",
      " ['is', 'i', 't'],\n",
      " ['isi', 't']]\n"
     ]
    }
   ],
   "source": [
    "pprint(segment_naive(\"isit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401975f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['가방에'], ['가', '방에'], ['가', '방', '에'], ['가방', '에']]\n"
     ]
    }
   ],
   "source": [
    "pprint(segment_naive(\"가방에\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a93ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 8192\n"
     ]
    }
   ],
   "source": [
    "text = \"thisislongtext\"\n",
    "print(len(text), len(segment_naive(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a93a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1024\n"
     ]
    }
   ],
   "source": [
    "text = \"아버지가방에들어가신다\" # Father goes into the bag or Father enters the room\n",
    "print(len(text), len(segment_naive(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7523592",
   "metadata": {},
   "source": [
    "### Dynamic Programming\n",
    "\n",
    "- Dynamic programming is a technique to solve a problem by breaking it into subproblems and storing the results of subproblems to avoid computing the same results again.\n",
    "- The time complexity of dynamic programming is $O(n)$.\n",
    "- For long strings, dynamic programming is much more efficient than the naive recursive algorithm.\n",
    "\n",
    "```python\n",
    "def segment(string, dictionary):\n",
    "    if not string:\n",
    "        return []\n",
    "    for end in range(1, len(string) + 1):\n",
    "        first, rest = string[:end], string[end:]\n",
    "        if first in dictionary:\n",
    "            return [first] + segment(rest, dictionary)\n",
    "    return [string]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17af89",
   "metadata": {},
   "source": [
    "### Triangular Matrix\n",
    "\n",
    "- The dynamic programming algorithm can be implemented using a triangular matrix.\n",
    "- The tryangular matrix algorithm uses nested loops and a circular buffer to store the results of subproblems.\n",
    "- A triangular matrix of parts with increasing length is generated and organized in a circular buffer.\n",
    "- This allows a constant amount of memory to be used for the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d2d7f",
   "metadata": {},
   "source": [
    "### Maximum Matching Algorithm\n",
    "\n",
    "- If we have all known words in a dictionary, we can use the maximum matching algorithm to segment a sentence.\n",
    "- The maximum matching algorithm is a greedy algorithm that finds the longest matching word from the dictionary.\n",
    "- The algorithm is as follows:\n",
    "\n",
    "    1. Find the longest matching word from the dictionary.\n",
    "    2. If the word is found, add the word to the result and remove the word from the input.\n",
    "    3. If the word is not found, add the first character to the result and remove the first character from the input.\n",
    "    4. Repeat 1-3 until the input is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8845f",
   "metadata": {},
   "source": [
    "### Unknown Words\n",
    "\n",
    "- We can not rely on the dictionary to segment all words.\n",
    "- There are uncommon words, new words, misspelled words, foreign words, proper nouns, slang words, etc.\n",
    "- Even in these cases, we want to segment the words into meaningful parts.\n",
    "- Therefore, we have to estimate the probability of any possible segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1763b6",
   "metadata": {},
   "source": [
    "## Evaluation of Compositions\n",
    "\n",
    "- Generally, we can evaluate a composition by calculating the probability of the composition.\n",
    "- Word probabilities can be estimated from a corpus:\n",
    "\n",
    "    $$\n",
    "    P(w_i) = \\frac{c(w_i)}{N}\n",
    "    $$\n",
    "\n",
    "    where $c(w_i)$ is the count of word $w_i$ and $N$ is the total number of words in the corpus.\n",
    "\n",
    "- However, for unkonwn words, we have to use other criteria to evaluate the composition.\n",
    "- At word boundary, the uncertainty of the segmentation increases.\n",
    "- By measuring the uncertainty, we can evaluate the composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "### Uncertainty of word boundaries\n",
    "\n",
    "- The uncertainty of word boundaries can be measured by the entropy of the word boundary.\n",
    "- {cite}`harris1970phoneme` said that if the uncertainty of successive tokens increases, the location is a word boundary.\n",
    "- {cite}`feng2004accessor` proposed a statistical criterion called accessor variety (AV) to measure how likely a sub-sequence is a word, and then to find the best segmentation pattern that maximizes a target function of accessor variety and the length of the sub-sequence as variants. \n",
    "- {cite}`jin2006unsupervised` proposed branching entropy as another criterion for unsupervised segmentation.\n",
    "- Both criteria share a similar assumption as in the fundamental work by Harris, 1970, that the uncertainty of successive tokens increases at word boundaries.\n",
    "- The latter is the countinous version of the former.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321cbbe",
   "metadata": {},
   "source": [
    "![](../figs/intro_nlp/words/branching_entropy_uncertainty.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2f2dc",
   "metadata": {},
   "source": [
    "\n",
    "### Accessor Variety\n",
    "\n",
    "- The accessor variety (AV) defines that the uncertainty of a sub-sequence is the number of different words that can be formed by adding a sub-sequence to the sub-sequence.\n",
    "- For the forward accessor variety, it is the number of different words that can be formed by adding a sub-sequence to the right side of the sub-sequence.\n",
    "- For the following sub-sequence, the forward accessor variety of `hope` is 2, because `hope` can be followed by `less` or `fully`.\n",
    "\n",
    "    ```\n",
    "    \"hopeful\": 100\n",
    "    \"hopeless\": 80\n",
    "    ```\n",
    "- The backward accessor variety is the number of different words that can be formed by adding a sub-sequence to the left side of the sub-sequence.\n",
    "- For example, the backward accessor variety of `less` is 3, because `hopeless`, `useless`, and `pointless` can be formed by adding `less` to the left side of `less`.\n",
    "\n",
    "    ```\n",
    "    \"hopeless\": 80\n",
    "    \"unless\": 160\n",
    "    \"pointless\": 70\n",
    "    ```\n",
    "- Depending on the language, the forward accessor variety or the backward accessor variety may be more suitable for segmentation.\n",
    "- Threshold values can be used to determine the word boundaries.\n",
    "- The threshold values can be determined by the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8e83c",
   "metadata": {},
   "source": [
    "### Branching Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38328484",
   "metadata": {},
   "source": [
    "**Assumption 1**: The uncertainty of successive tokens increases at word boundaries.\n",
    "\n",
    "- Given a set of elements $X$ and a set of n-gram sequences $X_n$, the conditional entropy of an element occuring after an n-gram sequence $X_n$ is defined as:\n",
    "\n",
    "    $$\n",
    "    H(X|X_n) = -\\sum_{x_n \\in X_n} P(x_n) \\sum_{x \\in X} P(x|x_n) \\log P(x|x_n)\n",
    "    $$\n",
    "\n",
    "    where $P(x) = P(X=x)$, $P(x|x_n) = P(X=x|X_n=x_n)$, and $P(X=x)$ indicates the probability of an element $x$ occuring in $X$.\n",
    "- $H(X|X_n)$ decreases as $n$ increases, meaning that $X$ will become more predictable as $X_n$ becomes longer.\n",
    "- The latter half of the equation, the entropy of $X$ given $X_n$, indicates the average information of branching out from a specific n-gram sequence $X_n$:\n",
    "\n",
    "    $$\n",
    "    H(X|X_n=x_n) = -\\sum_{x \\in X} P(x|x_n) \\log P(x|x_n)\n",
    "    $$\n",
    "\n",
    "- This local entropy is the branching entropy of $X$ given $X_n$, and denoted as $h(x_n)$:\n",
    "\n",
    "    $$\n",
    "    h(x_n) = -\\sum_{x \\in X} P(x|x_n) \\log P(x|x_n)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7fcdf",
   "metadata": {},
   "source": [
    "\n",
    "**Assumption 2**: If the entropy of successive tokens is high or increasing, the location is a word boundary.\n",
    "\n",
    "- Generally, as the length of the n-gram sequence increases, the entropy of the n-gram sequence decreases.\n",
    "\n",
    "    $$\n",
    "    h(x_n) \\geq h(x_{n-1})\n",
    "    $$\n",
    "    \n",
    "- If $x_n$ is the prefix of $x_{n+1}$, the branching entropy of $x_n$ will likely be smaller than that of $x_{n+1}$:\n",
    "\n",
    "    $$\n",
    "    h(x_n) < h(x_{n+1})\n",
    "    $$\n",
    "    \n",
    "- There are three boundary conditions to decide whether $x_n$ is a word boundary:\n",
    "\n",
    "    1. $B_{max}$: If $h(x_n) > \\text{val}_{max}$\n",
    "    2. $B_{increase}$: $h(x_n) > h(x_{n-1})$\n",
    "    3. $B_{ordinary}$: $h(x_n) > \\text{val}_{ordinary}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5c094",
   "metadata": {},
   "source": [
    "![](../figs/intro_nlp/words/branching_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778caf04",
   "metadata": {},
   "source": [
    "## Word Segmentation in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "dca51dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n",
      "INFO:ekorpkit.base:setting environment variable CACHED_PATH_CACHE_ROOT to /workspace/.cache/cached_path\n",
      "INFO:ekorpkit.base:setting environment variable KMP_DUPLICATE_LIB_OK to TRUE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus : fomc\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "eKonf.setLogger(\"WARNING\")\n",
    "\n",
    "cfg = eKonf.compose(\"corpus\")\n",
    "cfg.name = \"fomc\"\n",
    "cfg.path.cache.uri = (\n",
    "    \"https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/fomc.zip\"\n",
    ")\n",
    "cfg.data_dir = cfg.path.cached_path\n",
    "cfg.auto.merge = True\n",
    "fomc_corpus = eKonf.instantiate(cfg)\n",
    "print(fomc_corpus)\n",
    "texts = fomc_corpus.data[fomc_corpus.data.content_type == \"fomc_statement\"].text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "37e799ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chairman Alan Greenspan announced today that the Federal Open Market Committee decided to increase slightly the degree of pressure on reserve positions. The action is expected to be associated with a small increase in short-term money market interest rates.\\n\\nThe decision was taken to move toward a less accommodative stance in monetary policy in order to sustain and enhance the economic expansion.\\n\\nChairman Greenspan decided to announce this action immediately so as to avoid any misunderstanding of the Committee's purposes, given the fact that this is the first firming of reserve market conditions by the Committee since early 1989.\""
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c6c3b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"like apples\",\n",
    "    \"likes pineapples apples\",\n",
    "    \"dislikes Apple pineapple\",\n",
    "    \"People dislike pineapples.\",\n",
    "    \"An apple makes people healthy.\",\n",
    "    \"Pine trees make pineapple\",\n",
    "    \"Pineapple unhealthy\",\n",
    "    \"likeness of apples\",\n",
    "    \"dislikeness of pineapples\",\n",
    "    \"health of pineapple\",\n",
    "    \"healthyness of apple\",\n",
    "    \"display of pineapples\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "99832f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def normalize_word(word):\n",
    "    # replace all non-alphanumeric characters at the end of the word with a space\n",
    "    word = re.sub(r\"[^a-zA-Z0-9]+$\", \" \", word)\n",
    "    # replace all non-alphanumeric characters at the beginning of the word with a space\n",
    "    word = re.sub(r\"^[^a-zA-Z0-9]+\", \" \", word)\n",
    "    return word.strip()\n",
    "\n",
    "\n",
    "def pre_tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    # remove urls\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return [\n",
    "        normalize_word(word) for word in text.split() if len(normalize_word(word)) > 0\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "ca4f1e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 subwords: [('in', 19653), ('th', 17728), ('on', 17147), ('an', 15225), ('er', 13980), ('at', 13068), ('re', 12853), ('ti', 12777), ('he', 12611), ('the', 11984)]\n",
      "Top 10 forward subwords: [('thef', 2483), ('thec', 2166), ('theco', 1916), ('thefe', 1775), ('oft', 1310), ('monetaryp', 1267), ('monetarypo', 1267), ('ofth', 1227), ('int', 1134), ('fort', 1035)]\n",
      "Top 10 backward subwords: [('sand', 2283), ('sof', 1808), ('efederal', 1781), ('hefederal', 1781), ('nthe', 1653), ('ecommittee', 1626), ('hecommittee', 1626), ('rthe', 1436), ('ethe', 1306), ('ypolicy', 1267)]\n",
      "23821 14651 13723\n"
     ]
    }
   ],
   "source": [
    "whitespace_token = \"▁\"\n",
    "empty_token = \"\"\n",
    "max_fwd_len = 2\n",
    "max_bwd_len = 2\n",
    "character_freqs = collections.defaultdict(int)\n",
    "subwords_freqs = collections.defaultdict(int)\n",
    "word_freqs = collections.defaultdict(int)\n",
    "forward_subwords_freqs = collections.defaultdict(int)\n",
    "backward_subwords_freqs = collections.defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    words = pre_tokenize(text)\n",
    "    for left_word, word, right_word in zip(\n",
    "        [empty_token] + words[:-1], words, words[1:] + [empty_token]\n",
    "    ):\n",
    "        word_freqs[word] += 1\n",
    "        for i in range(len(word)):\n",
    "            character_freqs[word[i]] += 1\n",
    "            # Loop through the subwords of length at least 2\n",
    "            for j in range(i + 2, len(word) + 1):\n",
    "                subwords_freqs[word[i:j]] += 1\n",
    "\n",
    "        for j in range(min(len(word), max_fwd_len)):\n",
    "            # forward_subword = word + whitespace_token + right_word[: j + 1]\n",
    "            forward_subword = word + right_word[: j + 1]\n",
    "            forward_subwords_freqs[forward_subword] += 1\n",
    "        for j in range(min(len(word), max_bwd_len)):\n",
    "            # backward_subword = left_word[-(j + 1) :] + whitespace_token + word\n",
    "            backward_subword = left_word[-(j + 1) :] + word\n",
    "            backward_subwords_freqs[backward_subword] += 1\n",
    "\n",
    "# Sort subwords by frequency\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 subwords: {}\".format(sorted_subwords[:10]))\n",
    "# sort forward subwords by frequency\n",
    "sorted_forward_subwords = sorted(\n",
    "    forward_subwords_freqs.items(), key=lambda x: x[1], reverse=True\n",
    ")\n",
    "print(\"Top 10 forward subwords: {}\".format(sorted_forward_subwords[:10]))\n",
    "# sort backward subwords by frequency\n",
    "sorted_backward_subwords = sorted(\n",
    "    backward_subwords_freqs.items(), key=lambda x: x[1], reverse=True\n",
    ")\n",
    "print(\"Top 10 backward subwords: {}\".format(sorted_backward_subwords[:10]))\n",
    "# subwords_freqs\n",
    "# forward_subwords_freqs\n",
    "# backward_subwords_freqs\n",
    "print(len(subwords_freqs), len(sorted_forward_subwords), len(sorted_backward_subwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "216a7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_tokens = list(character_freqs.items()) + sorted_subwords + sorted_backward_subwords\n",
    "bwd_tokens = {token: freq for token, freq in bwd_tokens}\n",
    "bwd_tokens = collections.Counter(bwd_tokens)\n",
    "\n",
    "fwd_tokens = list(character_freqs.items()) + sorted_subwords + sorted_forward_subwords\n",
    "fwd_tokens = {token: freq for token, freq in fwd_tokens}\n",
    "fwd_tokens = collections.Counter(fwd_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "f6167bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trie:\n",
    "    def __init__(self, end_symbol=\"<END>\", direction=\"forward\"):\n",
    "        self.root = {}\n",
    "        self.end_symbol = end_symbol\n",
    "        self.direction = direction\n",
    "\n",
    "    def add(self, word, value):\n",
    "        if self.direction == \"backward\":\n",
    "            # reverse the word\n",
    "            word = word[::-1]\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            if ch not in node:\n",
    "                node[ch] = {}\n",
    "            node = node[ch]\n",
    "        node[self.end_symbol] = value\n",
    "\n",
    "    def get_value(self, word):\n",
    "        if self.direction == \"backward\":\n",
    "            # reverse the word\n",
    "            word = word[::-1]\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            if ch not in node:\n",
    "                return 0\n",
    "            node = node[ch]\n",
    "        if self.end_symbol not in node:\n",
    "            return 0\n",
    "        return node[self.end_symbol]\n",
    "\n",
    "    def set_value(self, word, value):\n",
    "        if self.direction == \"backward\":\n",
    "            # reverse the word\n",
    "            word = word[::-1]\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            if ch not in node:\n",
    "                raise ValueError(\"word not in trie\")\n",
    "            node = node[ch]\n",
    "        if self.end_symbol not in node:\n",
    "            raise ValueError(\"word not in trie\")\n",
    "        node[self.end_symbol] = value\n",
    "\n",
    "    def get_children(self, word):\n",
    "        if self.direction == \"backward\":\n",
    "            # reverse the word\n",
    "            word = word[::-1]\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            if ch not in node:\n",
    "                return []\n",
    "            node = node[ch]\n",
    "        children = node.copy()\n",
    "        return children\n",
    "\n",
    "    def get_values_of_children(self, word):\n",
    "        children = self.get_children(word)\n",
    "        values = []\n",
    "        for child in children:\n",
    "            if child == self.end_symbol:\n",
    "                continue\n",
    "            else:\n",
    "                if self.end_symbol in children[child]:\n",
    "                    values.append(children[child][self.end_symbol])\n",
    "        return values\n",
    "    \n",
    "    def num_children(self, word):\n",
    "        return len(self.get_children(word))\n",
    "\n",
    "    def get_leafs(self, word):\n",
    "        node = self.get_children(word)\n",
    "        _ = node.pop(self.end_symbol, None)\n",
    "        return self._get_leafs(node)\n",
    "    \n",
    "    def _get_leafs(self, node):\n",
    "        if self.end_symbol in node:\n",
    "            return [node[self.end_symbol]]\n",
    "        leafs = []\n",
    "        for child in node:\n",
    "            leafs += self._get_leafs(node[child])\n",
    "        return leafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "adff0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_trie(tokens, direction=\"forward\"):\n",
    "    trie = Trie(direction=direction)\n",
    "\n",
    "    maxlen = 0\n",
    "    for tok, val in tokens.items():\n",
    "        trie.add(tok, val)\n",
    "        maxlen = max(maxlen, len(tok))\n",
    "\n",
    "    return trie, maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "3d7e6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_trie, fwd_maxlen = initialize_trie(fwd_tokens, direction=\"forward\")\n",
    "bwd_trie, bwd_maxlen = initialize_trie(bwd_tokens, direction=\"backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "d19a9452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<END>': 1439,\n",
      " 'a': {'<END>': 144,\n",
      "       'c': {'<END>': 11},\n",
      "       'n': {'<END>': 131},\n",
      "       'u': {'<END>': 2}},\n",
      " 'b': {'<END>': 1, 'a': {'<END>': 1}},\n",
      " 'c': {'<END>': 9, 'o': {'<END>': 9}},\n",
      " 'e': {'<END>': 10, 'a': {'<END>': 10}},\n",
      " 'p': {'<END>': 1267, 'o': {'<END>': 1267}},\n",
      " 's': {'<END>': 7, 't': {'<END>': 7}},\n",
      " 't': {'<END>': 1, 'i': {'<END>': 1}}}\n"
     ]
    }
   ],
   "source": [
    "# pprint(fwd_trie.get_children(\"pineapple\"))\n",
    "pprint(fwd_trie.get_children(\"monetary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "32c465e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 1.5279808395403343\n",
      "ra 1.2169536856123169\n",
      "rat 0.4515554246117919\n",
      "rate 1.255306316910514\n",
      "rates 1.4232282798426306\n",
      "s 1.2349366144473954\n",
      "es 1.4550346892773653\n",
      "tes 0.7783313449194234\n",
      "ates 0.9638677718791596\n",
      "rates 0.8214645612592879\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from scipy.special import digamma\n",
    "\n",
    "def entropy(trie, word):\n",
    "    leafs = trie.get_leafs(word)\n",
    "    val = trie.get_value(word)\n",
    "    logsum = digamma(sum(leafs) + val)\n",
    "    entropy = 0\n",
    "    for freq in leafs:\n",
    "        logprob = digamma(freq) - logsum\n",
    "        entropy += math.exp(logprob) * logprob\n",
    "    return -1 * entropy\n",
    "\n",
    "word = \"rates\"\n",
    "# word = \"pineapples\"\n",
    "# word = \"people\"\n",
    "# word = \"unhealthy\"\n",
    "# word = \"healthyness\"\n",
    "# word = \"dislikeness\"\n",
    "# word = \"likes\"s\n",
    "for i in range(1, len(word)+1):\n",
    "    print(word[:i], entropy(fwd_trie, word[:i]))\n",
    "\n",
    "for i in range(1, len(word)+1):\n",
    "    print(word[-i:], entropy(bwd_trie, word[-i:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f8bc3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Uncertanty to word boundary; Accessor Variety & Branching Entropy](https://lovit.github.io/nlp/2018/04/09/branching_entropy_accessor_variety/)\n",
    "- [Fast Word Segmentation of Noisy Text](https://medium.com/towards-data-science/fast-word-segmentation-for-noisy-text-2c2c41f9e8da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9265f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
