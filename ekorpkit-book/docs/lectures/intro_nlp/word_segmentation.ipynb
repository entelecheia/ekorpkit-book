{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Word Segmentation and Association\n",
    "\n",
    "![](../figs/intro_nlp/words/entelecheia_associaltion_vs_segmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f9e1e",
   "metadata": {},
   "source": [
    "## Word Segmentation\n",
    "\n",
    "- **Word segmentation** is the task of splitting a string of characters into words.\n",
    "- Word segmentation is important for a machine to understand the meaning of a sentence.\n",
    "- In English, we can split a string of characters into words by spaces.\n",
    "- However, in languages like Chinese and Janpanese, there is no space between words.  \n",
    "- Even in English, there are some cases where no space is used between words.\n",
    "- Humans can easily segment a string of characters into words, even though there is no space between words.\n",
    "- For example, we can easily segment the string of characters `Ilikechocolate` into words `I like chocolate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7cf33",
   "metadata": {},
   "source": [
    "## Why should we segment words?\n",
    "\n",
    "There are many applications that require word segmentation, even in English.\n",
    "\n",
    "- Normalizing English compound nouns that are variably written for search engines.\n",
    "  - For example, `ice cream` and `ice-cream` should be segmented into `icecream`.\n",
    "- Word segmentation for compounds: Both orginal words and split words should be in the dictionary.\n",
    "- Typing errors may be corrected by word segmentation.\n",
    "- Conversion errors: During conversion, some spaces may be lost.\n",
    "- OCR errors: OCRed text may contain errors.\n",
    "- Keyword extraction from URL addresses, domain names, table column description or programming variables that are written without spaces.\n",
    "- For password analysis, the extraction of terms from passwords can be required.\n",
    "- Automatic CamelCasing of programming variables.\n",
    "- Speech recognition: Speech recognition systems may not properly recognize spaces between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de54549",
   "metadata": {},
   "source": [
    "## Generating segment variants\n",
    "\n",
    "We can generate all possible segment variants of a string of characters. Each distinct segment variant is called a **composition**.\n",
    "\n",
    "- En a string of length $n$, there are $n-1$ possible positions to split the string.\n",
    "- Each of the $n-1$ positions can be used as word boundary.\n",
    "- Therefore, there are $2^{n-1}$ possible compositions.\n",
    "\n",
    "The compositions have to be evaluated to find the best segmentation.\n",
    "\n",
    "- The best segmentation is the one that has the highest probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf09dec",
   "metadata": {},
   "source": [
    "### Naive Recursive Algorithm\n",
    "\n",
    "- The naive recursive algorithm is to generate all possible compositions and evaluate them.\n",
    "- The time complexity of the naive recursive algorithm is $O(2^n)$.\n",
    "- The naive recursive algorithm is not efficient for long strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d279eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def segment_naive(string):\n",
    "    if not string:\n",
    "        return []\n",
    "    else:\n",
    "        return [[string]] + [\n",
    "            [string[:i]] + rest\n",
    "            for i in range(1, len(string))\n",
    "            for rest in segment_naive(string[i:])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdac8abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['isit'],\n",
      " ['i', 'sit'],\n",
      " ['i', 's', 'it'],\n",
      " ['i', 's', 'i', 't'],\n",
      " ['i', 'si', 't'],\n",
      " ['is', 'it'],\n",
      " ['is', 'i', 't'],\n",
      " ['isi', 't']]\n"
     ]
    }
   ],
   "source": [
    "pprint(segment_naive(\"isit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401975f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['가방에'], ['가', '방에'], ['가', '방', '에'], ['가방', '에']]\n"
     ]
    }
   ],
   "source": [
    "pprint(segment_naive(\"가방에\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a93ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 8192\n"
     ]
    }
   ],
   "source": [
    "text = \"thisislongtext\"\n",
    "print(len(text), len(segment_naive(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a93a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1024\n"
     ]
    }
   ],
   "source": [
    "text = \"아버지가방에들어가신다\" # Father goes into the bag or Father enters the room\n",
    "print(len(text), len(segment_naive(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7523592",
   "metadata": {},
   "source": [
    "### Dynamic Programming\n",
    "\n",
    "- Dynamic programming is a technique to solve a problem by breaking it into subproblems and storing the results of subproblems to avoid computing the same results again.\n",
    "- The time complexity of dynamic programming is $O(n)$.\n",
    "- For long strings, dynamic programming is much more efficient than the naive recursive algorithm.\n",
    "\n",
    "```python\n",
    "def segment(string, dictionary):\n",
    "    if not string:\n",
    "        return []\n",
    "    for end in range(1, len(string) + 1):\n",
    "        first, rest = string[:end], string[end:]\n",
    "        if first in dictionary:\n",
    "            return [first] + segment(rest, dictionary)\n",
    "    return [string]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17af89",
   "metadata": {},
   "source": [
    "### Triangular Matrix\n",
    "\n",
    "- The dynamic programming algorithm can be implemented using a triangular matrix.\n",
    "- The tryangular matrix algorithm uses nested loops and a circular buffer to store the results of subproblems.\n",
    "- A triangular matrix of parts with increasing length is generated and organized in a circular buffer.\n",
    "- This allows a constant amount of memory to be used for the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8845f",
   "metadata": {},
   "source": [
    "### Unknown Words\n",
    "\n",
    "- We can not rely on the dictionary to segment all words.\n",
    "- There are uncommon words, new words, misspelled words, foreign words, proper nouns, slang words, etc.\n",
    "- Even in these cases, we want to segment the words into meaningful parts.\n",
    "- Therefore, we have to estimate the probability of any possible segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1763b6",
   "metadata": {},
   "source": [
    "## Evaluation of Compositions\n",
    "\n",
    "- Generally, we can evaluate a composition by calculating the probability of the composition.\n",
    "- Word probabilities can be estimated from a corpus:\n",
    "\n",
    "    $$\n",
    "    P(w_i) = \\frac{c(w_i)}{N}\n",
    "    $$\n",
    "\n",
    "    where $c(w_i)$ is the count of word $w_i$ and $N$ is the total number of words in the corpus.\n",
    "\n",
    "- However, for unkonwn words, we have to use other criteria to evaluate the composition.\n",
    "- At word boundary, the uncertainty of the segmentation increases.\n",
    "- By measuring the uncertainty, we can evaluate the composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "### Uncertainty of word boundaries\n",
    "\n",
    "- The uncertainty of word boundaries can be measured by the entropy of the word boundary.\n",
    "- Harris, 1970 said that if the uncertainty of successive tokens increases, the location is a word boundary.\n",
    "- Feng et al., 2004 proposed a statistical criterion called accessor variety (AV) to measure how likely a sub-sequence is a word, and then to find the best segmentation pattern that maximizes a target function of accessor variety and the length of the sub-sequence as variants. \n",
    "- Jin and TanakaIshii, 2006 proposed branch entropy as another criterion for unsupervised segmentation.\n",
    "- Both criteria share a similar assumption as in the fundamental work by Harris, 1970, that the uncertainty of successive tokens increases at word boundaries.\n",
    "- The latter is the countinous version of the former.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321cbbe",
   "metadata": {},
   "source": [
    "![](../figs/intro_nlp/words/branching_entropy_uncertainty.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2f2dc",
   "metadata": {},
   "source": [
    "\n",
    "### Accessor Variety\n",
    "\n",
    "- The accessor variety (AV) defines that the uncertainty of a sub-sequence is the number of different words that can be formed by adding a sub-sequence to the sub-sequence.\n",
    "- For the right-side accessor variety, it is the number of different words that can be formed by adding a sub-sequence to the right side of the sub-sequence.\n",
    "- For the following sub-sequence, the right-side accessor variety of `hope` is 2, because `hope` can be followed by `less` or `fully`.\n",
    "\n",
    "    ```\n",
    "    \"hopeful\": 100\n",
    "    \"hopeless\": 80\n",
    "    ```\n",
    "- The left-side accessor variety is the number of different words that can be formed by adding a sub-sequence to the left side of the sub-sequence.\n",
    "- For example, the left-side accessor variety of `less` is 3, because `hopeless`, `useless`, and `pointless` can be formed by adding `less` to the left side of `less`.\n",
    "\n",
    "    ```\n",
    "    \"hopeless\": 80\n",
    "    \"unless\": 160\n",
    "    \"pointless\": 70\n",
    "    ```\n",
    "- Depending on the language, the left-side accessor variety or the right-side accessor variety may be more suitable for segmentation.\n",
    "- Threshold values can be used to determine the word boundaries.\n",
    "- The threshold values can be determined by the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8e83c",
   "metadata": {},
   "source": [
    "### Branch Entropy\n",
    "\n",
    "- The branch entropy is defined as the entropy of the distribution of the number of words that can be formed by adding a single character to the end of a sub-sequence.\n",
    "\n",
    "    $$\n",
    "    \\text{BE}(w|c) = -\\sum_{i=1}^n p_i \\log p_i\n",
    "    $$\n",
    "\n",
    "    where $p_i$ is the probability of the number of words that can be formed by adding a single character to the end of a sub-sequence $w$ and $c$ is the character.\n",
    "\n",
    "- As in the case of accessor variety, the branch entropy can be calculated for the left-side and the right-side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f8bc3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Uncertanty to word boundary; Accessor Variety & Branching Entropy](https://lovit.github.io/nlp/2018/04/09/branching_entropy_accessor_variety/)\n",
    "- [Fast Word Segmentation of Noisy Text](https://medium.com/towards-data-science/fast-word-segmentation-for-noisy-text-2c2c41f9e8da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9265f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
