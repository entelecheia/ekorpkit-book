
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>T5: Text-To-Text Transfer Transformer &#8212; eKorpkit Tutorials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/ekorpkit.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="AI Art (Text-to-Image)" href="../aiart/index.html" />
    <link rel="prev" title="BERT: Bidirectional Encoder Representations from Transformers" href="bert.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FN4GFT8HP8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FN4GFT8HP8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/ekorpkit.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">eKorpkit Tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    eKonomic Research Python Toolkit
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/index.html">
   Getting started
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/install.html">
     Installation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/usage.html">
     Usage
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/features/index.html">
   Key features
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/easy_config.html">
     Easy Configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/no_boiler.html">
     No Boilerplate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/workflows.html">
     Workflows
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/share.html">
     Sharable and Reproducible
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/plug.html">
     Pluggable Architece
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/config/index.html">
   Configuring ekorpkit
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/config/eKonf.html">
     Using eKonf class
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/datasets/index.html">
   Datasets
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/corpus.html">
     Build and Load Corpora
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/datasets/dataset.html">
     Build and Load Datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/pipeline.html">
     Corpus task pipelines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/models/index.html">
   Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/automl/index.html">
     Auto ML
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/embeddings/index.html">
     Word Embeddings
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/word2vec_basics.html">
       Word2Vec Basics
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/eval_vectors.html">
       Evaluate pretrained embeddings
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/ngram/index.html">
     N-Grams
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram.html">
       N-Grams
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_lexicons.html">
       N-Gram model for ngram lexicon features
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_words.html">
       N-Gram model for unigram lexicon features
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/sentiment/index.html">
     Sentiment Analyers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/lbsa_en.html">
       Lexicon-based Sentiment Analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/financial_phrasebank_lm.html">
       Evaluate LM with financial phrasebank
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/sentiment.html">
       LM Dictionary vs. finbert vs. T5
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/transformers/index.html">
     Transformers
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/art/index.html">
     AI Art (Text-to-Image)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/disco.html">
       Disco Diffusion
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/dalle-mini.html">
       DALL·E Mini
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/pipelines/index.html">
   Pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/pipelines/pipeline.html">
     Instantiating pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/preprocessors/index.html">
   Preprocessors
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/normalizer/index.html">
     Normalizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/normalizer/normalizer.html">
       Normalizers
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/segmenter/index.html">
     Segmenters
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/segmenter/segmenter.html">
       Segmenters
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/index.html">
     Tokenizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
    <label for="toctree-checkbox-14">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/mecab.html">
       Mecab Tokenizer
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/visualizers/index.html">
   Visualizers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/visualizers/plot.html">
     Plots
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/workflow/index.html">
   Workflows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../corpus/ekorpkot_corpus.html">
   The eKorpkit Corpus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Use Cases
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/fomc/index.html">
   FOMC
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/02_FOMC_numerical_data.html">
     Preparing Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/03_FOMC_corpus.html">
     Preparing Textual Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/04_FOMC_EDA_numericals.html">
     EDA on Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/05_FOMC_features.html">
     Visualizing Features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/06_FOMC_AutoML.html">
     Checking Baseline with AutoML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/07_FOMC_sentiments.html">
     Predicting Sentiments of FOMC Corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/08_FOMC_EDA_sentiments.html">
     EDA on Sentiment Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/09_FOMC_AutoML_with_tones.html">
     Predicting the next decisions with tones
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../usecases/bok/index.html">
   Bank of Korea
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/edgar/index.html">
   EDGAR
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/edgar/predict_edgar.html">
     Prediciting Sentiments
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg/index.html">
   ESG
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/snorkel-polarity.html">
     Preparing polarity classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix.html">
     Improving classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/classifiers.html">
     Training Classifiers for ESG Ratings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/econ_news_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       econ_news_kr
      </span>
     </code>
     corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_news.html">
     Predicting ESG Categories and Polarities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix_classifiers.html">
     Preparing classifiers for active learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_for_learning.html">
     Preparing active learning data
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg-en/index.html">
   ESG (English)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg-en/esg_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       JOCo
      </span>
     </code>
     corpus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/cointax/index.html">
   Taxation on Cryptocurrency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/cointax/coin_dataset.html">
     Improving classification datasets
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/analyst.html">
   ESG Topic Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/rethink_esg.html">
   Rethinking ESG
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../intro_nlp/index.html">
   Introduction to NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/research.html">
     Research Applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/topic.html">
     Topic Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/topic_models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/topic_coherence.html">
     Topic Coherence Measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/sentiments.html">
     Sentiment Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Deep Learning for NLP
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="zeroshot.html">
     Zero Shot, Prompt, and Search Strategies
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloom.html">
     What is BLOOM?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloom_apps.html">
     Bloom Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="transformers.html">
     Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert.html">
     BERT: Bidirectional Encoder Representations from Transformers
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     T5: Text-To-Text Transfer Transformer
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../aiart/index.html">
   AI Art (Text-to-Image)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
  <label for="toctree-checkbox-23">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/project-themes.html">
     Project Themes - A Brave New World
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle1.html">
     DALL·E 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle2.html">
     DALL·E 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/imagen.html">
     Imagen
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mlops/index.html">
   Machine Learning Systems Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ds/index.html">
   Data Science for Economics and Finance
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/index.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/cite.html">
   Citation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/entelecheia/ekorpkit-book/main?urlpath=tree/ekorpkit-book/docs/lectures/deep_nlp/t5.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/entelecheia/ekorpkit-book/blob/main/ekorpkit-book/docs/lectures/deep_nlp/t5.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book/issues/new?title=Issue%20on%20page%20%2Fdocs/lectures/deep_nlp/t5.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/docs/lectures/deep_nlp/t5.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#t5-text-to-text-framework">
   T5: Text-to-Text Framework
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unified-input-output-format">
     Unified Input &amp; Output Format
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c4-colossal-clean-crawled-corpus">
   C4: Colossal Clean Crawled Corpus
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-crawl">
     Common Crawl
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#colossal-clean-crawled-corpus">
     Colossal Clean Crawled Corpus
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-model">
   The Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#encoder-decoder-transformer-model">
     Encoder-Decoder Transformer Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#baseline">
     Baseline
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-systematic-study-of-transfer-learning-methodology">
   A Systematic Study of Transfer Learning Methodology
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#architectures">
     Architectures
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-objective">
     Unsupervised Objective
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#corruption-rates">
     Corruption Rates
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#corrupted-span-length">
     Corrupted Span Length
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unlabeled-datasets">
     Unlabeled Datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unlabeled-dataset-sizes">
     Unlabeled Dataset Sizes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fine-tuning-methods">
     Fine-Tuning Methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-task-learning">
     Multi-Task Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scaling">
     Scaling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sota-comparisons">
     SOTA Comparisons
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>T5: Text-To-Text Transfer Transformer</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#t5-text-to-text-framework">
   T5: Text-to-Text Framework
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unified-input-output-format">
     Unified Input &amp; Output Format
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c4-colossal-clean-crawled-corpus">
   C4: Colossal Clean Crawled Corpus
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-crawl">
     Common Crawl
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#colossal-clean-crawled-corpus">
     Colossal Clean Crawled Corpus
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-model">
   The Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#encoder-decoder-transformer-model">
     Encoder-Decoder Transformer Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#baseline">
     Baseline
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-systematic-study-of-transfer-learning-methodology">
   A Systematic Study of Transfer Learning Methodology
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#architectures">
     Architectures
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-objective">
     Unsupervised Objective
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#corruption-rates">
     Corruption Rates
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#corrupted-span-length">
     Corrupted Span Length
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unlabeled-datasets">
     Unlabeled Datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unlabeled-dataset-sizes">
     Unlabeled Dataset Sizes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fine-tuning-methods">
     Fine-Tuning Methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-task-learning">
     Multi-Task Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scaling">
     Scaling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sota-comparisons">
     SOTA Comparisons
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="t5-text-to-text-transfer-transformer">
<h1>T5: Text-To-Text Transfer Transformer<a class="headerlink" href="#t5-text-to-text-transfer-transformer" title="Permalink to this headline">#</a></h1>
<p><img alt="" src="../../../_images/t5.gif" /></p>
<p>“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer” <span id="id1">[<a class="reference internal" href="../../about/index.html#id16" title="Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, and others. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.">Raffel <em>et al.</em>, 2020</a>]</span></p>
<ul class="simple">
<li><p>A unified framework that converts all text-based language problems into a text-to-text format by framing them as conditional text generation tasks.</p></li>
<li><p>Combining the pre-training objectives of BERT and GPT-2, T5 is trained on a very large number of tasks and is able to perform well on a wide range of tasks with minimal task-specific architecture modifications.</p></li>
<li><p>C4 (Corpus of Cleaned Web Crawled Text) is used as the training corpus, which is a large-scale dataset of 3.3 billion web pages.</p></li>
<li><p>Achieves state-of-the-art results on 11 out of 15 tasks in GLUE, SuperGLUE, and SQuAD v1.1.</p></li>
</ul>
<section id="t5-text-to-text-framework">
<h2>T5: Text-to-Text Framework<a class="headerlink" href="#t5-text-to-text-framework" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../../../_images/t5-training.png" /></p>
<section id="unified-input-output-format">
<h3>Unified Input &amp; Output Format<a class="headerlink" href="#unified-input-output-format" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>T5 means “<code class="docutils literal notranslate"><span class="pre">T</span></code>ext-<code class="docutils literal notranslate"><span class="pre">T</span></code>o-<code class="docutils literal notranslate"><span class="pre">T</span></code>ext <code class="docutils literal notranslate"><span class="pre">T</span></code>ransfer <code class="docutils literal notranslate"><span class="pre">T</span></code>ransformer”.</p></li>
<li><p>Every task considered by T5 is framed as a conditional text generation task with a single input and output sequence.</p></li>
<li><p>Translation: <code class="docutils literal notranslate"><span class="pre">translate</span> <span class="pre">English</span> <span class="pre">to</span> <span class="pre">German:</span> <span class="pre">How</span> <span class="pre">old</span> <span class="pre">are</span> <span class="pre">you?</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">Wie</span> <span class="pre">alt</span> <span class="pre">bist</span> <span class="pre">du?</span></code></p></li>
<li><p>Text classification: <code class="docutils literal notranslate"><span class="pre">classify</span> <span class="pre">sentiment:</span> <span class="pre">This</span> <span class="pre">movie</span> <span class="pre">is</span> <span class="pre">so</span> <span class="pre">bad.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">negative</span></code></p></li>
<li><p>Text summarization: <code class="docutils literal notranslate"><span class="pre">summarize:</span> <span class="pre">The</span> <span class="pre">movie</span> <span class="pre">was</span> <span class="pre">not</span> <span class="pre">good.</span> <span class="pre">The</span> <span class="pre">animation</span> <span class="pre">and</span> <span class="pre">the</span> <span class="pre">graphics</span> <span class="pre">were</span> <span class="pre">good.</span> <span class="pre">This</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">good</span> <span class="pre">movie.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">movie</span> <span class="pre">was</span> <span class="pre">not</span> <span class="pre">good.</span></code></p></li>
<li><p>Entailment: <code class="docutils literal notranslate"><span class="pre">entailment:</span> <span class="pre">I</span> <span class="pre">like</span> <span class="pre">to</span> <span class="pre">eat</span> <span class="pre">broccoli</span> <span class="pre">and</span> <span class="pre">bananas.</span> <span class="pre">I</span> <span class="pre">eat</span> <span class="pre">a</span> <span class="pre">banana</span> <span class="pre">every</span> <span class="pre">day.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">neutral</span></code></p></li>
<li><p>MNLI (entailment): <code class="docutils literal notranslate"><span class="pre">mnli:</span> <span class="pre">Premise:</span> <span class="pre">A</span> <span class="pre">person</span> <span class="pre">on</span> <span class="pre">a</span> <span class="pre">horse</span> <span class="pre">jumps</span> <span class="pre">over</span> <span class="pre">a</span> <span class="pre">broken</span> <span class="pre">down</span> <span class="pre">airplane.</span> <span class="pre">Hypothesis:</span> <span class="pre">The</span> <span class="pre">person</span> <span class="pre">is</span> <span class="pre">training</span> <span class="pre">his</span> <span class="pre">horse</span> <span class="pre">for</span> <span class="pre">a</span> <span class="pre">competition.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">entailment</span></code></p></li>
<li><p>MNLI (neutral): <code class="docutils literal notranslate"><span class="pre">mnli:</span> <span class="pre">Premise:</span> <span class="pre">A</span> <span class="pre">person</span> <span class="pre">on</span> <span class="pre">a</span> <span class="pre">horse</span> <span class="pre">jumps</span> <span class="pre">over</span> <span class="pre">a</span> <span class="pre">broken</span> <span class="pre">down</span> <span class="pre">airplane.</span> <span class="pre">Hypothesis:</span> <span class="pre">The</span> <span class="pre">person</span> <span class="pre">is</span> <span class="pre">at</span> <span class="pre">the</span> <span class="pre">zoo,</span> <span class="pre">riding</span> <span class="pre">a</span> <span class="pre">horse.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">neutral</span></code></p></li>
<li><p>Regression: <code class="docutils literal notranslate"><span class="pre">sts-b:</span> <span class="pre">The</span> <span class="pre">cat</span> <span class="pre">was</span> <span class="pre">playing</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">garden.</span> <span class="pre">The</span> <span class="pre">cat</span> <span class="pre">was</span> <span class="pre">playing</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">yard.</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">5.0</span></code></p></li>
</ul>
</section>
</section>
<section id="c4-colossal-clean-crawled-corpus">
<h2>C4: Colossal Clean Crawled Corpus<a class="headerlink" href="#c4-colossal-clean-crawled-corpus" title="Permalink to this headline">#</a></h2>
<section id="common-crawl">
<h3><a class="reference external" href="https://commoncrawl.org/">Common Crawl</a><a class="headerlink" href="#common-crawl" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Common Crawl is a web crawl data repository that contains petabytes of data collected from the public web.</p></li>
<li><p>It is a non-profit organization that collects data from the public web and makes it freely available to the research community.</p></li>
<li><p>It produces around 30TB of data per month.</p></li>
<li><p>However, Common Crawl contains large amounts of noisy data, such as error messages, spam, and duplicate content.</p></li>
<li><p>To address this problem, the authors of T5 use the C4 dataset, which is a cleaned version of Common Crawl.</p></li>
</ul>
</section>
<section id="colossal-clean-crawled-corpus">
<h3>Colossal Clean Crawled Corpus<a class="headerlink" href="#colossal-clean-crawled-corpus" title="Permalink to this headline">#</a></h3>
<p>For C4, the authors took Common Crawl scrape from April 2019 and applied some cleansing filters on it:</p>
<ol class="simple">
<li><p>Only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark).</p></li>
<li><p>Discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words.</p></li>
<li><p>Removed any page that contained any word on the <code class="docutils literal notranslate"><span class="pre">List</span> <span class="pre">of</span> <span class="pre">Dirty,</span> <span class="pre">Naughty,</span> <span class="pre">Obscene</span> <span class="pre">or</span> <span class="pre">Otherwise</span> <span class="pre">Bad</span> <span class="pre">Words</span></code>.</p></li>
<li><p>Removed any line with the word <code class="docutils literal notranslate"><span class="pre">Javascript</span></code>.</p></li>
<li><p>Removed any page where the phrase <code class="docutils literal notranslate"><span class="pre">lorem</span> <span class="pre">ipsum</span></code> appeared.</p></li>
<li><p>Removed any pages that contained a curly bracket.</p></li>
<li><p>Deduplicate the data set, discarded all but one of any three-sentence span occurring more than once in the data set.</p></li>
<li><p>Since most of the downstream tasks are focused on English-language text, langdetect is used to filter out any pages that were not classified as English with a probability of at least 0.99.</p></li>
</ol>
<ul class="simple">
<li><p>The filtered dataset is larger than most data sets used for pre-training (about 750 GB).</p></li>
<li><p>It also comprises reasonably clean and natural English text.</p></li>
<li><p>This data set is called the “Colossal Clean Crawled Corpus” (or C4 for short) and released as part of TensorFlow Datasets.</p></li>
</ul>
</section>
</section>
<section id="the-model">
<h2>The Model<a class="headerlink" href="#the-model" title="Permalink to this headline">#</a></h2>
<section id="encoder-decoder-transformer-model">
<h3>Encoder-Decoder Transformer Model<a class="headerlink" href="#encoder-decoder-transformer-model" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>T5 uses the same encoder-decoder Transformer architecture as BERT.</p></li>
<li><p>However, a simplified layer normalization is used the activations are only rescaled and no additive bias is applied.</p></li>
<li><p>After layer normalization, a residual skip connection, originated from ResNet, adds each subcomponent’s input to its output.</p></li>
<li><p>Also, instead of using a fixed positional encoding, T5 uses a relative positional encoding.</p></li>
</ul>
</section>
<section id="baseline">
<h3>Baseline<a class="headerlink" href="#baseline" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The baseline model is designed so that the encoder and decoder are each similar in size and configuration to a BERT-base model.</p></li>
<li><p>Specifically, the encoder and decoder each have 12 layers, with about 220 million parameters.</p></li>
<li><p>The objective of the baseline model is to predict the dropped-out tokens in the input sequence.</p></li>
</ul>
<p><img alt="T5 Baseline" src="../../../_images/t5-baseline.png" /></p>
</section>
</section>
<section id="a-systematic-study-of-transfer-learning-methodology">
<h2>A Systematic Study of Transfer Learning Methodology<a class="headerlink" href="#a-systematic-study-of-transfer-learning-methodology" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../../../_images/t5-strategies.png" /></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">architectures</span></code>, where we found that encoder-decoder models generally outperformed “decoder-only” language models;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pre-training</span> <span class="pre">objectives</span></code>, where we confirmed that fill-in-the-blank-style denoising objectives (where the model is trained to recover missing words in the input) worked best and that the most important factor was the computational cost;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">unlabeled</span> <span class="pre">datasets</span></code>, where we showed that training on in-domain data can be beneficial but that pre-training on smaller datasets can lead to detrimental overfitting;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">training</span> <span class="pre">strategies</span></code>, where we found that multitask learning could be close to competitive with a pre-train-then-fine-tune approach but requires carefully choosing how often the model is trained on each task;</p></li>
<li><p>and <code class="docutils literal notranslate"><span class="pre">scale</span></code>, where we compare scaling up the model size, the training time, and the number of ensembled models to determine how to make the best use of fixed compute power.</p></li>
</ul>
<section id="architectures">
<h3>Architectures<a class="headerlink" href="#architectures" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../../../_images/t5-structure.png" /></p>
<p>At an architectural level, there are several options in selecting the training approach:</p>
<ol class="simple">
<li><p>Encoder-Decoder:</p>
<ul class="simple">
<li><p>This is the standard encoder-decoder Transformer architecture.</p></li>
<li><p>Encoder is trained a BERT-style, fully visible language modeling objective. (i.e. every token contributes to the attention calculation of every other token)</p></li>
<li><p>Decoder is trained in a GPT-style, causal language modeling objective. (i.e. every token contributes to the attention calculation of every token that appears before it)</p></li>
</ul>
</li>
<li><p>Language Model: This is essentially the causal attention language modeling objective.</p></li>
<li><p>Prefix Language Model: This is a combination of the BERT-style and language model approaches. For example, the taks of translating from English to German is framed as a prefix language model task, where the input <code class="docutils literal notranslate"><span class="pre">translate</span> <span class="pre">English</span> <span class="pre">to</span> <span class="pre">German:</span></code> attended in BERT-style, and the output <code class="docutils literal notranslate"><span class="pre">Wie</span> <span class="pre">alt</span> <span class="pre">bist</span> <span class="pre">du?</span></code> is attended autoregressively.</p></li>
</ol>
<p><img alt="" src="../../../_images/t5-mask-patterns.png" /></p>
<p><img alt="" src="../../../_images/t5-architectures.png" /></p>
</section>
<section id="unsupervised-objective">
<h3>Unsupervised Objective<a class="headerlink" href="#unsupervised-objective" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../../../_images/t5-unsupervised.png" /></p>
<p>With respect to the pre-training objective, authors have tested the following:</p>
<ol class="simple">
<li><p>Language Modeling: This is the standard language modeling objective, where the model is trained to predict the next token in a sequence.</p></li>
<li><p>Deshuffling: This is a variant of language modeling, where the model is trained to predict the original order of a shuffled sequence.</p></li>
<li><p>Corrupting Spans: Masking a sequence of tokens and training the model to predict these masked tokens.</p></li>
</ol>
<p><img alt="" src="../../../_images/t5-unsupervised-obj.png" /></p>
<p>The BERT-style objective performs best, though the prefix language modeling objective attains similar performance on the translation tasks.</p>
<p><img alt="" src="../../../_images/t5-unsupervised-obj-perf1.png" /></p>
<p><img alt="" src="../../../_images/t5-unsupervised-obj-perf2.png" /></p>
</section>
<section id="corruption-rates">
<h3>Corruption Rates<a class="headerlink" href="#corruption-rates" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Using a larger corruption rate results in longer targets, which can potentially slow down training.</p></li>
<li><p>Based on the results and the historical precedent set by BERT, a corruption rate of 15% is used going forward.</p></li>
</ul>
<p><img alt="" src="../../../_images/t5-corruption-rates.png" /></p>
</section>
<section id="corrupted-span-length">
<h3>Corrupted Span Length<a class="headerlink" href="#corrupted-span-length" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>When multiple consecutive tokens have been corrupted, they are treated as a “span” and a single unique mask token is used to replace the entire span.</p></li>
<li><p>A limited difference is found between these objectives, though the version with an average span length of 10 slightly underperforms the other values in some cases.</p></li>
</ul>
<p><img alt="" src="../../../_images/t5-corrupted-span-length.png" /></p>
</section>
<section id="unlabeled-datasets">
<h3>Unlabeled Datasets<a class="headerlink" href="#unlabeled-datasets" title="Permalink to this headline">#</a></h3>
<p>Different pretraining datasets are tried.</p>
<ul class="simple">
<li><p>C4: The one mentioned in Section 2 in this story article.</p></li>
<li><p>C4, unfiltered: C4 but without filtering, to know the effect of the heuristic filtering</p></li>
<li><p>RealNews-like: C4 but only include content from one of the domains used in the “RealNews” data set.</p></li>
<li><p>WebText-like: Similarly, the WebText data set only uses content from webpages.</p></li>
<li><p>Wikipedia: English Wikipedia text data from TensorFlow Datasets.</p></li>
<li><p>Wikipedia+TBC: Toronto Books Corpus (TBC) contains text extracted from eBooks, combining with Wikipedia following BERT.</p></li>
</ul>
<p>Pre-training on in-domain unlabeled data can improve performance on downstream tasks. (e.g.: unlabled news data helps downstream news dataset.) But this is unsatisfying if the goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary domains.</p>
<p><img alt="" src="../../../_images/t5-unlabeled-datasets.png" /></p>
</section>
<section id="unlabeled-dataset-sizes">
<h3>Unlabeled Dataset Sizes<a class="headerlink" href="#unlabeled-dataset-sizes" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>C4 has <span class="math notranslate nohighlight">\(2^{35}\)</span>= 34B tokens.</p></li>
<li><p>Truncated variants of C4 consisting of <span class="math notranslate nohighlight">\(2^{29}\)</span>, <span class="math notranslate nohighlight">\(2^{27}\)</span>, <span class="math notranslate nohighlight">\(2^{25}\)</span> and <span class="math notranslate nohighlight">\(2^{23}\)</span> tokens.</p></li>
<li><p>These sizes correspond to repeating the data set 64, 256, 1,024, and 4,096 times respectively over the course of pre-training.</p></li>
<li><p>As expected, performance degrades as the data set size shrinks.</p></li>
</ul>
<p><img alt="" src="../../../_images/t5-unlabeled-dataset-sizes.png" /></p>
</section>
<section id="fine-tuning-methods">
<h3>Fine-Tuning Methods<a class="headerlink" href="#fine-tuning-methods" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Fine-tuning all parameters.</p></li>
<li><p>Adapter Layers: keeping most of the original model fixed while fine-tuning. Adapter layers are additional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward networks in each block of the Transformer.</p></li>
<li><p>Gradual Freezing: More and more of the model’s parameters are finetuned over time. At the start of fine-tuning, only the parameters of the final layer are updated, then after training for a certain number of updates the parameters of the second-to-last layer are also included, and so on until the entire network’s parameters are being fine-tuned.</p></li>
</ul>
<p>It is found that gradual unfreezing caused a minor degradation in performance across all tasks, though it did provide some speedup during fine-tuning.</p>
<p><img alt="" src="../../../_images/t5-fine-tuning-methods.png" /></p>
</section>
<section id="multi-task-learning">
<h3>Multi-Task Learning<a class="headerlink" href="#multi-task-learning" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Multi-task learning is to train the model on multiple tasks at a time.</p></li>
<li><p>Multi-task learning underperforms pre-training followed by fine-tuning on most tasks.</p></li>
</ul>
<p><img alt="" src="../../../_images/t5-multi-task-learning.png" /></p>
</section>
<section id="scaling">
<h3>Scaling<a class="headerlink" href="#scaling" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>There are a variety of possible ways to scale, including using a bigger model, training the model for more steps, and ensembling.</p></li>
<li><p>There is no large difference between training a 2x bigger model for 2x as long and training a 4x bigger model on any of the tasks.</p></li>
<li><p>This suggests that increasing the training time and increasing the model size can be complementary means of improving performance.</p></li>
<li><p>The results also suggest that ensembling provides an orthogonal and effective means of improving performance through scale.</p></li>
</ul>
<p><img alt="" src="../../../_images/t5-scaling.png" /></p>
</section>
<section id="sota-comparisons">
<h3>SOTA Comparisons<a class="headerlink" href="#sota-comparisons" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Small, Base, Large, 3B, and 11B refer to model configurations with 60 million, 220 million, 770 million, 3 billion, and 11 billion parameters, respectively. (by tuning different hyperparameters.)</p></li>
<li><p>Overall, state-of-the-art performance is achieved on 18 out of the 24 tasks.</p></li>
<li><p>T5–3B model variant did beat the previous state of the art in a few tasks, but scaling the model size to 11 billion parameters was the most important ingredient for achieving the best performance.</p></li>
<li><p>For SQuAD, T5 outperformed the previous state-of-the-art ALBERT by over one point on the Exact Match score.</p></li>
<li><p>For SuperGLUE, T5 improved upon the state-of-the-art RoBERTa by a large margin from an average score of 84.6 to 88.9.</p></li>
</ul>
<p><img alt="" src="../../../_images/t5-sota-comparisons.png" /></p>
<p>Further experiment is performed on three configurations as above:</p>
<ol class="simple">
<li><p>The standard baseline model, which was pre-trained on 2²⁵=34B tokens.</p></li>
<li><p>Baseline-1T: The baseline trained instead for about 1 trillion tokens (i.e. the same amount of pre-training used for T5).</p></li>
<li><p>T5-Base.</p></li>
</ol>
<p>T5-Base performs substantially better than Baseline-1T, suggesting that scale is not the only factor that contributes to T5’s success.</p>
<p><img alt="" src="../../../_images/t5-sota-comparisons-2.png" /></p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/t5">T5 by huggingface</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/lectures/deep_nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="bert.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">BERT: Bidirectional Encoder Representations from Transformers</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../aiart/index.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">AI Art (Text-to-Image)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Young Joon Lee<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>