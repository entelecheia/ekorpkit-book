

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>ByT5: Towards a token-free future with pre-trained byte-to-byte models &#8212; eKorpkit Tutorials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/ekorpkit.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-FN4GFT8HP8"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-FN4GFT8HP8');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/lectures/nlp_deep/byt5';</script>
    <link rel="canonical" href="https://entelecheia.cc/docs/lectures/nlp_deep/byt5.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Pretrained Language Models" href="plms.html" />
    <link rel="prev" title="SentencePiece Tokenizer" href="sentencepiece.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/ekorpkit.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../../_static/ekorpkit.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    eKonomic Research Python Toolkit
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../basics/index.html">Getting started</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../basics/install.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../basics/usage.html">Usage</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../basics/features/index.html">Key features</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../basics/features/easy_config.html">Easy Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../basics/features/no_boiler.html">No Boilerplate</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../basics/features/workflows.html">Workflows</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../basics/features/share.html">Sharable and Reproducible</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../basics/features/plug.html">Pluggable Architece</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/config/index.html">Configuring ekorpkit</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/config/eKonf.html">Using eKonf class</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/datasets/index.html">Datasets</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/corpora/corpus.html">Build and Load Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/datasets/dataset.html">Build and Load Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/corpora/pipeline.html">Corpus task pipelines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/models/index.html">Models</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/models/automl/index.html">Auto ML</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/models/embeddings/index.html">Word Embeddings</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/models/embeddings/word2vec_basics.html">Word2Vec Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/models/embeddings/eval_vectors.html">Evaluate pretrained embeddings</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/models/ngram/index.html">N-Grams</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/models/ngram/ngram.html">N-Grams</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/models/ngram/ngram_for_lexicons.html">N-Gram model for ngram lexicon features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/models/ngram/ngram_for_words.html">N-Gram model for unigram lexicon features</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/models/sentiment/index.html">Sentiment Analyers</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/models/sentiment/lbsa_en.html">Lexicon-based Sentiment Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/models/sentiment/financial_phrasebank_lm.html">Evaluate LM with financial phrasebank</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/models/sentiment/sentiment.html">LM Dictionary vs. finbert vs. T5</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/models/transformers/index.html">Transformers</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/pipelines/index.html">Pipelines</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pipelines/pipeline.html">Instantiating pipeline</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/preprocessors/index.html">Preprocessors</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/preprocessors/normalizer/index.html">Normalizers</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/preprocessors/normalizer/normalizer.html">Normalizers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/preprocessors/segmenter/index.html">Segmenters</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/preprocessors/segmenter/segmenter.html">Segmenters</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/preprocessors/tokenizer/index.html">Tokenizers</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/preprocessors/tokenizer/mecab.html">Mecab Tokenizer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/visualizers/index.html">Visualizers</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/visualizers/plot.html">Plots</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/workflow/index.html">Workflows</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Datasets</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../corpus/ekorpkot_corpus.html">The eKorpkit Corpus</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Use Cases</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../usecases/fomc/index.html">FOMC</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/fomc/02_FOMC_numerical_data.html">Preparing Numerical Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/fomc/03_FOMC_corpus.html">Preparing Textual Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/fomc/04_FOMC_EDA_numericals.html">EDA on Numerical Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/fomc/05_FOMC_features.html">Visualizing Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/fomc/06_FOMC_AutoML.html">Checking Baseline with AutoML</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/fomc/07_FOMC_sentiments.html">Predicting Sentiments of FOMC Corpus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/fomc/08_FOMC_EDA_sentiments.html">EDA on Sentiment Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/fomc/09_FOMC_AutoML_with_tones.html">Predicting the next decisions with tones</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../usecases/bok/index.html">Bank of Korea</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../usecases/edgar/index.html">EDGAR</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/edgar/predict_edgar.html">Prediciting Sentiments</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../usecases/esg/index.html">ESG</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/esg/prepare_datasets.html">Preparing training datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/esg/improve_datasets.html">Improving classification datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/esg/train_classifiers.html">Training Classifiers for ESG Ratings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/esg/build_news_corpus.html">Building <code class="docutils literal notranslate"><span class="pre">econ_news_kr</span></code> corpus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/esg/predict_esg_classes.html">Predicting ESG Categories and Polarities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/esg/cross_validate_datasets.html">Cross validating datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/esg/prepare_datasets_for_labeling.html">Preparing active learning data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/esg/all_in_one_pipeline.html">Putting them together in a pipeline</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../usecases/esg-en/index.html">ESG (English)</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/esg-en/esg_corpus.html">Building <code class="docutils literal notranslate"><span class="pre">JOCo</span></code> corpus</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../usecases/cointax/index.html">Taxation on Cryptocurrency</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../usecases/cointax/coin_dataset.html">Improving classification datasets</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Research</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../research/esg_topics/analyst.html">ESG Topic Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../research/esg_topics/rethink_esg.html">Rethinking ESG</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../nlp_intro/index.html">Introduction to NLP</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/research.html">Research Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/language_models.html">Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic.html">Topic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_models.html">Topic Models </a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/topic_coherence.html">Topic Coherence Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/sentiments.html">Sentiment Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_segmentation.html">Word Segmentation and Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/vectorization.html">Vector Semantics and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_intro/lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Deep Learning for NLP</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="ekorpkit.html">Getting started with ekorpkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="zeroshot.html">Zero Shot, Prompt, and Search Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="bloom.html">What is BLOOM?</a></li>
<li class="toctree-l2"><a class="reference internal" href="bloom_apps.html">Bloom Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers.html">Transformers </a></li>
<li class="toctree-l2"><a class="reference internal" href="bert.html">BERT: Bidirectional Encoder Representations from Transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="t5.html">T5: Text-To-Text Transfer Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="tokenization.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="sentencepiece.html">SentencePiece Tokenizer</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></li>
<li class="toctree-l2"><a class="reference internal" href="plms.html">Pretrained Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab1-corpus.html">Lab 1: Preparing Wikipedia Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab2-corpus-eda.html">Lab 2: EDA on Corpora</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab3-train-tokenizers.html">Lab 3: Training Tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab4-pretraining-lms.html">Lab 4: Pretraining Language Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../nlp_apps/index.html">NLP Applications</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../aiart/index.html">AI Art (Text-to-Image)</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../aiart/intro.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/project-themes.html">Project Themes - A Brave New World</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle1.html">DALL·E 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle2.html">DALL·E 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/imagen.html">Imagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/dalle-mini.html">DALL·E Mini</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/stable-diffusion.html">Stable Diffusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/prompt-generator.html">Prompt Generator for Stable Diffusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/textual-inversion.html">Textual Inversion (Dreambooth)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/whisper.html">Automatic Speech Recognition (Whisper)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/text2music.html">Text to Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aiart/image2music.html">Image to Music</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">Machine Learning Systems Design</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dev-env.html">Development Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/dotfiles.html">Dotfiles</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ds/index.html">Data Science for Economics and Finance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../projects/robot_drawing/index.html">Robot Drawing System</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../projects/robot_drawing/intro.html">Introduction to Robot Drawing Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../projects/robot_drawing/proposal.html">Proposal for a Robot Drawing System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../projects/robot_drawing/sketch_generation.html">Reference - Sketch Generation with Drawing Process Guided by Vector Flow and Grayscale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../projects/robot_drawing/robocodraw.html">Reference - Avatar-GAN (RoboCoDraw)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../projects/robot_drawing/fast_robotic_pencil_drawing.html">Reference - Fast Robotic Pencil Drawing</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://entelecheia.me">entelecheia.me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/index.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/cite.html">Citation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/entelecheia/ekorpkit-book/blob/main/ekorpkit-book/docs/lectures/nlp_deep/byt5.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/ekorpkit-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/ekorpkit-book/issues/new?title=Issue%20on%20page%20%2Fdocs/lectures/nlp_deep/byt5.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/docs/lectures/nlp_deep/byt5.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>ByT5: Towards a token-free future with pre-trained byte-to-byte models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-token-in-machine-learning">What Is A Token In Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-byt5">What Is ByT5?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-changes-to-the-mt5-architecture">Key Changes To the mT5 Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-tokens-are-used">How Tokens Are Used</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-technical-details">The Technical Details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-pros-and-cons-of-byt5">The Pros And Cons Of ByT5</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pros">Pros</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cons">Cons</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-results">The Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#english-classification-tasks-glue-superglue">English Classification Tasks (GLUE, SuperGLUE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#english-generation-tasks-xsum-tweetqa-drop">English Generation Tasks (XSum, TweetQA, DROP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-lingual-benchmarks">Cross-lingual Benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-level-tasks">Word-Level Tasks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments-on-synthetic-noise">Experiments on Synthetic Noise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ablation-study">Ablation Study</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="byt5-towards-a-token-free-future-with-pre-trained-byte-to-byte-models">
<h1>ByT5: Towards a token-free future with pre-trained byte-to-byte models<a class="headerlink" href="#byt5-towards-a-token-free-future-with-pre-trained-byte-to-byte-models" title="Permalink to this heading">#</a></h1>
<p><img alt="" src="../../../_images/byt5-intro.png" /></p>
<p>The ByT5 model was presented in <span id="id1">[<a class="reference internal" href="../../about/index.html#id22" title="Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. Byt5: towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291–306, 2022. URL: https://arxiv.org/pdf/2105.13626v1.pdf.">Xue <em>et al.</em>, 2022</a>]</span>.</p>
<p>The abstract from the paper is the following:</p>
<blockquote>
<div><p>Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.</p>
</div></blockquote>
<section id="what-is-a-token-in-machine-learning">
<h2>What Is A Token In Machine Learning?<a class="headerlink" href="#what-is-a-token-in-machine-learning" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A token is a sequence of characters that is considered as a single entity during processing.</p></li>
<li><p>Tokens are usually derived from words, but they can also be derived from subwords, characters, or even bytes.</p></li>
<li><p>For example, the sentence “The quick brown fox jumps over the lazy dog” can be tokenized into the following tokens: [“The”, “quick”, “brown”, “fox”, “jumps”, “over”, “the”, “lazy”, “dog”].</p></li>
<li><p>Some words can be tokenized into multiple tokens, for example, “don’t” can be tokenized into [“do”, “n’t”].</p></li>
<li><p>At the byte or chracter level, the sentence can be tokenized into the 43 character tokens.</p></li>
<li><p>In transformer models, tokens are usually represented as vectors of fixed length, for example, 512-dimensional vectors to limit the cost of computation.</p></li>
<li><p>Attention mechanisms are expensive, and the cost of computation increases with the order of <span class="math notranslate nohighlight">\(N^2\)</span> where <span class="math notranslate nohighlight">\(N\)</span> is the number of tokens in the sequence.</p></li>
<li><p>This explains why tokenization is important, it dramatically reduces the number of tokens in a sequence, and thus the cost of computation.</p></li>
</ul>
</section>
<section id="what-is-byt5">
<h2>What Is ByT5?<a class="headerlink" href="#what-is-byt5" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>ByT5 is a token-free model that operates directly on raw text (bytes or characters).</p></li>
<li><p>Therefore, it does not require a tokenizer, and it can process text in any language out of the box.</p></li>
<li><p>One advantage of token-free models is that they are more robust to noise.</p></li>
<li><p>Also, out-of-vocabulary words are not a problem for token-free models. It is <code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code>-free.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code> is the token used to represent out-of-vocabulary words in token-based models.</p></li>
<li><p>The proposed ByT5 is based on Google’s recent token-based mT5 (Massively Multilingual Text-to-Text Transfer Transformer)</p></li>
</ul>
<p><img alt="" src="../../../_images/byt5-vs-mt5.png" /></p>
</section>
<section id="key-changes-to-the-mt5-architecture">
<h2>Key Changes To the mT5 Architecture<a class="headerlink" href="#key-changes-to-the-mt5-architecture" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Feeding UTF-8 bytes of the SentencePiece vocabulary directly to the embedding layer without any text preprocessing.</p></li>
<li><p>An additional 3 IDs are reserved for the special tokens: padding, end-of-sequence, and an unused token.</p></li>
<li><p>The team then modifies the pretrained task such that instead of adding 100 new tokens for the sentinels, they reuse the final 100 byte IDs.</p></li>
<li><p>Rather than using an average span length of 3 subwords, they mask longer byte-spans with a mean length of 20 bytes.</p></li>
<li><p>They found that byte-level models with a heavier encoder perform better on both classification and generation tasks.</p></li>
<li><p>They dropped any illegal bytes in the model’s ouput to keep the output valid UTF-8.</p></li>
</ul>
</section>
<section id="how-tokens-are-used">
<h2>How Tokens Are Used<a class="headerlink" href="#how-tokens-are-used" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>The mT5 tokens are much shorter than the ByT5 tokens because every character is a token in ByT5.</p></li>
<li><p>The model is designed to remove <span class="math notranslate nohighlight">\(n\)</span> tokens from the input and replace with placeholder tokens.</p></li>
<li><p>The encoder is given the content with the sets of tokens missing and the placeholder to understand where the tokens are missing.</p></li>
<li><p>The decoder is given the missing tokens and the placeholder assigned to the content that is hidden to it.</p></li>
<li><p>Essentially, the encoder knows the content with the tokens missing and the decoder knows the missing tokens.</p></li>
<li><p>On training, accuracy is measured by how well the decoder can predict the missing tokens.</p></li>
<li><p>Accuracy of 80% means that the decoder can predict 80% of the missing tokens correctly.</p></li>
</ul>
<p><img alt="" src="../../../_images/byt5-mc4.png" /></p>
</section>
<section id="the-technical-details">
<h2>The Technical Details<a class="headerlink" href="#the-technical-details" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>“We release ByT5 in five sizes analogous to T5 and mT5 (Small, Base, Large, XL, XXL). We aim for ByT5 to cover the same use cases as mT5: it is a general-purpose pre-trained text-to-text model covering 100+ languages. We expect ByT5 will be particular useful for tasks operating on short-to-medium length text sequences (a few sentences or less), as these will incur less slowdown in fine-tuning and inference.”</p>
</div></blockquote>
<ul class="simple">
<li><p>There are five sizes of ByT5 models covering 100+ languages.</p></li>
<li><p>The prediction is that ByT5 will be better for tasks operating on shorter text sequences.</p></li>
<li><p>A marginally better model that takes 100x more time to train and run is not a good trade-off.</p></li>
<li><p>ByT5 requires ~5x more tokens for the same text length compared to mT5.</p></li>
</ul>
<blockquote>
<div><p>“Second, we modify the pre-training task. mT5 uses the “span corruption” pre-training objective first proposed by Raffel et al. (2020) where spans of tokens in unlabeled text data are replaced with a single “sentinel” ID and the model must fill in the missing spans. Rather than adding 100 new tokens for the sentinels, we find it sufficient to reuse the final 100 byte IDs. While mT5 uses an average span length of 3 subword tokens, we find that masking longer byte-spans is valuable.”</p>
</div></blockquote>
<ul class="simple">
<li><p>mT5 replaces 3 tokens with a sentinel token.</p></li>
<li><p>ByT5 replaces 20 bytes with a sentinel token.</p></li>
</ul>
<blockquote>
<div><p>“Third, we find that ByT5 performs best when we decouple the depth of the encoder and decoder transformer stacks. While T5 and mT5 used “balanced” architectures, we find byte-level models benefit significantly from a “heavier” encoder. Specifically, we set our encoder depth to 3 times that of the decoder.”</p>
</div></blockquote>
<ul class="simple">
<li><p>mT5 used balances architectures with the same number of layers in the encoder and decoder.</p></li>
<li><p>ByT5 uses a heavier encoder with 3 times the number of layers as the decoder.</p></li>
</ul>
</section>
<section id="the-pros-and-cons-of-byt5">
<h2>The Pros And Cons Of ByT5<a class="headerlink" href="#the-pros-and-cons-of-byt5" title="Permalink to this heading">#</a></h2>
<section id="pros">
<h3>Pros<a class="headerlink" href="#pros" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>In a model with a large vocabulary, the vocaboulary matrix can take up a substantial proportion of the model’s parameters.</p></li>
<li><p>Vocabulary of mT5 takes up 85% ~ 16% of the model’s parameters depending on the model size.</p></li>
<li><p>By switching to a byte-level model, the saved parameters can be used for other purposes, such as adding more layers or making the layers wider.</p>
<p><img alt="" src="../../../_images/byt5-vocab.png" /></p>
</li>
</ul>
</section>
<section id="cons">
<h3>Cons<a class="headerlink" href="#cons" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>Byte-level sequences are much longer than token-level sequences, and this increases the cost of computation.</p></li>
<li><p>If the decoder is particularly large, autoregressive sampling can be expensive.</p></li>
<li><p>In terms of FLOPs (floating-point operations per second), ByT5 requires ~1.2x more operations for the pre-training.</p></li>
<li><p>On word-level tasks, ByT5 is fairly competitive with mT5.</p></li>
<li><p>On tasks with longer input sequences, the slowdown is more pronounced.</p>
<p><img alt="" src="../../../_images/byt5-flops-pretrain.png" /></p>
</li>
</ul>
</section>
</section>
<section id="the-results">
<h2>The Results<a class="headerlink" href="#the-results" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>“ByT5 is competitive with mT5 on standard English and multilingual NLP benchmarks and outperforms mT5 at small model sizes. Additionally ByT5 excels on free-form generation tasks and transliteration.”</p>
</div></blockquote>
<section id="english-classification-tasks-glue-superglue">
<h3>English Classification Tasks (GLUE, SuperGLUE)<a class="headerlink" href="#english-classification-tasks-glue-superglue" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>ByT5 outperforms mT5 on small and base model sizes by sizable margins, and loses close battles on larger models.</p>
<p><img alt="" src="../../../_images/byt5-glue.png" /></p>
</li>
</ul>
</section>
<section id="english-generation-tasks-xsum-tweetqa-drop">
<h3>English Generation Tasks (XSum, TweetQA, DROP)<a class="headerlink" href="#english-generation-tasks-xsum-tweetqa-drop" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>XSum gets the model to summarize a news article in a single sentence, and TweetQA is question answering from Tweets. DROP is a challenging reading comprehension task that requires numerical reasoning.</p></li>
<li><p>ByT5 outperforms mT5 on all three tasks across all model sizes.</p>
<p><img alt="" src="../../../_images/byt5-gen.png" /></p>
</li>
</ul>
</section>
<section id="cross-lingual-benchmarks">
<h3>Cross-lingual Benchmarks<a class="headerlink" href="#cross-lingual-benchmarks" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>Changes to vocabulary and tokenization are likely to affect different languages in different ways.</p></li>
<li><p>On the most realistic in-language setting, where some gold training data is available in all languages, ByT5 surpasses the previous state-of-art mT5 on all tasks and model sizes.</p></li>
<li><p>One might expect languages with rich inflectional morphology (e.g. Turkish) to benefit most from the move away from a fixed vocabulary.</p></li>
<li><p>Languages with a higher SentencePiece token compression rate (e.g. Thai and Telugu) tend to favor mT5, whereas those with a lower compression rate (e.g. Indonesian and Vietnamese) tend to favor ByT5.</p>
<p><img alt="" src="../../../_images/byt5-xling.png" /></p>
</li>
</ul>
</section>
<section id="word-level-tasks">
<h3>Word-Level Tasks<a class="headerlink" href="#word-level-tasks" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>Unsurprisingly, “characteraware” models excel on tasks around word-internal phenonema.</p></li>
<li><p>These core NLP tasks have often been overlooked in evaluating general-purpose NLP models.</p>
<p><img alt="" src="../../../_images/byt5-word.png" /></p>
</li>
</ul>
</section>
</section>
<section id="experiments-on-synthetic-noise">
<h2>Experiments on Synthetic Noise<a class="headerlink" href="#experiments-on-synthetic-noise" title="Permalink to this heading">#</a></h2>
<p>The authors test six types of noise on the model:</p>
<ul class="simple">
<li><p><strong>Drop</strong>: Each character has a 10% chance of being dropped.</p></li>
<li><p><strong>Add/Drop/Mutate</strong>: At each character position, there is a 10% chance of applying one of three actions, with equal likelihood: Add (inserts a random character from the input), Drop (deletes this character) or Mutate (replaces this character with a random character from the input).</p></li>
<li><p><strong>Repetitions</strong>: Each character has a 20% chance of being selected for repetition. If selected, 1–3 repetitions (with equal likelihood) are appended after the original character.</p></li>
<li><p><strong>Antspeak</strong>: Each character is capitalized and padded with spaces. For example, “abc def” becomes “ A B C D E F ”.</p></li>
<li><p><strong>Uppercase</strong>: Each character is converted to uppercase. Here, we restrict to languages whose scripts distinguish case (for XNLI: Bulgarian, English, French, German, Greek, Russian, Spanish, Swahili, Turkish, Vietnamese; for TyDiQA-GoldP: English, Finnish, Indonesian, Russian, Swahili).</p></li>
<li><p><strong>Random case</strong>: Each character is set to a random case (upper or lower). Again, only languages whose scripts distinguish case are considered.</p></li>
</ul>
<section id="results">
<h3>Results<a class="headerlink" href="#results" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>Character-level models are more robust to real and synthetic noise than BPE or word-based models, across a range of morphological, syntactic and semantic tagging tasks.</p></li>
<li><p>Token-free models are more robust to noise across many tasks.</p>
<p><img alt="" src="../../../_images/byt5-noise.png" /></p>
</li>
</ul>
</section>
</section>
<section id="ablation-study">
<h2>Ablation Study<a class="headerlink" href="#ablation-study" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>“ByT5 outperforms mT5 in any of these four scenarios: (1) at model sizes under 1 billion parameters, (2) on generative tasks, (3) on multilingual tasks with in-language labels, and (4) in the presence of various types of noise.”</p>
</div></blockquote>
<blockquote>
<div><p>“… the gains we observe with ByT5 are achieved despite the fact that the model is pretrained on 4 times less text than mT5. This suggests that byte-level models could be more data efficient learners.”</p>
</div></blockquote>
<blockquote>
<div><p>“Our “hands-off” approach of feeding raw UTF-8 bytes directly into the transformer costs +33% pre-training time, as well as longer inference time (10–50% longer for small models, and 100–600% longer for our largest models). As such, there is significant room for improvement. We believe techniques such as hash embeddings, local attention and down-sampling (Clark et al., 2021), as well as sparse computation (Fedus et al., 2021) can help address latency issues, removing the remaining barriers to a token-free future.”</p>
</div></blockquote>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>While ByT5 takes longer to train, and underperforms in some tasks (zero-shot translation for example) the payoff for tasks where noise is an issue (think social and voice) is significant.</p></li>
<li><p>Token-free models and token-based models can coexist, and the best model for a given task will be assigned based on the task’s characteristics.</p></li>
<li><p>Token-free models will likely dramatically enhance voice search technologies where noise is prevalent.</p></li>
<li><p>Token-free models could collect information from noisy environments and to more quickly learn new terms being used as shorthand, slang, or jargon, and even emojis.</p></li>
<li><p>These information could be used to improve the performance of token-based models.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2105.13626v1.pdf">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></p></li>
<li><p><a class="reference external" href="https://wandb.ai/onlineinference/byt5/reports/ByT5-What-It-Might-Mean-For-SEO--Vmlldzo4NzY1NzE#what-is-byt5?">ByT5: What It Might Mean For SEO</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/lectures/nlp_deep"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="sentencepiece.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">SentencePiece Tokenizer</p>
      </div>
    </a>
    <a class="right-next"
       href="plms.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pretrained Language Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-token-in-machine-learning">What Is A Token In Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-byt5">What Is ByT5?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-changes-to-the-mt5-architecture">Key Changes To the mT5 Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-tokens-are-used">How Tokens Are Used</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-technical-details">The Technical Details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-pros-and-cons-of-byt5">The Pros And Cons Of ByT5</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pros">Pros</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cons">Cons</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-results">The Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#english-classification-tasks-glue-superglue">English Classification Tasks (GLUE, SuperGLUE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#english-generation-tasks-xsum-tweetqa-drop">English Generation Tasks (XSum, TweetQA, DROP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-lingual-benchmarks">Cross-lingual Benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-level-tasks">Word-Level Tasks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments-on-synthetic-noise">Experiments on Synthetic Noise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ablation-study">Ablation Study</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://entelecheia.me">Young Joon Lee</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>