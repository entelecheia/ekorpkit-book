{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc871e8",
   "metadata": {},
   "source": [
    "# Getting Started with Bloom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686cd6d",
   "metadata": {},
   "source": [
    "## Zero Shot and Few Shot Learners\n",
    "\n",
    "Large Language Models can be fine-tuned to new tasks very quickly. \n",
    "For some of the tasks, these models can show good enough performance based on just a few examples. \n",
    "These examples have come to be called `prompts` to a language model, and formatting the examples as an input is being referred to as `prompt engineering`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3afe2",
   "metadata": {},
   "source": [
    "## Downloading a Pre-Trained Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d9504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926be8ccedd64d71978e74d6daa3d770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00016-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e71cde0261d4e83a3ea143606f0db00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00017-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eb44d0310341ac9c61d3d2b9fe8d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00018-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\")\n",
    "model = AutoModel.from_pretrained(\"bigscience/bloom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52130105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "s = time.time()\n",
    "pipe = pipeline(model=\"bigscience/bloom\", torch_dtype=torch.bfloat16)\n",
    "print(f\"Time to load model: {time.time()-s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8de902",
   "metadata": {},
   "outputs": [],
   "source": [
    "rom IPython.display import HTML as html_print\n",
    "\n",
    "def cstr(s, color='black'):\n",
    "    #return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s.replace('\\n', '<br>'))\n",
    "\n",
    "def cstr_with_newlines(s, color='black'):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s.replace('\\n', '<br>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caec008",
   "metadata": {},
   "source": [
    "Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\n",
    "A: Let’s think step by step.\n",
    "Let $B \\overset{\\text{ref}}{=}$ blue golf balls.\n",
    "Since half of the balls are a few foreign yones distinctive of Chinese name cosmogonies but are China a unique case have variations been overlooked do exist Thesis method to cover all ages according which are almost but in the former case but again with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fa2fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"One of the hottest areas of investing in recent years has been ESG: \"\n",
    "promtt += \"the use of environmental, social, and governance criteria to evaluate possible investments.\"\n",
    "\n",
    "result_length = 100\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1fac6d",
   "metadata": {},
   "source": [
    "- `result_length` calibrates the size of the response (in tokens) we get for the prompt from the model.\n",
    "- `inputs` contains the embedding representation of prompt, encoded for use specifically by PyTorch. If we were using TensorFlow we’d pass return_tensors=\"tf\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b342170",
   "metadata": {},
   "source": [
    "## Running Inference: Strategies for Better Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4cc136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the hottest areas of investing in recent years has been ESG: the use of environmental, social, and governance criteria to evaluate possible investments. The ESG movement has been growing steadily since the financial crisis, and the number of companies that have adopted ESG criteria has increased by more than 50% in the last five years. The ESG movement is also growing in the United States, where the number of companies that have adopted ESG criteria has increased by more than 50% in the\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(inputs[\"input_ids\"], max_length=result_length)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_inf(prompt, temperature=0.7, top_p=None, max_new_tokens=32, repetition_penalty=None, do_sample=False, num_return_sequences=1):  \n",
    "    response = pipe(f\"{prompt}\", \n",
    "                    temperature = temperature, # 0 to 1\n",
    "                    top_p = top_p, # None, 0-1\n",
    "                    max_new_tokens = max_new_tokens, # up to 2047 theoretically\n",
    "                    return_full_text = False, # include prompt or not.\n",
    "                    repetition_penalty = repetition_penalty, # None, 0-100 (penalty for repeat tokens.\n",
    "                    do_sample = do_sample, # True: use sampling, False: Greedy decoding.\n",
    "                    num_return_sequences = num_return_sequences\n",
    "                    )\n",
    "    return html_print(cstr(prompt, color='#f1f1c7') + cstr(response[0]['generated_text'], color='#a1d8eb')), response[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"\"\"# Use OpenCV in Python\"\"\"\n",
    "color_resp, resp = local_inf(inp, max_new_tokens=64)\n",
    "color_resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39410c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f869af7787e6a1c49e09e367fc6e1b81d93d1c6583b43249c80edc047bd13cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
