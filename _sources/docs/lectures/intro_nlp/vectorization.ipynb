{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Vector Semantics and Word Embeddings\n",
    "\n",
    "![](../figs/intro_nlp/vector/entelecheia_alphabet_letters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## Vector Semantics and Word Embeddings\n",
    "\n",
    "- Lexical semantics is the study of the meaning of words\n",
    "- Distributional hypothesis: words that occur in similar contexts have similar meanings\n",
    "- Sparse vectors: one-hot encoding\n",
    "- Dense vectors: word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f57ee",
   "metadata": {},
   "source": [
    "### What do words mean, and how do we represent that?\n",
    "\n",
    "> `cassoulet`\n",
    "\n",
    "Do we want to represent that ...\n",
    "\n",
    "- \"cassoulet\" is a French dish?\n",
    "- \"cassoulet\" contains meat and beans?\n",
    "- \"cassoulet\" is a stew?\n",
    "\n",
    "> `bar`\n",
    "\n",
    "Do we want to represent that ...\n",
    "\n",
    "- \"bar\" is a place where you can drink alcohol?\n",
    "- \"bar\" is a long rod?\n",
    "- \"bar\" is to prevent something from moving?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf9cd3",
   "metadata": {},
   "source": [
    "### Different approaches to lexical semantics\n",
    "\n",
    "NLP draws on two different approaches to lexical semantics:\n",
    "\n",
    "- **Lexical semantics**: \n",
    "  - The study of the meaning of words\n",
    "  - The lexicographic tradition aims to capture the information represented in lexical entries in dictionaries\n",
    "- **Distributional semantics**: \n",
    "  - The study of the meaning of words based on their distributional properties in large corpora\n",
    "  - The distributional hypothesis: words that occur in similar contexts have similar meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55701de7",
   "metadata": {},
   "source": [
    "#### Lexical semantics\n",
    "\n",
    "- Uses resources such as `lexicons`, `thesauri`, `ontologies` etc. that capture explicit knowledge about word meanings.\n",
    "- Assumes that words have `discrete word senses` that can be represented in a `lexicon`.\n",
    "  - bank 1 = a financial institution\n",
    "  - bank 2 = a river bank\n",
    "- May capture explicit knowledge about word meanings, but is limited in its ability to capture the meaning of words that are not in the lexicon.\n",
    "  -  `dog` is a `canine` (lexicon)\n",
    "  -  `cars` have `wheels` (lexicon)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d7870",
   "metadata": {},
   "source": [
    "#### Distributional semantics\n",
    "\n",
    "- Uses `large corpora of raw text` to learn the meaning of words from the contexts in which they occur.\n",
    "- Maps words to `vector representations` that capture the `distributional properties` of the words in the corpus.\n",
    "- Uses neural networks to learn the dense vector representations of words, `word embeddings`, from large corpora of raw text.\n",
    "- If each word is mapped to a single vector, this ignores the fact that words can have multiple meanings or parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c71ae",
   "metadata": {},
   "source": [
    "### How do we represent words to capture word similarities?\n",
    "\n",
    "- As `atomic symbols`\n",
    "  - in a traditional n-gram language model\n",
    "  - explicit features in a machine learning model\n",
    "  - this is equivalent to very high-dimensional one-hot vectors:\n",
    "    - aardvark = [1,0,...,0], bear = [0,1,...,0], ..., zebra = [0,0,...,1]\n",
    "    - height and tall are as different as aardvark and zebra\n",
    "- As very high-dimensional `sparse vectors`\n",
    "  - to capture the distributional properties of words\n",
    "- As low-dimensional `dense vectors`\n",
    "  - word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943297e",
   "metadata": {},
   "source": [
    "### What should word representations capture?\n",
    "\n",
    "- Vector representations of words were originally used to capture `lexical semantics` so that words with similar meanings would be represented by vectors that are close together in vector space.\n",
    "- These representations may also capture some `morphological` and `syntactic` information about words. (part of speech, inflections, stems, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04d66b",
   "metadata": {},
   "source": [
    "#### The Distributional Hypothesis\n",
    "\n",
    "Zellig Harris (1954):\n",
    "- Words that occur in similar contexts have similar meanings.\n",
    "- `oculist` and `eye doctor` occur in almost the same contexts\n",
    "- If A and B have almost the same environment, then A and B are synonymous.\n",
    "\n",
    "John Firth (1957):\n",
    "- You shall know a word by the company it keeps.\n",
    "\n",
    "> The `contexts` in which words occur tell us a lot about the meaning of words.\n",
    "> \n",
    "> Words that occur in similar contexts have similar meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad362e4",
   "metadata": {},
   "source": [
    "#### Why do we care about word contexts?\n",
    "\n",
    "What is `tezgüino`?\n",
    "\n",
    "- A bottle of `tezgüino` is on the table.\n",
    "- Everybody likes `tezgüino`.\n",
    "- `Tezgüino` makes you drunk.\n",
    "- We make `tezgüino` out of corn.\n",
    "\n",
    "We don't know what `tezgüino` is, but we can guess that it is a drink because we understand these sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f6e92",
   "metadata": {},
   "source": [
    "If we have the following sentences:\n",
    "\n",
    "- A bottle of `wine` is on the table.\n",
    "- There is a `beer` bottle on the table\n",
    "- `Beer` makes you drunk.\n",
    "- We make `bourbon` out of corn.\n",
    "- Everybody likes `chocolate`\n",
    "- Everybody likes `babies`\n",
    "\n",
    "Could we guess that `tezgüino` is a drink like `wine` or `beer`?\n",
    "\n",
    "However, there are also red herrings:\n",
    "- Everybody likes `babies`\n",
    "- Everybody likes `chocolate`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca22e33",
   "metadata": {},
   "source": [
    "### Two ways NLP uses context for semantics\n",
    "\n",
    "`Distributional similarity`: (vector-space semantics)\n",
    "- Assume that words that occur in similar contexts have similar meanings.\n",
    "- Use the `set of all contexts` in which a word occurs to measure the `similarity` between words.\n",
    "\n",
    "`Word sense disambiguation`:\n",
    "- Assume that if a word has multiple meanings, then it will occur in different contexts for each meaning.\n",
    "- Use the context of a particular occurrence of a word to identify the `sense` of the word in that context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abcaaa",
   "metadata": {},
   "source": [
    "## Distributional Similarity\n",
    "\n",
    "### Basic idea\n",
    "\n",
    "- Measure the semantic `similarities of words` by measuring the `similarity of their contexts` in which they occur\n",
    "\n",
    "### How?\n",
    "\n",
    "- Represent words as `sparse vectors` such that:\n",
    "  - each `vector element` (dimension) represents a different `context`\n",
    "  - the `value` of each element is the `frequency` of the context in which the word occurs, capturing how `strongly` the word is associated with that context\n",
    "- Compute the `semantic similarity of words` by measuring the `similarity of their context vectors`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d1978",
   "metadata": {},
   "source": [
    "Distributional similarities represent each word $w$ as a vector $v_w$ of context counts:\n",
    "\n",
    "$$w = (w_1 , \\ldots , w_N ) \\in R^N$$\n",
    "\n",
    "in a vector space $R^N$ where $N$ is the number of contexts.\n",
    "\n",
    "- each dimension $i$ represents a different context $c_i$\n",
    "- each element $v_{w,i}$ captures how strongly $w$ is associated with context $c_i$\n",
    "- $v_{w,i}$ is the co-occurrence count of $w$ and $c_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6149211",
   "metadata": {},
   "source": [
    "### The Information Retrieval perspective: The Term-Document Matrix\n",
    "\n",
    "In information retrieval, we search a collection of $N$ documents for $M$ terms:\n",
    "\n",
    "- We can represent each `word` in the vocabulary $V$ as an $N$-dimensional vector $v_w$ where $v_{w,i}$ is the `frequency` of the word $w$ in document $i$.\n",
    "- Conversely, we can represent each `document` as an $M$-dimensional vector $v_d$ where $v_{d,j}$ is the `frequency` of the term $t_j$ in document $d$.\n",
    "\n",
    "Finding the `most relevant` documents for a query $q$ is equivalent to finding the `most similar` documents to the query vector $v_q$.\n",
    "- Queries are also documents, so we can use the same vector representation for queries and documents.\n",
    "- Use the similarity of the query vector $v_q$ to the document vectors $v_d$ to rank the documents.\n",
    "- Documents are similar to queries if they have similar terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4f370",
   "metadata": {},
   "source": [
    "### Term-Document Matrix\n",
    "\n",
    "![](../figs/intro_nlp/vector/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c953ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
