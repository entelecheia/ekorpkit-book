{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "![](../figs/intro_nlp/tokenization/entelecheia_puzzle_pieces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "NLP systems have three main components that help machines understand natural language:\n",
    "\n",
    "- **Tokenization**: Splitting a string into a list of tokens.\n",
    "- **Embedding**: Mapping tokens to vectors.\n",
    "- **Model**: A neural network that takes token vectors as input and outputs predictions.\n",
    "\n",
    "Tokenization is the first step in the NLP pipeline. \n",
    "\n",
    "- Tokenization is the process of splitting a string into a list of tokens. \n",
    "- For example, the sentence \"I like to eat apples\" can be tokenized into the list of tokens `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`. \n",
    "- The tokens can be words, characters, or subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39810160",
   "metadata": {},
   "source": [
    "## What is Tokenization?\n",
    "\n",
    "- Tokenization is the process of representing a text in smaller units called tokens.\n",
    "- In a very simple case, we can simply map every word in the text to a numerical index.\n",
    "- For example, the sentence \"I like to eat apples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`. \n",
    "- Then, each token can be mapped to a unique index, such as:\n",
    "  > `{\"I\": 0, \"like\": 1, \"to\": 2, \"eat\": 3, \"apples\": 4}`.\n",
    "- There are more linguistic features to consider when tokenizing a text, such as punctuation, capitalization, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cc5c5",
   "metadata": {},
   "source": [
    "## Why do we need tokenization?\n",
    "\n",
    "- \"How can we make a machine read a sentence?\"\n",
    "- Machines don’t know any language, nor do they understand sounds or phonetics.\n",
    "- They need to be taught from scratch.\n",
    "- The first step is to break down the sentence into smaller units that the machine can process.\n",
    "- Tokenization determines how the input is represented to the model.\n",
    "- This decision has a huge impact on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29cf2e",
   "metadata": {},
   "source": [
    "## How do we identify words in text?\n",
    "\n",
    "For a language like English, this seems like a simple task. We can simply split the text by spaces. \n",
    "\n",
    "```\n",
    "A word is any sequence of alphabetical characters between whitespaces that’s not a punctuation mark?\n",
    "```\n",
    "\n",
    "\n",
    "However, there are many cases where this is not true.\n",
    "\n",
    "- What about contractions? \n",
    "  - \"I'm\" is a single word, but it is split into two tokens.\n",
    "- What about abbreviations? \n",
    "  - \"U.S.\" is a single word, but it is split into two tokens.\n",
    "- What about hyphenated words? \n",
    "  - \"self-driving\", \"R2-D2\" are single words, but they are split into two tokens.\n",
    "- What about complex names? \n",
    "  - \"New York\" is a single word, but it is split into two tokens.\n",
    "- What about languages like Chinese that have no spaces between words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fcaf3e",
   "metadata": {},
   "source": [
    "### Words aren’t just defined by blanks\n",
    "\n",
    "Problem 1: Compounding\n",
    "\n",
    "```\n",
    "“ice cream”, “website”, “web site”, “New York-based”\n",
    "```\n",
    "\n",
    "Problem 2: Other writing systems have no blanks\n",
    "\n",
    "```\n",
    "Chinese: 我开始写⼩说 = 我 开始 写 ⼩说 (I start(ed) writing novel(s))\n",
    "```\n",
    "\n",
    "Problem 3: Contractions and Clitics\n",
    "\n",
    "```\n",
    "English: “doesn’t” , “I’m” ,\n",
    "Italian: “dirglielo” = dir + gli(e) + lo (tell + him + it)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421276ae",
   "metadata": {},
   "source": [
    "### Tokenization Standards\n",
    "\n",
    "Any actual NLP system will assume a particular tokenization standard.\n",
    "\n",
    "- NLP systems are usually trained on particular corpora (text datasets) that everybody uses.\n",
    "- These corpora often define a de facto standard.\n",
    "\n",
    "Penn Treebank 3 standard:\n",
    "\n",
    "- Input:\n",
    "  > `\"The San Francisco-based restaurant,\" they said, \"doesn’t charge $10\".`\n",
    "- Output:\n",
    "  > `“_ The _ San _ Francisco-based _ restaurant _ , _” _ they_ said* ,* \"_ does _ n’t _ charge_ $_ 10 _ \" _ . _`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d9c79",
   "metadata": {},
   "source": [
    "### What about sentence boundaries?\n",
    "\n",
    "How can we identify that this is two sentences?\n",
    "\n",
    "```\n",
    "Mr. Smith went to D.C. Ms. Xu went to Chicago instead.\n",
    "```\n",
    "\n",
    "- We can use a period to identify the end of a sentence.\n",
    "- However, this is not always true.\n",
    "- Abbreviations, such as \"Mr.\", \"D.C.\", \"Ms.\", \"U.S.\", \"etc.\" can be followed by a period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1463193",
   "metadata": {},
   "source": [
    "How many sentences are in this text?\n",
    "\n",
    "```\n",
    "\"The San Francisco-based restaurant,\" they said, \"doesn’t charge $10\".\n",
    "```\n",
    "\n",
    "Answer: just one, because the comma is not a sentence boundary.\n",
    "\n",
    "Similarly, we typically treat this also just as one sentence:\n",
    "\n",
    "```\n",
    "They said: ”The San Francisco-based restaurant doesn’t charge $10\".\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565cdd6",
   "metadata": {},
   "source": [
    "### Spelling variants, typos, etc.\n",
    "\n",
    "The same word can be written in different ways:\n",
    "\n",
    "- with different `capitalizations`:\n",
    "  - lowercase “cat” (in standard running text)\n",
    "  - capitalized “Cat” (as first word in a sentence, or in titles/headlines),\n",
    "  - all-caps “CAT” (e.g. in headlines)\n",
    "- with different abbreviation or hyphenation styles:\n",
    "  - US-based, US based, U.S.-based, U.S. based\n",
    "  - US-EU relations, U.S./E.U. relations, …\n",
    "- with spelling variants (e.g. regional variants of English):\n",
    "  - labor vs labour, materialize vs materialise,\n",
    "- with typos (teh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d58bbc0",
   "metadata": {},
   "source": [
    "## How many different words are there in English?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5f64f",
   "metadata": {},
   "source": [
    "### Counting words: tokens vs. types\n",
    "\n",
    "When counting words in text, we distinguish between word types and word tokens:\n",
    "\n",
    "- The vocabulary of a language is the set of (unique) word types:\n",
    "  > V = {a, aardvark, …., zyzzva}\n",
    "- The tokens in a document include all occurrences of the word types in that document or corpus\n",
    "- The frequency of a word (type) in a document  \n",
    "  = the number of occurrences (tokens) of that type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705c650",
   "metadata": {},
   "source": [
    "How large is the vocabulary of English (or any other language)?\n",
    "\n",
    "- Vocabulary size = the number of distinct word types\n",
    "  > Google N-gram corpus: 1 trillion tokens, 13 million word types that appear 40+ times\n",
    "- You may have heard statements such as:\n",
    "  > `adults know about 30,000 words`\n",
    "  \n",
    "  > `you need to know at least 5,000 words to be fluent`\n",
    "\n",
    "If you count words in text, you will find that ...\n",
    "\n",
    "- a few words (mostly closed-class) are very frequent (the, be, to, of, and, a, in, that,…)\n",
    "- most words (all open class) are very rare.\n",
    "- even if you’ve read a lot of text, you will keep finding words you haven’t seen before. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e7cc9",
   "metadata": {},
   "source": [
    "### Zipf’s law: the long tail\n",
    "\n",
    "In a natural language:\n",
    "\n",
    "- A small number of events (e.g. words) occur with high frequency\n",
    "- A large number of events occur with very low frequency\n",
    "\n",
    "![](../figs/intro_nlp/tokenization/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7563b2a",
   "metadata": {},
   "source": [
    "#### Implications of Zipf’s Law for NLP\n",
    "\n",
    "The good:\n",
    "\n",
    "```\n",
    "Any text will contain a number of words that are very common.\n",
    "We have seen these words often enough that we know (almost) everything about them.\n",
    "These words will help us get at the structure (and possibly meaning) of this text.\n",
    "```\n",
    "\n",
    "The bad:\n",
    "\n",
    "```\n",
    "Any text will contain a number of words that are rare.\n",
    "We know something about these words, but haven’t seen them often enough to know everything about them.\n",
    "They may occur with a meaning or a part of speech we haven’t seen before.\n",
    "```\n",
    "\n",
    "The ugly:\n",
    "\n",
    "```\n",
    "Any text will contain a number of words that are unknown to us.\n",
    "We have never seen them before, but we still need to get at the structure (and meaning) of these texts.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f40ad7",
   "metadata": {},
   "source": [
    "#### Dealing with the bad and the ugly\n",
    "\n",
    "NLP systems need to be able to generalize from the known to the unknown.\n",
    "\n",
    "There are two main strategies:\n",
    "\n",
    "- Linguistic knowledge\n",
    "  - a finite set of grammatical rules is enough to generate an infinite number of languages\n",
    "- Machine learning or statistical methods\n",
    "  - learn representations of words from large amounts of data that often work well for unseen words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14416e2c",
   "metadata": {},
   "source": [
    "## How do we represent words?\n",
    "\n",
    "Option 1: Words are atomic symbols\n",
    "\n",
    "- Each (surface) word is a unique symbol\n",
    "- Add some generalization rules to map different surface forms to the same symbol\n",
    "  - `Normalization`: map all variants of the same word (form) to the same canonical variant \n",
    "    > e.g. lowercase everything, normalize spellings, perhaps spell-check)\n",
    "  - `Lemmatization`: map each word to its lemma (esp. in English, the lemma is still a word in the language, but lemmatized text is no longer grammatical)\n",
    "  - `Stemming`: remove endings that differ among word forms (no guarantee that the resulting symbol is an actual word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c0ac7",
   "metadata": {},
   "source": [
    "Option 2: Represent the structure of each word\n",
    "\n",
    "```\n",
    "\"books\" => \"book N pl\" (or \"book V 3rd sg\")\n",
    "```\n",
    "\n",
    "- This requries a morphological analyzer\n",
    "- The output is often a lemma (e.g. \"book\") and morphological features (e.g. \"N pl\" for noun plural, \"V 3rd sg\" for verb 3rd person singular)\n",
    "- This is particularly useful for languages with rich morphology (e.g. Turkish, Finnish, Hungarian, etc.)\n",
    "- Less useful for languages with little morphology (e.g. English, German, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8de82",
   "metadata": {},
   "source": [
    "## How do we represent unknown words?\n",
    "\n",
    "Many NLP systems assume a fixed vocabulary, but still have to handle out-of-vocabulary (OOV) words.\n",
    "\n",
    "Option 1: `the UNK token`\n",
    "- Replace all rare words (with a frequency at or below a given threshold, e.g. 2, 3, or 5) in your training data with an UNK token (UNK = “Unknown word”). \n",
    "- Replace all unknown words that you come across after training (including rare training words) with the same UNK token\n",
    "\n",
    "Option 2: `substring-based representations`\n",
    "- Often used in neural models\n",
    "- Represent (rare and unknown) words [“Champaign”] as sequences of characters [‘C’, ‘h’, ‘a’,…,’g’, ’n'] or substrings [“Ch”, “amp”, “ai”, “gn”]  \n",
    "- `Byte Pair Encoding (BPE)`: learn which character sequences are common in the vocabulary of your language, and treat those common sequences as atomic units of your vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ea27e",
   "metadata": {},
   "source": [
    "**Which words appear in this text?**\n",
    "\n",
    "```\n",
    "Of course he wants to take the advanced course too. He already took two beginners’ courses.\n",
    "```\n",
    "\n",
    "Actual text doesn’t consist of dictionary entries:\n",
    "\n",
    "- wants is a form of want\n",
    "- took is a form of take\n",
    "- courses is a form of course\n",
    "\n",
    "Linguists distinguish between\n",
    "\n",
    "- **the (surface) forms that occur in text**: want, wants, beginners’, took,…\n",
    "- **and the lemmas that are the uninflected forms of these words**: want, beginner, take, …\n",
    "\n",
    "In NLP, we sometimes map words to lemmas (or simpler “stems”), but the raw data always consists of surface forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14728924",
   "metadata": {},
   "source": [
    "## How many different words are there?\n",
    "\n",
    "`Inflection` creates different forms of the same word:\n",
    "\n",
    "- Verbs: to be, being, I am, you are, he is, I was,\n",
    "- Nouns: one book, two books\n",
    "\n",
    "`Derivation` creates different words from the same lemma:\n",
    "\n",
    "- grace ⇒ disgrace ⇒ disgraceful ⇒ disgracefully\n",
    "\n",
    "`Compounding` combines two words into a new word:\n",
    "\n",
    "- cream ⇒ ice cream ⇒ ice cream cone ⇒ ice cream cone bakery\n",
    "\n",
    "Word formation is productive:\n",
    "\n",
    "- New words are subject to all of these processes:\n",
    "  Google ⇒ Googler, to google, to ungoogle, to misgoogle, googlification, ungooglification, googlified, Google Maps, Google Maps service,...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c761e3",
   "metadata": {},
   "source": [
    "### Inflectional morphology in English\n",
    "\n",
    "**Verbs**:\n",
    "\n",
    "- Infinitive/present tense: walk, go\n",
    "- 3rd person singular present tense (s-form): walks, goes\n",
    "- Simple past: walked, went\n",
    "- Past participle (ed-form): walked, gone\n",
    "- Present participle (ing-form): walking, going\n",
    "\n",
    "**Nouns**:\n",
    "\n",
    "- Common nouns inflect for number:\n",
    "  singular (book) vs. plural (books)\n",
    "- Personal pronouns inflect for person, number, gender, case:\n",
    "  I saw him; he saw me; you saw her; we saw them; they saw us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9621a",
   "metadata": {},
   "source": [
    "### Derivational morphology in English\n",
    "\n",
    "Nominalization:\n",
    "\n",
    "- V + -ation: computerization\n",
    "- V+ -er: killer\n",
    "- Adj + -ness: fuzziness\n",
    "\n",
    "Negation:\n",
    "\n",
    "- un-: undo, unseen, ...\n",
    "- mis-: mistake,...\n",
    "\n",
    "Adjectivization:\n",
    "\n",
    "- V+ -able: doable\n",
    "- N + -al: national\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd5efb",
   "metadata": {},
   "source": [
    "### Morphemes: stems, affixes\n",
    "\n",
    "```\n",
    "   dis-grace-ful-ly\n",
    "prefix-stem-suffix-suffix\n",
    "```\n",
    "\n",
    "Many word forms consist of a `stem` plus a number of `affixes (prefixes or suffixes)`\n",
    "\n",
    "- Exceptions: Infixes are inserted inside the stem Circumfixes (German gesehen) surround the stem\n",
    "\n",
    "`Morphemes`: the smallest (meaningful/grammatical) parts of words.\n",
    "\n",
    "- Stems (grace) are often `free morphemes`.\n",
    "  Free morphemes can occur by themselves as words.\n",
    "- Affixes (dis-, -ful, -ly) are usually `bound morphemes`.\n",
    "  Bound morphemes have to combine with others to form words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c4ca4",
   "metadata": {},
   "source": [
    "**Morphemes and morphs**\n",
    "\n",
    "The same information (plural, past tense, ...) is often expressed in different ways in the same language.\n",
    "\n",
    "- One way may be more common than others, and exceptions may depend on specific words:\n",
    "  - Most plural nouns: add **-s** to singular: book-book**s**, but: box-box**es**, fly-fl**ies**, child-child**ren**\n",
    "  - Most past tense verbs add **-ed** to infinitive: walk-walk**ed**, but: like-like**d**, leap-leap**t**\n",
    "    Such exceptions are called `irregular word forms`\n",
    "\n",
    "Linguists say that there is `one underlying morpheme` (e.g. for plural nouns) that is “realized” as different “surface” forms (morphs) (e.g. -s/-es/-ren)\n",
    "\n",
    "- Allomorphs: two different realizations (-s/-es/-ren) of the same underlying morpheme (plural)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40e36e",
   "metadata": {},
   "source": [
    "**“Surface”?**\n",
    "\n",
    "This terminology comes from Chomskyan Transformational Grammar.\n",
    "\n",
    "- Dominant early approach in theoretical linguistics, superseded by other approaches (“minimalism”).\n",
    "- Not computational, but has some historical influence on computational linguistics (e.g. Penn Treebank)\n",
    "\n",
    "`“Surface”` = standard English (Chinese, Hindi, etc.).\n",
    "\n",
    "- `“Surface string”` = a written sequence of characters or words\n",
    "\n",
    "vs. `“Deep”/“Underlying”` structure/representation:\n",
    "\n",
    "- A more abstract representation.\n",
    "- Might be the same for different sentences/words with the same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aec77a",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### Input:\n",
    "\n",
    "- A set of documents (e.g. text files), $D$\n",
    "\n",
    "### Output (tokens):\n",
    "\n",
    "- A sequence, $W$ , containing a list of tokens – words or word pieces for use in natural language processing\n",
    "\n",
    "### Output (n-grams):\n",
    "\n",
    "- A matrix, $X$, containing statistics about word/phrase frequencies in those documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815632b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
