{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc871e8",
   "metadata": {},
   "source": [
    "# Getting Started with Bloom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b679041",
   "metadata": {},
   "source": [
    "## Transparency, openness, and inclusivity\n",
    "\n",
    "While most major LLMs have been trained exclusively on English text, BLOOM’s training corpus includes 46 natural languages and 13 programming languages. This makes it useful for the many regions where English is not the main language.\n",
    "\n",
    "BLOOM is also a break from the de facto reliance on big tech to train models. One of the main problems of LLMs is the prohibitive costs of training and tuning them. This hurdle has made 100-billion-parameter LLMs the exclusive domain of big tech companies with deep pockets. Recent years have seen AI labs gravitate toward big tech to gain access to subsidized cloud compute resources and fund their research.\n",
    "\n",
    "The BLOOM research team has been completely transparent about the entire process of training the model. They have published the dataset, the meeting notes, discussions, and code, as well as the logs and technical details of training the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3e881",
   "metadata": {},
   "source": [
    "## Zero Shot and Few Shot Learners\n",
    "\n",
    "Large Language Models can show good enough performance for some tasks based on just a few examples.\n",
    "These examples are called `prompts` to a language model.\n",
    "\n",
    "For clarity, we will define a prompting task as one that requires no fine-tuning to the base language model. \n",
    "This is done by inputting some prompts into the language model and asking it to return a response. \n",
    "The model does not see any training data for this task and is expected to generalize from these few examples. \n",
    "\n",
    "Formatting the examples as input is referred to as `prompt engineering` and is a process that comes with some trial and error.\n",
    "The goal of prompt engineering is to take your prompts and format them in a way, so they are easy to input into the model.\n",
    "\n",
    "Language generation based on prompts is a brilliant concept, and it can be done in two ways, mainly - Zero Shot predictions and Few Shot predictions. \n",
    "Zero-Shot prediction is where the model is not trained on any data for that specific task, and Few Shot predictions are where the model is trained on a very few amount of data for that specific task. \n",
    "In both cases, we need some sort of prompt or seed text to get started with so that the model can generate new text from there. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3564e",
   "metadata": {},
   "source": [
    "\n",
    "## Prompting on LLMs\n",
    "\n",
    " All you need to do is input several examples of your prompts into the `generate` function on either `gpt2` or `gpt3`. You don't even have to specify what kind of task it is, just give it some example inputs and let it figure out how to generalize from there! The only parameters required are:\n",
    "\n",
    " - `text`, which contains the text you want to be generated for (this should be your prompt set)  \n",
    " - `length`, which specifies how long you want each generated sequence returned by the model   \n",
    " - `num_return_sequences`, which specifies how many sequences you want to be returned by the model\n",
    "\n",
    "In Zero-shot predictions, you mainly pass prompts which give a task description to the LLM to generate text. For example, for zero-shot summarization, you can present a body of text to the LLM along with an instruction for it to follow, like 'In summary', or 'tldr:', or even 'To explain to a 5-year-old'.\n",
    "\n",
    "In Few-Shot summarization, you can preset a few examples of text & their summary to an LLM. You can then present a text to the model and could expect the summary generated by the model. In other words, you give it a few examples vs. none.\n",
    "Language generation based on prompts is a brilliant concept and it is so much simpler than fine-tuning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e04ef0",
   "metadata": {},
   "source": [
    "### Zero-shot \n",
    "\n",
    "The model predicts the answer when provided only a description of the task. \n",
    "No gradient updates are performed on the model. \n",
    "\n",
    "- prompt => Translate English to French: (This is the task description)\n",
    "- Cheese => (this is you prompting the LLM to complete the sentence)\n",
    "\n",
    "### One-shot\n",
    "\n",
    "In addition to task description, you provide the model with one example of what you are expecting it to produce. \n",
    "\n",
    "- prompt => Translate English to French: (Task description for the model)\n",
    "- Sea Otter => loutre de mer (One example for the model to learn from)\n",
    "- Cheese => (providing a prompt to LLM to follow the lead)\n",
    "\n",
    "### Few-shot\n",
    "\n",
    "On addition to task description, the model is provided with a few examples of the task. \n",
    "\n",
    "- prompt => Translate English to French: (Task description for the model)\n",
    "- Sea Otter => loutre de mer (a few examples for the model to learn from)\n",
    "- Plush girafe => girafe poivree\n",
    "- Cheese => (providing a prompt to LLM to follow the lead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86e4aefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilgpt2 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Using pad_token, but it is not set yet.\n",
      "Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'One of the hottest areas of investing in recent years has been ESG',\n",
       " 'labels': ['business', 'politics', 'education'],\n",
       " 'scores': [0.43735742568969727, 0.2975316643714905, 0.26511093974113464]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"distilgpt2\")\n",
    "classifier(\n",
    "    \"One of the hottest areas of investing in recent years has been ESG\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a90025",
   "metadata": {},
   "source": [
    "## Zero Shot Reasoners and Chain-of-Thought\n",
    "\n",
    "The [paper](https://arxiv.org/abs/2205.11916) from University of Tokyo and Google Brain team suggests that LLMs have fundamental zero-shot capabilities in high-level broad cognitive tasks and that these capabilities can be extracted by simple Chain-of-Thought (or CoT) prompting.\n",
    "\n",
    "Another [paper](https://arxiv.org/abs/2201.11903) by Google Brain team has further investigated the CoT prompting. They noted that by generating a chain-of-thought (or a series of intermediate reasoning steps) LLMs significantly improve their ability to perform complex reasoning. Their experiments on three large language models have shown that chain-of-thought prompting improves performance on a range of arithmetic, common sense, and symbolic reasoning tasks.\n",
    "\n",
    "One exmaple:\n",
    "\n",
    "- Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis  balls. How many tennis balls does he have now? \n",
    "\n",
    "- A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Additionally: \n",
    "\n",
    "- Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have\n",
    "\n",
    "- A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.\n",
    "\n",
    "> Chain of thought reasoning allows models to decompose complex problems into intermediate steps that are solved individually. Moreover, the language-based nature of chain of thought makes it applicable to any task that a person could solve via language. We find through empirical experiments that chain of thought prompting can improve performance on various reasoning tasks, and that successful chain of thought reasoning is an emergent property of model scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db896a5b",
   "metadata": {},
   "source": [
    "## Bloom Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3afe2",
   "metadata": {},
   "source": [
    "## Downloading a Pre-Trained Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d9504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133889f36f1d4e019278e472c2b7cca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00024-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466b2b4024ac4bef967615896651ce0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00025-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50906c96442f4026989bf623f57e86be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00026-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1376aad9db5243b1ad2e8cdfeb004b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00027-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6848bc8816184e588b8a75ec3600e327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00028-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc202e5265ad4f99a8a106f301a640c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00029-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4356872f95db4eabb3d6537eb9d4d935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00030-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9f34cf0a0746029e580fe6c4869752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00031-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b545b9877e941a183557e2c15f3d561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00032-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1311eb2d33d74412b6064c60246558dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00033-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338f195b6d63437a9be939325f2be3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model_00034-of-00072.bin:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\")\n",
    "model = AutoModel.from_pretrained(\"bigscience/bloom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52130105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "s = time.time()\n",
    "pipe = pipeline(model=\"bigscience/bloom\", torch_dtype=torch.bfloat16)\n",
    "print(f\"Time to load model: {time.time()-s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c28c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rom IPython.display import HTML as html_print\n",
    "\n",
    "def cstr(s, color='black'):\n",
    "    #return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s.replace('\\n', '<br>'))\n",
    "\n",
    "def cstr_with_newlines(s, color='black'):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s.replace('\\n', '<br>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce5fa2",
   "metadata": {},
   "source": [
    "Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\n",
    "A: Let’s think step by step.\n",
    "Let $B \\overset{\\text{ref}}{=}$ blue golf balls.\n",
    "Since half of the balls are a few foreign yones distinctive of Chinese name cosmogonies but are China a unique case have variations been overlooked do exist Thesis method to cover all ages according which are almost but in the former case but again with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fa2fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"One of the hottest areas of investing in recent years has been ESG: \"\n",
    "promtt += \"the use of environmental, social, and governance criteria to evaluate possible investments.\"\n",
    "\n",
    "result_length = 100\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1fac6d",
   "metadata": {},
   "source": [
    "- `result_length` calibrates the size of the response (in tokens) we get for the prompt from the model.\n",
    "- `inputs` contains the embedding representation of prompt, encoded for use specifically by PyTorch. If we were using TensorFlow we’d pass return_tensors=\"tf\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fccdd38",
   "metadata": {},
   "source": [
    "## Running Inference: Strategies for Better Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f904f1",
   "metadata": {},
   "source": [
    "## Decoding / search strategies\n",
    "\n",
    "With autoregressive transformers (trained for next token prediction) we have a number of options to search the answer space for the most “reasonable” output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4cc136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the hottest areas of investing in recent years has been ESG: the use of environmental, social, and governance criteria to evaluate possible investments. The ESG movement has been growing steadily since the financial crisis, and the number of companies that have adopted ESG criteria has increased by more than 50% in the last five years. The ESG movement is also growing in the United States, where the number of companies that have adopted ESG criteria has increased by more than 50% in the\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(inputs[\"input_ids\"], max_length=result_length)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_inf(prompt, temperature=0.7, top_p=None, max_new_tokens=32, repetition_penalty=None, do_sample=False, num_return_sequences=1):  \n",
    "    response = pipe(f\"{prompt}\", \n",
    "                    temperature = temperature, # 0 to 1\n",
    "                    top_p = top_p, # None, 0-1\n",
    "                    max_new_tokens = max_new_tokens, # up to 2047 theoretically\n",
    "                    return_full_text = False, # include prompt or not.\n",
    "                    repetition_penalty = repetition_penalty, # None, 0-100 (penalty for repeat tokens.\n",
    "                    do_sample = do_sample, # True: use sampling, False: Greedy decoding.\n",
    "                    num_return_sequences = num_return_sequences\n",
    "                    )\n",
    "    return html_print(cstr(prompt, color='#f1f1c7') + cstr(response[0]['generated_text'], color='#a1d8eb')), response[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"\"\"# Use OpenCV in Python\"\"\"\n",
    "color_resp, resp = local_inf(inp, max_new_tokens=64)\n",
    "color_resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c95f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "240ec383",
   "metadata": {},
   "source": [
    "- Greedy Search \n",
    "  - simply chooses the next word at each timestep t+1 that has the highest predicted probability of following the word at t. \n",
    "  - One of the main issues here is that greedy search will miss words with a high probability at t+1 if it is preceded by a word with a low probability at t.\n",
    "- Beam Search \n",
    "  - keeps track of the n-th (num_beams) most likely word sequences and outputs the most likely sequence. \n",
    "  - Sounds great, but this method breaks down when the output length can be highly variable — as in the case of open-ended text generation. \n",
    "  - Both greedy and beam search also produce outputs whose distribution does not align very well with the way humans might perform the same task (i.e. both are liable to produce fairly repetitive, boring text).\n",
    "- Sampling With Top-k + Top-p\n",
    "  - a combination of three methods. \n",
    "  - By sampling, we mean that the next word is chosen randomly based on its conditional probability distribution (von Platen, 2020). \n",
    "  - In Top-k, we choose the k most likely words, and then redistribute the probability mass amongst them before the next draw. \n",
    "  - Top-p adds an additional constraint to top-k, in that we’re choosing from the smallest set of words whose cumulative probability exceed p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e5692",
   "metadata": {},
   "source": [
    "## [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)\n",
    "\n",
    "by Patrick von Platen (Huggingface)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b7002",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f869af7787e6a1c49e09e367fc6e1b81d93d1c6583b43249c80edc047bd13cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
