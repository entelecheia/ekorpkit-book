{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc871e8",
   "metadata": {},
   "source": [
    "# BLOOM\n",
    "\n",
    "**BigScience Large Open-science Open-access Multilingual Language Model**\n",
    "\n",
    "![bloom](../figs/deepnlp_2_bloom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdf646",
   "metadata": {},
   "source": [
    "## What is BLOOM?\n",
    "\n",
    "- BLOOM is a 175-billion parameter model for language processing, able to generate text much like GPT-3 and OPT-175B. \n",
    "- It was developed to be multilingual, being deliberately trained on datasets containing 46 natural languages and 13 programming languages.\n",
    "- Unlioke GPT-3, BLOOM is open-access, meaning anyone is able to download and use BLOOM for themselves. \n",
    "- Everything about BLOOM is openly available on the various pages within BigScience's Hugging Face page, from the training logs to models of various sizes to checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3afe2",
   "metadata": {},
   "source": [
    "## Model Details  \n",
    "\n",
    "- BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. \n",
    "- As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. \n",
    "- BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57faf42b",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "**Developed by:** BigScience ([website](https://bigscience.huggingface.co))\n",
    "\n",
    "*All collaborators are either volunteers or have an agreement with their employer. (Further breakdown of participants forthcoming.)*\n",
    "    \n",
    "**Model Type:** Transformer-based Language Model\n",
    "**Checkpoints format:** `transformers` (Megatron-DeepSpeed format available [here](https://huggingface.co/bigscience/bloom-optimizer-states))\n",
    "\n",
    "**Version:** 1.0.0\n",
    "\n",
    "**Languages:** Multiple; see [training data](#training-data)\n",
    "\n",
    "**License:** RAIL License v1.0 ([link](https://huggingface.co/spaces/bigscience/license) / [article and FAQ](https://bigscience.huggingface.co/blog/the-bigscience-rail-license))\n",
    "\n",
    "**Release Date Estimate:** Monday, 11.July.2022\n",
    "\n",
    "**Send Questions to:** bigscience-contact@googlegroups.com\n",
    "\n",
    "**Cite as:** BigScience, _BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model_. International, May 2021-May 2022\n",
    "\n",
    "**Funded by:** \n",
    "    \n",
    "* The French government.\n",
    "* Hugging Face ([website](https://huggingface.co))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a71ff",
   "metadata": {},
   "source": [
    "## Technical Specifications\n",
    "\n",
    "### Model Architecture and Objective\n",
    "\n",
    "* Modified from Megatron-LM GPT2 (see [paper](https://arxiv.org/abs/1909.08053), [BLOOM Megatron code](https://github.com/bigscience-workshop/Megatron-DeepSpeed)):\n",
    "\n",
    "* Decoder-only architecture\n",
    "\n",
    "* Layer normalization applied to word embeddings layer (`StableEmbedding`; see [code](https://github.com/facebookresearch/bitsandbytes), [paper](https://arxiv.org/pdf/2110.02861.pdf))\n",
    "\n",
    "* ALiBI positional encodings (see [paper](https://arxiv.org/pdf/2108.12409.pdf)), with GeLU activation functions\n",
    "\n",
    "* 176 billion parameters:\n",
    "\n",
    "    * 70 layers, 112 attention heads\n",
    "\n",
    "    * Hidden layers are 14336-dimensional\n",
    "\n",
    "    * Sequence length of 2048 tokens used (see [BLOOM tokenizer](https://huggingface.co/bigscience/tokenizer), [tokenizer description](#tokenization))\n",
    "\n",
    "**Objective Function:** Cross Entropy with mean reduction (see [API documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8899f93a",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### Compute infrastructure\n",
    "Jean Zay Public Supercomputer, provided by the French government (see [announcement](https://www.enseignementsup-recherche.gouv.fr/fr/signature-du-marche-d-acquisition-de-l-un-des-supercalculateurs-les-plus-puissants-d-europe-46733)).\n",
    "#### Hardware\n",
    "\n",
    "* 384 A100 80GB GPUs (48 nodes)\n",
    "    \n",
    "* Additional 32 A100 80GB GPUs (4 nodes) in reserve\n",
    "* 8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath links\n",
    "\n",
    "* CPU: AMD\n",
    "\n",
    "* CPU memory: 512GB per node\n",
    "\n",
    "* GPU memory: 640GB per node\n",
    "\n",
    "* Inter-node connect: Omni-Path Architecture (OPA)\n",
    "\n",
    "* NCCL-communications network: a fully dedicated subnet\n",
    "\n",
    "* Disc IO network: shared network with other types of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfa21b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
