{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# T5: Text-To-Text Transfer Transformer\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef336780",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer” {cite}`raffel2020exploring`\n",
    "\n",
    "- A unified framework that converts all text-based language problems into a text-to-text format by framing them as conditional text generation tasks.\n",
    "- Combining the pre-training objectives of BERT and GPT-2, T5 is trained on a very large number of tasks and is able to perform well on a wide range of tasks with minimal task-specific architecture modifications.\n",
    "- C4 (Corpus of Cleaned Web Crawled Text) is used as the training corpus, which is a large-scale dataset of 3.3 billion web pages.\n",
    "- Achieves state-of-the-art results on 11 out of 15 tasks in GLUE, SuperGLUE, and SQuAD v1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ac5b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## T5: Text-to-Text Framework\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0924d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Unified Input & Output Format\n",
    "\n",
    "- T5 means \"`T`ext-`T`o-`T`ext `T`ransfer `T`ransformer\".\n",
    "- Every task considered by T5 is framed as a conditional text generation task with a single input and output sequence.\n",
    "- Translation: `translate English to German: How old are you?` $\\rightarrow$ `Wie alt bist du?`\n",
    "- Text classification: `classify sentiment: This movie is so bad.` $\\rightarrow$ `negative`\n",
    "- Text summarization: `summarize: The movie was not good. The animation and the graphics were good. This is a good movie.` $\\rightarrow$ `The movie was not good.`\n",
    "- MNLI (entailment): `mnli: Premise: A person on a horse jumps over a broken down airplane. Hypothesis: The person is training his horse for a competition.` $\\rightarrow$ `entailment`\n",
    "- MNLI (neutral): `mnli: Premise: A person on a horse jumps over a broken down airplane. Hypothesis: The person is at the zoo, riding a horse.` $\\rightarrow$ `neutral`\n",
    "- Regression: `sts-b: The cat was playing in the garden. The cat was playing in the yard.` $\\rightarrow$ `5.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ddb5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## C4: Colossal Clean Crawled Corpus\n",
    "\n",
    "### [Common Crawl](https://commoncrawl.org/)\n",
    "\n",
    "- Common Crawl is a web crawl data repository that contains petabytes of data collected from the public web.\n",
    "- It is a non-profit organization that collects data from the public web and makes it freely available to the research community.\n",
    "- It produces around 30TB of data per month.\n",
    "- However, Common Crawl contains large amounts of noisy data, such as error messages, spam, and duplicate content.\n",
    "- To address this problem, the authors of T5 use the C4 dataset, which is a cleaned version of Common Crawl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc4496",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Colossal Clean Crawled Corpus\n",
    "\n",
    "For C4, the authors took Common Crawl scrape from April 2019 and applied some cleansing filters on it:\n",
    "\n",
    "1. Only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark).\n",
    "2. Discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words.\n",
    "3. Removed any page that contained any word on the `List of Dirty, Naughty, Obscene or Otherwise Bad Words`.\n",
    "4. Removed any line with the word `Javascript`.\n",
    "5. Removed any page where the phrase `lorem ipsum` appeared.\n",
    "6. Removed any pages that contained a curly bracket.\n",
    "7. Deduplicate the data set, discarded all but one of any three-sentence span occurring more than once in the data set.\n",
    "8. Since most of the downstream tasks are focused on English-language text, langdetect is used to filter out any pages that were not classified as English with a probability of at least 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc4a17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- The filtered dataset is larger than most data sets used for pre-training (about 750 GB).\n",
    "- It also comprises reasonably clean and natural English text. \n",
    "- This data set is called the “Colossal Clean Crawled Corpus” (or C4 for short) and released as part of TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27451162",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The Model\n",
    "\n",
    "### Encoder-Decoder Transformer Model\n",
    "\n",
    "- T5 uses the same encoder-decoder Transformer architecture as BERT.\n",
    "- However, a simplified layer normalization is used the activations are only rescaled and no additive bias is applied.\n",
    "- After layer normalization, a residual skip connection, originated from ResNet, adds each subcomponent's input to its output.\n",
    "- Also, instead of using a fixed positional encoding, T5 uses a relative positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a88c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Baseline\n",
    "\n",
    "- The baseline model is designed so that the encoder and decoder are each similar in size and configuration to a BERT-base model.\n",
    "- Specifically, the encoder and decoder each have 12 layers, with about 220 million parameters.\n",
    "- The objective of the baseline model is to predict the dropped-out tokens in the input sequence.\n",
    "\n",
    "![T5 Baseline](../figs/deep_nlp/t5/t5-baseline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a082b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A Systematic Study of Transfer Learning Methodology\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-strategies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce893cb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- `model architectures`, where we found that encoder-decoder models generally outperformed \"decoder-only\" language models;\n",
    "- `pre-training objectives`, where we confirmed that fill-in-the-blank-style denoising objectives (where the model is trained to recover missing words in the input) worked best and that the most important factor was the computational cost;\n",
    "- `unlabeled datasets`, where we showed that training on in-domain data can be beneficial but that pre-training on smaller datasets can lead to detrimental overfitting;\n",
    "- `training strategies`, where we found that multitask learning could be close to competitive with a pre-train-then-fine-tune approach but requires carefully choosing how often the model is trained on each task;\n",
    "- and `scale`, where we compare scaling up the model size, the training time, and the number of ensembled models to determine how to make the best use of fixed compute power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52041854",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Architectures\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bc58a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "At an architectural level, there are several options in selecting the training approach:\n",
    "\n",
    "1. Encoder-Decoder:\n",
    "   - This is the standard encoder-decoder Transformer architecture.\n",
    "   - Encoder is trained a BERT-style, fully visible language modeling objective. (i.e. every token contributes to the attention calculation of every other token)\n",
    "   - Decoder is trained in a GPT-style, causal language modeling objective. (i.e. every token contributes to the attention calculation of every token that appears before it)\n",
    "2. Language Model: This is essentially the causal attention language modeling objective.\n",
    "3. Prefix Language Model: This is a combination of the BERT-style and language model approaches. For example, the taks of translating from English to German is framed as a prefix language model task, where the input `translate English to German:` attended in BERT-style, and the output `Wie alt bist du?` is attended autoregressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf29cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![](../figs/deep_nlp/t5/t5-mask-patterns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f8c43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![](../figs/deep_nlp/t5/t5-architectures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248642e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Unsupervised Objective\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-unsupervised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a5f9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "With respect to the pre-training objective, authors have tested the following:\n",
    "\n",
    "1. Language Modeling: This is the standard language modeling objective, where the model is trained to predict the next token in a sequence.\n",
    "2. Deshuffling: This is a variant of language modeling, where the model is trained to predict the original order of a shuffled sequence.\n",
    "3. Corrupting Spans: Masking a sequence of tokens and training the model to predict these masked tokens.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-unsupervised-obj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0feec8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The BERT-style objective performs best, though the prefix language modeling objective attains similar performance on the translation tasks.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-unsupervised-obj-perf1.png)\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-unsupervised-obj-perf2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a3bf5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "###  Corruption Rates\n",
    "\n",
    "- Using a larger corruption rate results in longer targets, which can potentially slow down training.\n",
    "- Based on the results and the historical precedent set by BERT, a corruption rate of 15% is used going forward.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-corruption-rates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c473bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Corrupted Span Length\n",
    "\n",
    "- When multiple consecutive tokens have been corrupted, they are treated as a “span” and a single unique mask token is used to replace the entire span.\n",
    "- A limited difference is found between these objectives, though the version with an average span length of 10 slightly underperforms the other values in some cases.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-corrupted-span-length.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6411e9d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Unlabeled Datasets\n",
    "\n",
    "Different pretraining datasets are tried.\n",
    "\n",
    "- C4: The one mentioned in Section 2 in this story article.\n",
    "- C4, unfiltered: C4 but without filtering, to know the effect of the heuristic filtering\n",
    "- RealNews-like: C4 but only include content from one of the domains used in the “RealNews” data set.\n",
    "- WebText-like: Similarly, the WebText data set only uses content from webpages.\n",
    "- Wikipedia: English Wikipedia text data from TensorFlow Datasets.\n",
    "- Wikipedia+TBC: Toronto Books Corpus (TBC) contains text extracted from eBooks, combining with Wikipedia following BERT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c342e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Pre-training on in-domain unlabeled data can improve performance on downstream tasks. (e.g.: unlabled news data helps downstream news dataset.) But this is unsatisfying if the goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary domains.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-unlabeled-datasets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb513b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Unlabeled Dataset Sizes\n",
    "\n",
    "- C4 has $2^{35}$= 34B tokens.\n",
    "- Truncated variants of C4 consisting of $2^{29}$, $2^{27}$, $2^{25}$ and $2^{23}$ tokens. \n",
    "- These sizes correspond to repeating the data set 64, 256, 1,024, and 4,096 times respectively over the course of pre-training.\n",
    "- As expected, performance degrades as the data set size shrinks.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-unlabeled-dataset-sizes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0e1a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Fine-Tuning Methods\n",
    "\n",
    "- Fine-tuning all parameters.\n",
    "- Adapter Layers: keeping most of the original model fixed while fine-tuning. Adapter layers are additional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward networks in each block of the Transformer.\n",
    "- Gradual Freezing: More and more of the model’s parameters are finetuned over time. At the start of fine-tuning, only the parameters of the final layer are updated, then after training for a certain number of updates the parameters of the second-to-last layer are also included, and so on until the entire network’s parameters are being fine-tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738733d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "It is found that gradual unfreezing caused a minor degradation in performance across all tasks, though it did provide some speedup during fine-tuning.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-fine-tuning-methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeee28a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Multi-Task Learning\n",
    "\n",
    "- Multi-task learning is to train the model on multiple tasks at a time. \n",
    "- Multi-task learning underperforms pre-training followed by fine-tuning on most tasks.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-multi-task-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f53b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Scaling\n",
    "\n",
    "- There are a variety of possible ways to scale, including using a bigger model, training the model for more steps, and ensembling.\n",
    "- There is no large difference between training a 2x bigger model for 2x as long and training a 4x bigger model on any of the tasks.\n",
    "- This suggests that increasing the training time and increasing the model size can be complementary means of improving performance. \n",
    "- The results also suggest that ensembling provides an orthogonal and effective means of improving performance through scale.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-scaling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de9d28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### SOTA Comparisons\n",
    "\n",
    "- Small, Base, Large, 3B, and 11B refer to model configurations with 60 million, 220 million, 770 million, 3 billion, and 11 billion parameters, respectively. (by tuning different hyperparameters.)\n",
    "- Overall, state-of-the-art performance is achieved on 18 out of the 24 tasks.\n",
    "- T5–3B model variant did beat the previous state of the art in a few tasks, but scaling the model size to 11 billion parameters was the most important ingredient for achieving the best performance.\n",
    "- For SQuAD, T5 outperformed the previous state-of-the-art ALBERT by over one point on the Exact Match score.\n",
    "- For SuperGLUE, T5 improved upon the state-of-the-art RoBERTa by a large margin from an average score of 84.6 to 88.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb97bf2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![](../figs/deep_nlp/t5/t5-sota-comparisons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8f83b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Further experiment is performed on three configurations as above:\n",
    "\n",
    "1. The standard baseline model, which was pre-trained on 2²⁵=34B tokens.\n",
    "2. Baseline-1T: The baseline trained instead for about 1 trillion tokens (i.e. the same amount of pre-training used for T5).\n",
    "3. T5-Base.\n",
    "\n",
    "T5-Base performs substantially better than Baseline-1T, suggesting that scale is not the only factor that contributes to T5’s success.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-sota-comparisons-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)\n",
    "- [T5 by huggingface](https://huggingface.co/docs/transformers/model_doc/t5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cd8c41",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
