{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjgz1WTcPWee"
   },
   "source": [
    "# Lab 3: Training Tokenizers\n",
    "\n",
    "![](../figs/deep_nlp/lab/tokenize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ekorpkit[wiki,fetch]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --pre ekorpkit[tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BzxPwsOVPWef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.utils.notebook:Google Colab not detected.\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_WORKSPACE_ROOT to /content/drive/MyDrive/workspace/\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_PROJECT to ekorpkit-book\n",
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0.1.40.post0.dev16\n",
      "is colab? False\n",
      "project_dir: /content/drive/MyDrive/workspace/projects/ekorpkit-book\n",
      "time: 1.56 s (started: 2022-11-12 06:30:45 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%load_ext autotime\n",
    "\n",
    "from ekorpkit import eKonf\n",
    "\n",
    "eKonf.setLogger(\"INFO\")\n",
    "print(\"version:\", eKonf.__version__)\n",
    "\n",
    "is_colab = eKonf.is_colab()\n",
    "print(\"is colab?\", is_colab)\n",
    "if is_colab:\n",
    "    eKonf.mount_google_drive()\n",
    "project_dir = eKonf.set_workspace(workspace=\"/content/drive/MyDrive/workspace/\", project=\"ekorpkit-book\")\n",
    "print(\"project_dir:\", project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "time: 20.8 ms (started: 2022-11-12 07:16:33 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it1Y0MVPXRsJ"
   },
   "source": [
    "### Load the saved corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.io.file:Processing [1] files from ['enko_filtered.parquet']\n",
      "INFO:ekorpkit.io.file:Loading 1 dataframes from ['/content/drive/MyDrive/workspace/projects/ekorpkit-book/data/enko_filtered.parquet']\n",
      "INFO:ekorpkit.io.file:Loading data from /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/enko_filtered.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.54 s (started: 2022-11-12 06:30:47 +00:00)\n"
     ]
    }
   ],
   "source": [
    "data = eKonf.load_data(\"enko_filtered.parquet\", project_dir + \"/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert pandas datafame to huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 603719\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 s (started: 2022-11-12 06:30:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "text_column = \"text\"\n",
    "raw_dataset = Dataset.from_pandas(data[[text_column]])\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eylex Films Pvt is a chain of multiplex and single screen theatres. Eylex films pioneered the multiplex model in Jharkhand and Bihar. Its first multiplex was established in Ranchi in 2007 – the first Multiplex in the city of Ranchi and States like Jharkhand, Bihar, West Bengal, Odisha and Assam.\\nAt present the chain operates 24 screens in cities like Asansol, Deoghar, Ranchi, Jamshedpur, Muzaffarpur, Sambalpur, Jharsuguda and Silchar. The chain is aggressively looking forward to expand its footprint across India.\\nChain of theatres.\\nThough the initial plan was to build one multiplex Eylex, the positive response from the cine viewers led to the launch of Eylex in Ranchi, Jamshedpur, Sambalpur, Deoghar, Asansol and Silchar, and newly opened DRB Palace Motijheel, Muzaffarpur.\\nFilm Production.\\nEylex Films ventured into film production in 2016. The first film produced by them is Mandobasar Galpo, a Bengali film, scheduled to release on 24 March 2017. The film has been directed by Tathagata Banerjee and produced by Akash Jalan and Vickash Chowdhury. The film stars Parambrata Chattopadhyay, Paoli Dam, Anindya Chatterjee, Indrasish Roy and Kaushik Sen.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.17 ms (started: 2022-11-12 06:30:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "text_en = raw_dataset[2][text_column].split(\"\\n\")\n",
    "text_ko = raw_dataset[5555][text_column].split(\"\\n\")\n",
    "\n",
    "print(text_en)\n",
    "print(text_ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 146 ms (started: 2022-11-12 06:40:29 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# shuffle the dataset\n",
    "\n",
    "raw_dataset = raw_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizers with Hugging Face Tokenizers\n",
    "\n",
    "[Hugging Face's Tokenizers](https://huggingface.co/docs/tokenizers/quicktour) provides a wide range of tokenizers, including BPE, WordPiece, Unigram, SentencePiece, and ByteLevel. We will use the BPE and Unigram tokenizers in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries and prepare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.35 ms (started: 2022-11-12 06:51:08 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "from ekorpkit.tokenizers.spm import batch_chunks\n",
    "\n",
    "\n",
    "unk_token = \"<UNK>\"  # token for unknown words\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\", \"[MASK]\"]  # special tokens\n",
    "\n",
    "def prepare_tokenizer_trainer(algo):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if algo == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
    "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
    "    elif algo == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
    "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
    "\n",
    "         \n",
    "    normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    return tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 577 µs (started: 2022-11-12 06:53:10 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def train_tokenizer(algo=\"WLV\"):\n",
    "    \"\"\"\n",
    "    Takes the files and trains the tokenizer.\n",
    "    \"\"\"\n",
    "    save_path = f\"{project_dir}/tokenizers/{algo}_tokenizer.json\"\n",
    "    tokenizer, trainer = prepare_tokenizer_trainer(algo)\n",
    "    tokenizer.train_from_iterator(\n",
    "        batch_chunks(raw_dataset, batch_size=1000, text_column=text_column),\n",
    "        trainer=trainer,\n",
    "    )\n",
    "    tokenizer.save(save_path)\n",
    "    tokenizer = Tokenizer.from_file(save_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = train_tokenizer(\"BPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> time: 15h 30min 58s (started: 2022-11-11 06:36:01 +00:00)\n",
    "\n",
    "To train a BPE tokenizer, it took 15 hours and 30 minutes for 603,719 wiki articles. The tokenizer was saved in the `{project_dir}/tokenizers` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "['임', '서', '준', '(', '林', '序', '准', ',', '1993년', '4월', '19일', '~', ')', '은', '전', 'KBO', '리그', 'NC', '다이', '노', '스의', '투수이다', '.', '개명', '전', '이름은', \"'\", '임', '인', '혁', \"'\", '이다', '.', 'NC', '다이', '노스', '시절', '.', '2016년', '6월', '30일', '두산', '베어스', '와의', '경기에서', '선발', '등판', '하며', '데뷔', '첫', '경기를', '치렀고', ',', '경기에서', '2', '.', '2', '이닝', '2', '실', '점을', '기록하며', '패', '전', '투', '수가', '됐다', '.', '경찰', '야구', '단', '시절', '.', '2017년에', '입단하였다', '.', 'NC', '다이', '노스', '복귀', '.', '2018년에', '복귀하였다', '.']\n",
      "time: 43.4 ms (started: 2022-11-12 07:10:01 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = f\"{project_dir}/tokenizers/BPE_tokenizer.json\"\n",
    "\n",
    "bpe_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(f\"Vocab size: {bpe_tokenizer.get_vocab_size()}\")\n",
    "print(bpe_tokenizer.encode(raw_dataset[5555][\"text\"]).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Unigram tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tokenizer = train_tokenizer(\"UNI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took more time to train a Unigram tokenizer than a BPE tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizers with Google SentencePiece (SPM)\n",
    "\n",
    "### Install SentencePiece\n",
    "\n",
    "```bash\n",
    "pip install sentencepiece\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into sentences for training\n",
    "\n",
    "The sentencepiece module comes with a python training API, which uses sentences in a file, one sentence per line. We will use the `sent_tokenize` function from the `nltk` package to split the text into sentences. The `sent_tokenize` function is a wrapper around the `punkt` tokenizer, which is a pre-trained sentence tokenizer. The `punkt` tokenizer is trained on the Penn Treebank corpus, which is a collection of Wall Street Journal articles. The `punkt` tokenizer is a good choice for plain English text, but it may not be the best choice for other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.82 ms (started: 2022-11-12 06:40:36 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from ekorpkit.tokenizers.spm import export_sentence_chunk_files\n",
    "\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.tokenizers.spm:Writing sentence chunks to /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/tokenizers/enko_filtered_chunk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ebbb1b1a58498096c4db909dc50d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6393e3c31e4767912b265cf302c53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0000.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fe66a46197427ba6c666fc206d95f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0001.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c057403bef409a9c9e61bea947ff24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0002.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7579aa904b3e40ed92131be6d78e4421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0003.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cee6cb4a0047dea13757a0a75d1f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0004.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497f29f6bbca45cfa9fa1393012f9fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0005.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1317ae385c642c8a6f78331c473e986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0006.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486363dab1b74e6eb6a17f823e41fdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0007.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d6009136174737a4ff818e6d724c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0008.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2f50afa5ac467fa92e6876f1158e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0009.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1300392a0b4a4cb18c046cc7d20c7365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0010.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3268a2df14b74ceb959ca263bc784300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0011.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a27ed9113654169be6a9dbed5bb3836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0012.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d6341771d2417f97a5735179c8ee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0013.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4ff8a315de4aa4aab2101e094354a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0014.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3cbc5915f847d583811a30cd1f9219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0015.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a629be43c4a49309c26d9dc8fa8ad7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0016.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af75717dbc9448e9c75da815083295d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0017.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d621646ec5f45478373f45f754da45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0018.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bba6630437405781a7efb311b78646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0019.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3824cc7683c4bbd834fbb4c43c96732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0020.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953e34781070497e843b296e3b9ccffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0021.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf6cd36e9d643688d66fb27a1c516e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0022.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6237d87028447d941c5f2624aa7423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0023.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56754b1a995942d4a33374736c1467f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0024.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d86d8d2f62e402d8acfa68ff9949aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0025.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4471ec56f4a4a2695a0e21cfd2177d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0026.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5ac0cf428f4fe9a5ff21cce77f9afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0027.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae817a7e1a7647cba40ab08b7ddd92f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0028.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eda9a241fe74df2a582bce1bd04da04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0029.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92072fda4ac742ed86c11a2450cf98c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0030.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d3ca23620746a98aa73abd702b4478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0031.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9b08af8a6642d5ae8b1d5e46ec6c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0032.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ca5fc42bc741788dbaddd00e5740f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0033.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6453b8e02fea4fceb12622fafc279265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0034.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca60fc4d592454eb2075bb9f83cb7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0035.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9a1fc3f0fe453abac16bcb5c2f0686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0036.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210d7eb5a0084b3c830188bb82a9c6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0037.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7f3033590645419a2ca283ef3ad107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0038.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2051c0a19bd4459f8ce1ce28f6d8cf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0039.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faf54ba02cc41a393c0e4e6cd7a8a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0040.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2457b0f326146589daad0a41be7c17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0041.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0979246aaff4814b24f3cc9bba716e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0042.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14553ac08344b4d9ee7fee24e40258e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0043.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8b5e52aece404380e336d9195b5b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0044.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bf753237ee4dd789477701b011dd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0045.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc376d7c071444ee9d86ce1d36092d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0046.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27f8c1f7efe4dca82e4ac052382a276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0047.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9403a84983044f6bb7806bc71119df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0048.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb481928ec746a78c6965bc8a74ee2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0049.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac2760d07964aca8a025d54dbfd0391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0050.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a02b6eb2f1416daeaf8bf1937b09d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0051.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbb80a56de74c28b97b8432972eedeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0052.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2976ee0c6ab3473cbd817133e53b0e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0053.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f4da4373494d20b31f89fdd7f5ec9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0054.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4aea78ce10540d6abf62372b71c4a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0055.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc18f8b7a90b484c8f0b00dc1714aa6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0056.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ace2f9444c43e5b6bdd197a2214baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0057.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8e5508fe6040e7985b6ed8fd6276c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0058.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4be6bc48c3e4845b75b8e96d58539f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0059.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df69545ff92548eeb6ee8a21033a770e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0060.txt:   0%|          | 0/3719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 29s (started: 2022-11-12 06:40:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "output_dir = project_dir + \"/data/tokenizers/enko_filtered_chunk\"\n",
    "\n",
    "export_sentence_chunk_files(\n",
    "    raw_dataset,\n",
    "    output_dir=output_dir,\n",
    "    chunk_size=10000,\n",
    "    text_column=text_column,\n",
    "    sent_tokenize=sent_tokenize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample sentences and combine them into a single file\n",
    "\n",
    "If your dataset is too large, you can sample a subset of the sentence files for training. The `sample` function from the `random` module can be used to sample a subset of the files.\n",
    "\n",
    "You can use `sample_and_combine` function to sample a subset of sentence files and combine them into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.tokenizers.spm:sampled files: ['sent_chunk_0008.txt', 'sent_chunk_0055.txt', 'sent_chunk_0002.txt', 'sent_chunk_0059.txt', 'sent_chunk_0026.txt', 'sent_chunk_0047.txt', 'sent_chunk_0022.txt', 'sent_chunk_0004.txt', 'sent_chunk_0050.txt', 'sent_chunk_0045.txt', 'sent_chunk_0060.txt', 'sent_chunk_0042.txt', 'sent_chunk_0051.txt', 'sent_chunk_0014.txt', 'sent_chunk_0007.txt', 'sent_chunk_0056.txt', 'sent_chunk_0005.txt', 'sent_chunk_0041.txt', 'sent_chunk_0039.txt', 'sent_chunk_0038.txt', 'sent_chunk_0035.txt', 'sent_chunk_0000.txt', 'sent_chunk_0020.txt', 'sent_chunk_0029.txt', 'sent_chunk_0053.txt', 'sent_chunk_0046.txt', 'sent_chunk_0033.txt', 'sent_chunk_0006.txt', 'sent_chunk_0048.txt', 'sent_chunk_0001.txt', 'sent_chunk_0025.txt', 'sent_chunk_0027.txt', 'sent_chunk_0036.txt', 'sent_chunk_0016.txt', 'sent_chunk_0052.txt', 'sent_chunk_0037.txt', 'sent_chunk_0015.txt', 'sent_chunk_0049.txt', 'sent_chunk_0010.txt', 'sent_chunk_0021.txt', 'sent_chunk_0024.txt', 'sent_chunk_0032.txt', 'sent_chunk_0023.txt', 'sent_chunk_0012.txt', 'sent_chunk_0003.txt', 'sent_chunk_0013.txt', 'sent_chunk_0043.txt', 'sent_chunk_0034.txt', 'sent_chunk_0030.txt', 'sent_chunk_0018.txt']\n",
      "INFO:ekorpkit.tokenizers.spm:number of lines sampled: 24,425,187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ac7bbfdae4412dac087c2c99298788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24425187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.tokenizers.spm:saved sampled sentences to /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/tokenizers/enko_filtered_samples/sampled_sentences.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 47s (started: 2022-11-12 07:33:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.tokenizers.spm import sample_and_combine\n",
    "\n",
    "input_dir = project_dir + \"/data/tokenizers/enko_filtered_chunk\"\n",
    "output_dir = project_dir + \"/data/tokenizers/enko_filtered_samples\"\n",
    "\n",
    "sampled_file = sample_and_combine(\n",
    "    input_dir=input_dir,\n",
    "    output_dir=output_dir,\n",
    "    sample_size=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SentencePiece models\n",
    "\n",
    "You can use `train_spm` function to train a SentencePiece model. The `train_spm` function takes the following arguments:\n",
    "\n",
    "- `model_prefix`: The prefix of the model file. The model file will be saved as `{model_prefix}_{model_type}_vocab_{vocab_size}.model`.\n",
    "- `input`: The input file for training.\n",
    "- `output_dir`: The directory to save the model file.\n",
    "- `vocab_size`: The vocabulary size.\n",
    "- `model_type`: The model type. It can be `unigram` (default), `bpe`, `char`, or `word`.\n",
    "- `character_coverage`: The character coverage. It is only used for `unigram` and `bpe` model types. The default value is `1.0`.\n",
    "- `num_threads`: The number of threads to use for training. The default value is `1`. The max value is `128`.\n",
    "- `train_extremely_large_corpus`: Whether to train an extremely large corpus. The default value is `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.tokenizers.spm:Training SentencePiece model enko_wiki_unigram_vocab_30000.model\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/tokenizers/enko_filtered_samples/sampled_sentences.txt\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 128\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/tokenizers/enko_filtered_samples/sampled_sentences.txt\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 4000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 5000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 6000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 7000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 8000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 9000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 10000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 11000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 12000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 13000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 14000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 15000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 16000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 17000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 18000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 19000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 20000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 21000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 22000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 23000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 24000000 lines\n",
      "trainer_interface.cc(120) LOG(WARNING) Too many sentences are loaded! (24420004), which may slow down training.\n",
      "trainer_interface.cc(122) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(125) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 24420004 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=1528397615\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=3816\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 24419902 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) ExtracINFO:ekorpkit.tokenizers.spm:saved model to /content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/spm/enko_wiki_unigram_vocab_30000.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23min 6s (started: 2022-11-12 08:03:08 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 24419902\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 4696044\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 4696044 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=838106 obj=16.3019 num_tokens=14291441 num_tokens/piece=17.0521\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=688067 obj=14.038 num_tokens=14365800 num_tokens/piece=20.8785\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=516036 obj=13.9962 num_tokens=14484892 num_tokens/piece=28.0695\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=515998 obj=13.9855 num_tokens=14495195 num_tokens/piece=28.0916\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=386997 obj=14.0208 num_tokens=14763508 num_tokens/piece=38.1489\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=386994 obj=14.0098 num_tokens=14769200 num_tokens/piece=38.1639\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=290245 obj=14.1075 num_tokens=15208608 num_tokens/piece=52.3992\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=290245 obj=14.0799 num_tokens=15208857 num_tokens/piece=52.4001\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=217683 obj=14.2339 num_tokens=15712334 num_tokens/piece=72.1799\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=217683 obj=14.1986 num_tokens=15712378 num_tokens/piece=72.1801\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=163262 obj=14.3894 num_tokens=16252847 num_tokens/piece=99.5507\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=163262 obj=14.3501 num_tokens=16252560 num_tokens/piece=99.5489\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=122446 obj=14.5717 num_tokens=16812721 num_tokens/piece=137.307\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=122446 obj=14.5288 num_tokens=16812579 num_tokens/piece=137.306\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=91834 obj=14.7795 num_tokens=17416351 num_tokens/piece=189.65\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=91834 obj=14.7313 num_tokens=17416277 num_tokens/piece=189.65\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=68875 obj=15.012 num_tokens=18049647 num_tokens/piece=262.064\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=68875 obj=14.9565 num_tokens=18050042 num_tokens/piece=262.07\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=51656 obj=15.2728 num_tokens=18704298 num_tokens/piece=362.093\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=51656 obj=15.2075 num_tokens=18704520 num_tokens/piece=362.098\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=38742 obj=15.5638 num_tokens=19386320 num_tokens/piece=500.395\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=38742 obj=15.4915 num_tokens=19385990 num_tokens/piece=500.387\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=33000 obj=15.7017 num_tokens=19770798 num_tokens/piece=599.115\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=33000 obj=15.6585 num_tokens=19771014 num_tokens/piece=599.122\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.tokenizers.spm import train_spm\n",
    "\n",
    "uni_model_path = train_spm(\n",
    "    model_prefix=\"enko_wiki\",\n",
    "    input=sampled_file,\n",
    "    output_dir=project_dir + \"/tokenizers/spm\",\n",
    "    model_type=\"unigram\",\n",
    "    vocab_size=30000,\n",
    "    character_coverage=0.9995,\n",
    "    num_threads=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> time: 23min 6s (started: 2022-11-12 08:03:08 +00:00)\n",
    "\n",
    "It took 23 minutes to train a unigram model with a vocabulary size of 30,000. The model file was saved in the `{project_dir}/tokenizers` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "uni_model_path = model_path\n",
    "uni_spm = spm.SentencePieceProcessor(model_file=uni_model_path)\n",
    "print(f\"Vocab size: {uni_spm.vocab_size()}\")\n",
    "print(uni_spm.encode_as_pieces(raw_dataset[5555][\"text\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train BPE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_model_path = train_spm(\n",
    "    model_prefix=\"enko_wiki\",\n",
    "    input=sampled_file,\n",
    "    output_dir=project_dir + \"/tokenizers/spm\",\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=30000,\n",
    "    character_coverage=0.9995,\n",
    "    num_threads=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "bpe_spm = spm.SentencePieceProcessor(model_file=bpe_model_path)\n",
    "print(f\"Vocab size: {bpe_spm.get_piece_size()}\")\n",
    "print(bpe_spm.encode_as_pieces(raw_dataset[5555][\"text\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Tokenizers\n",
    "\n",
    "### Load the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {\n",
    "    \"BPE\": bpe_tokenizer,\n",
    "    \"UNI\": uni_tokenizer,\n",
    "    \"SPM\": spm_tokenizer,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the output of the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "num_samples = 10\n",
    "tokens = {name: [] for name in tokenizers.keys()}\n",
    "\n",
    "# sample 10 texts from the dataset randomly\n",
    "for i in range(num_samples):\n",
    "    texts.append(raw_dataset.shuffle(seed=i)[i][text_column])\n",
    "\n",
    "# tokenize the texts with the tokenizers\n",
    "for text in texts:\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        print(f\"Tokenizer: {name}\")\n",
    "        tokens[name].append(tokenizer.encode(text).tokens)\n",
    "        print(tokens[name][-1])\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_num = 5\n",
    "\n",
    "max_len = max(len(tokens[name][sample_num]) for name in tokenizers.keys())\n",
    "diffs = {name: max_len - len(tokens[name][sample_num]) for name in tokenizers.keys()}\n",
    "\n",
    "padded_tokens = {\n",
    "    name: tokens[name][sample_num] + [\"\"] * diffs[name] for name in tokenizers.keys()\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(padded_tokens)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "corpus.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
