{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Pretrained Language Models\n",
    "\n",
    "![](../figs/deep_nlp/plms/plm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613f372",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "- Language models are models that can predict the next word in a sequence of words.\n",
    "- They are trained on large corpora of text.\n",
    "- They are used to generate text, to fill in missing words, to correct spelling, and to do many other things.\n",
    "- They are also used to initialize the weights of other models, such as neural machine translation models. \n",
    "\n",
    "## Contextualized Word Embeddings\n",
    "\n",
    "- Contextualized word embeddings are word embeddings that are computed in the context of a sentence.\n",
    "- Word embeddings like Word2Vec and GloVe are static, meaning that they are computed without any context.\n",
    "  - E.g., the word \"bank\" in \"I went to the bank to deposit my check\" is the same as the word \"bank\" in \"The river bank was full of dead fish.\"\n",
    "- Deep learning based word embeddings like ELMo and BERT are contextualized, meaning that they are computed in the context of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319f694",
   "metadata": {},
   "source": [
    "## Pretrained Language Models\n",
    "\n",
    "- Pretrained language models are language models that have been trained on large corpora of text in a self-supervised manner.\n",
    "- Then, the weights of the model are saved and can be used to fine-tune other models for specific tasks. It is called transfer learning.\n",
    "- The power of pretrained language models is that they understand the generic linguistic structure of language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2f9a9",
   "metadata": {},
   "source": [
    "## Categories of Pretrained Language Models\n",
    "\n",
    "- Pretrained language models can be categorized by their training objectives.\n",
    "  - **Standard language models** are trained to predict the next word in a sequence of words.\n",
    "  - **Masked language models** are trained to predict the missing words in a sequence of words.\n",
    "  - **Permutation language models** are trained to predict workds in some random order.\n",
    "- PLMs can also be categorized by their architecture.\n",
    "  - **Encoder-only models** predict the masked or corrupted tokens based on all other tokens in the sequence.\n",
    "  - **Decoder-only models** predict the next token based on the previous tokens.\n",
    "  - **Encoder-decoder models** generate output sequences based on input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583da00",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "\n",
    "### Standard Autoregressive Language Modeling\n",
    "\n",
    "In autoregressive language modeling, the ideal language model $p^*(w)$ is a probability distribution over all possible sequences of words $w$:\n",
    "\n",
    "$$\n",
    "p^*(w) \\approx \\prod_{i=1}^n \\hat{p}_{\\theta}(w_i \\mid w_1, \\ldots, w_{i-1})\n",
    "$$\n",
    "\n",
    "where $\\theta$ are the model parameters and $\\hat{p}$ is the model's prediction of the probability of the next word given the previous words.\n",
    "\n",
    "- The autoregressive language modeling approach uses the previous words to predict the next word.\n",
    "- LSTM or Bi-LSTM, an RNN based architecture before the transformer architecture, uses this approach.\n",
    "- This inherent sequential nature of the models does not understand the context of the sentence as a whole.\n",
    "- Correctly predicting the next word depends on the previous words.\n",
    "- As a sequence of words gets longer, the model becomes more difficult to predict the next word.\n",
    "- This nature of the model makes it difficult and slow to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fc400",
   "metadata": {},
   "source": [
    "###  Masked Language Modeling\n",
    "\n",
    "- The main objective of the model is to predict the masked words, which are randomly masked in the input sequence.\n",
    "- This approach makes the model bidirectional in nature because the masked words are predicted based on the words before and after the masked words.\n",
    "- BERT (Bidirectional Encoder Representations from Transformers) is the most famous masked language model.\n",
    "- This approach only use the encoder part of the model.\n",
    "- Predicting the masked words is done independently of the other masked words in the sequence. This makes it easier to parallelize the training process.\n",
    "- However, this approach does not take into account the dependency between the masked words.\n",
    "\n",
    "For example, in the sentence \"I went to the [MASK] to deposit my [MASK]\", the model should predict \"bank\" for the first masked word and \"check\" for the second masked word. However, the model may predict \"bank\" for the first word and \"luggage\" for the second word because the model does not take into account the dependency between the masked words.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-mask-patterns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd44a4",
   "metadata": {},
   "source": [
    "### Causal Masked Language Modeling\n",
    "\n",
    "- Causal masked language modeling is also to predict the masked words in the input sequence.\n",
    "- However, unlike the masked language modeling, the model can only see the words before the masked word by the causal masking scheme.\n",
    "- Therefore, this approach is unidirectional in nature.\n",
    "- GPT (Generative Pre-Training) and its variants follow this approach.\n",
    "- This approach naturally takes into account the dependency between the masked words.\n",
    "- Because the model only uses the decoder part of the transformer architecture, it is also called decoder-only language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35906d5a",
   "metadata": {},
   "source": [
    "### Permutation Language Modeling\n",
    "\n",
    "- The major flaw of the masked language modeling approach is that it does not take into account the dependency between the masked words.\n",
    "- Permutation language modeling is to predict the tokens in a random order.\n",
    "- XLNet (Generalized Autoregressive Pretraining for Language Understanding) uses this approach. ({cite}`yang2019xlnet`)\n",
    "- Permutation language models learn the bidirectional dependency between all combinations of tokens in the sequence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## Transformer Architecture and PLMs\n",
    "\n",
    "The transformer architecture consist of two parts: the encoder and the decoder. The encoder is used to encode the input sequence into a vector representation. The decoder is used to decode the vector representation into the output sequence. \n",
    "\n",
    "We can classify PLMs into three categories based on the usage of the encoder and the decoder:\n",
    "\n",
    "- **Encoder-only PLMs** (e.g. BERT, RoBERTa, DistilBERT, ALBERT, ELECTRA, etc.) are used to encode the input sequence into a vector representation. The decoder is not used. The vector representation is used as the input to the downstream task. For example, the vector representation can be used as the input to a classifier to perform text classification.\n",
    "- **Decoder-only PLMs** (e.g. GPT-2, CTRL, etc.) only use the decoder to generate the output sequence. The encoder is not used. The decoder is trained to predict the next word in the sequence of words.\n",
    "- **Encoder-decoder PLMs** (e.g. T5, BART, MASS, etc.) use both the encoder and the decoder. The encoder is used to encode the input sequence into a vector representation. The decoder is used to generate the output sequence.\n",
    "\n",
    "![](../figs/deep_nlp/t5/t5-structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175dac0",
   "metadata": {},
   "source": [
    "## GPT Family\n",
    "\n",
    "- GPT (Generative Pre-Training) is the first decoder-only PLM.\n",
    "- GPT uses the uni-directional context to predict the next word.\n",
    "\n",
    "    Loss function is defined as:\n",
    "\n",
    "    $$\n",
    "    \\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^N \\log \\hat{p}_{\\theta}(w_i \\mid w_1, \\ldots, w_{i-1})\n",
    "    $$\n",
    "\n",
    "- Main usage of GPT is to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc69e19",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "- BERT (Bidirectional Encoder Representations from Transformers) is the first encoder-only PLM.\n",
    "- BERT uses two objectives to train the model: masked language modeling and next sentence prediction.\n",
    "- **Masked language modeling** is to predict the masked words in the input sequence.\n",
    "\n",
    "```{image} ../figs/deep_nlp/plms/bert_mlm.jpg\n",
    ":alt: bert_mlm\n",
    ":class: bg-primary mb-1\n",
    ":width: 400px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "  - **Next sentence prediction** is to predict whether the second sentence is the next sentence of the first sentence.\n",
    "\n",
    "```{image} ../figs/deep_nlp/plms/bert_nsp.jpg\n",
    ":alt: bert_nsp\n",
    ":class: bg-primary mb-1\n",
    ":width: 400px\n",
    ":align: center\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf250206",
   "metadata": {},
   "source": [
    "## RoBERTa\n",
    "\n",
    "- RoBERTa (Robustly Optimized BERT Pretraining Approach) is a variant of BERT.\n",
    "- Modifications to BERT:\n",
    "  - **Dynamic masking** is used instead of static masking.\n",
    "  - **Byte-level BPE** is used instead of WordPiece.\n",
    "  - **Training with longer sequences** is used.\n",
    "  - **Training with more data** is used.\n",
    "  - **Training with more epochs** is used.\n",
    "  - **No next sentence prediction** is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af49e693",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
