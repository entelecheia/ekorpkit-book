{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Pretraining a Language Model\n",
    "\n",
    "![](../figs/deep_nlp/lab/train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset and tokenizer, in this lab, we will train a language model on a large corpus of text from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Normalization\n",
    "\n",
    "One little thing to note is that we will need to normalize our text before training our language model. This is because the same character can be represented in different ways. For example, the character \"é\" can be represented as \"e\" followed by a combining accent character, or as a single character. We will use the [Unicode Normalization Form C](https://unicode.org/reports/tr15/#Norm_Forms) to normalize our text. This will convert all characters to their canonical representation.\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "Use `NFKC` normalization to normalize your text before training your language model.\n",
    "\n",
    "\n",
    "### Unicode Normalization Forms\n",
    "\n",
    "There are four normalization forms:\n",
    "\n",
    "- **NFC**: Normalization Form Canonical Composition\n",
    "- **NFD**: Normalization Form Canonical Decomposition\n",
    "- **NFKC**: Normalization Form Compatibility Composition\n",
    "- **NFKD**: Normalization Form Compatibility Decomposition\n",
    "\n",
    "In the above forms, \"C\" stands for \"Canonical\" and \"K\" stands for \"Compatibility\". The \"C\" forms are the most commonly used. The \"K\" forms are used when you need to convert characters to their compatibility representation. For example, the \"K\" forms will convert \"ﬁ\" to \"fi\".\n",
    "\n",
    "There two main differences between the two sets of forms:\n",
    "\n",
    "- The length of the string is changed or not: NFC and NFKC always produce a string of the same length or shorter, while NFD and NFKD may produce a string that is longer.\n",
    "- The original string is changed or not: NFC and NFD always produce a string that is identical to the original string, while NFKC and NFKD may produce a string that is different from the original string.\n",
    "\n",
    "### Unicode Normalization in Python\n",
    "\n",
    "In Python, you can use the `unicodedata` module to normalize your text. The `unicodedata.normalize` function takes two arguments:\n",
    "\n",
    "- `form`: The normalization form to use. This can be one of the following: `NFC`, `NFD`, `NFKC`, `NFKD`.\n",
    "- `unistr`: The string to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ａｂｃＡＢＣ１２３가나다…, 13\n",
      "NFC: ａｂｃＡＢＣ１２３가나다…, 13\n",
      "NFD: ａｂｃＡＢＣ１２３가나다…, 16\n",
      "NFKC: abcABC123가나다..., 15\n",
      "NFKD: abcABC123가나다..., 18\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "text = \"ａｂｃＡＢＣ１２３가나다…\"\n",
    "print(f\"Original: {text}, {len(text)}\")\n",
    "for form in [\"NFC\", \"NFD\", \"NFKC\", \"NFKD\"]:\n",
    "    ntext = unicodedata.normalize(form, text)\n",
    "    print(f\"{form}: {ntext}, {len(ntext)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Pretraining\n",
    "\n",
    "In this lab, we will train a BERT-like model using masked-language modeling, one of the two pretraining tasks used in the original BERT paper.\n",
    "\n",
    "### What is BERT?\n",
    "\n",
    "BERT is a large-scale language model that was trained on the English Wikipedia using a masked-language modeling objective. The model was then fine-tuned on a variety of downstream tasks, including question answering, natural language inference, and sentiment analysis. BERT was the first large-scale language model to be pre-trained using a deep bidirectional architecture and outperformed previous language models on a variety of tasks.\n",
    "\n",
    "For more information, see the lecture notes on BERT.\n",
    "\n",
    "### Masked-Language Modeling (MLM)\n",
    "\n",
    "Masked-language modeling is a pretraining task where we mask some of the input tokens and train the model to predict the original value of the masked tokens. For example, if we have the sentence \"The dog ate the apple\", we can mask the word \"ate\" and train the model to predict the original value of the masked token. The model will then learn to predict the original value of the masked tokens based on the context of the sentence.\n",
    "\n",
    "Example:\n",
    "\n",
    "> Input: \"The dog [MASK] the apple\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "Before training our language model, we need to preprocess our dataset. We will use our tokenizer to tokenize our dataset and then convert the tokens to their IDs. If we have a sentence that is longer than the maximum sequence length, we will truncate the sentence. If the sentence is shorter than the maximum sequence length, we will pad the sentence with the padding token.\n",
    "\n",
    "From four tokenizers we have trained in the previous lab, we will use the `unigram` model trained using `SentencePiece` to tokenize our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- [Unicode equivalence](https://en.wikipedia.org/wiki/Unicode_equivalence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
