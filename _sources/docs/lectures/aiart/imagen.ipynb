{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Imagen\n",
    "\n",
    "![Imagen](../figs/aiart/imagen/the-toronto-skyline-with-google-brain-logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78475c20",
   "metadata": {},
   "source": [
    "## Text-to-image model\n",
    "\n",
    "![Text-to-image model](../figs/aiart/imagen/text-to-image-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c6cc5",
   "metadata": {},
   "source": [
    "- High-performing text-to-image models will necessarily be able to combine unrelated concepts and objects in semantically plausible ways.\n",
    "- Challenges\n",
    "  - capturing spatial relationships\n",
    "  - understanding cardinality\n",
    "  - properly interpreting how words in the description relate to one another\n",
    "  - ”A brain riding a rocketship heading towards the moon”\n",
    "\n",
    "![](../figs/aiart/imagen/brain-rocketship-moon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca93af5",
   "metadata": {},
   "source": [
    "## How Imagen Works: A Bird's-Eye View\n",
    "\n",
    "1. The caption is input into a `text encoder`. This encoder converts the textual caption to a numerical representation that `encapsulates the semantic information within the text`.\n",
    "2. An image-generation model creates an image by starting with noise and slowly transforming it into an output image. To guide this process, the image-generation model receives the text encoding as an input, which has the effect of `telling the model what is in the caption` so it can create a corresponding image. The output is a small image that reflects visually the caption we input to the text encoder!\n",
    "3. The small image is then passed into a `super-resolution model`, which grows the image to a higher resolution. This model also takes the text encoding as input, which helps the model decide how to behave as it \"fills in the gaps\" of missing information that necessarily arise from quadrupling the size of our image. The result is a `medium sized image` of what we want.\n",
    "4. Finally, this medium sized image is then passed into yet `another super-resolution model`, which operates near-identically to the previous one, except this time it takes our medium sized image and grows it to a `high-resolution image`. The result is 1024 x 1024 pixel image that visually reflects the semantics within our caption.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c84c009",
   "metadata": {},
   "source": [
    "### Text Encoder\n",
    "\n",
    "- The text encoder is critical to Imagen's performance. It conditions all of Imagen's other components and is responsible for encoding the textual caption in a useful way.\n",
    "- The text encoder in Imagen is a Transformer encoder.\n",
    "- Encoder ensures that the text encoding understands how the words within the caption relate to one another (by a method called \"self-attention\"). \n",
    "The English language encodes information via its syntactic structure that affects the semantic meaning of a given sentence.\n",
    "The text-encoder is frozen during training, meaning that it does not learn or change the way it creates the encodings. It is only used to generate encodings that are fed to the rest of the model, which is trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9692281",
   "metadata": {},
   "source": [
    "A lack of consideration for how words relate to one another could yield an extremely poor result:\n",
    "\n",
    "![](../figs/aiart/imagen/imagen-bad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be407b9f",
   "metadata": {},
   "source": [
    "### Image Generator\n",
    "\n",
    "- Imagen uses a Diffusion Model to generate an image.\n",
    "- Diffusion Models are a method of creating data that is similar to a set of training data. They train by destroying the training data through the addition of noise, and then learning to recover the data by reversing this noising process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419637fb",
   "metadata": {},
   "source": [
    "![](../figs/aiart/imagen/diffusion-model1.png)\n",
    "![](../figs/aiart/imagen/diffusion-model2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834d64d",
   "metadata": {},
   "source": [
    "### Caption Conditioning\n",
    "\n",
    "![](../figs/aiart/imagen/caption-conditioning.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfcf90",
   "metadata": {},
   "source": [
    "### Image Super-Resolution\n",
    "\n",
    "![](../figs/aiart/imagen/super-res.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56233c5b",
   "metadata": {},
   "source": [
    "## How Imagen Works: A Deep Dive\n",
    "\n",
    "### Text Encoder: T5\n",
    "\n",
    "- The text encoder in Imagen is the encoder network of T5 (Text-to-Text Transfer Transformer), a language model released by Google in 2019.\n",
    "- T5 is intended to be finetuned for any NLP task that can be cast in this text-to-text manner.\n",
    "- Some other text-to-image models like DALL-E 2 use text-encoders which are trained on image-caption pairs and an associated objective that is explicitly designed for the purpose of linking textual and visual representations of the same semantic concept.\n",
    "- The central intuition in using T5 is that extremely large language models, by virtue of their sheer size alone, may still learn useful representations despite the fact that they are not explicitly trained with any text/image task in mind.\n",
    "- “whether or not a massive language model trained on a massive dataset independent of the task of image generation is a worthwhile trade-off for a non-specialized text encoder.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a8a3e",
   "metadata": {},
   "source": [
    "![](../figs/deep_nlp/t5/t5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3baa90",
   "metadata": {},
   "source": [
    "Image Generator:  Network Architecture\n",
    "\n",
    "- A Diffusion Model is sort of a \"metamodel\" framework that tells us how to use a neural model to denoise images. \n",
    "- U-Net architecture by Nichol and Dhariwa.\n",
    "\n",
    "![](../figs/aiart/imagen/unet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800662e8",
   "metadata": {},
   "source": [
    "### Image Generator:  Network Architecture\n",
    "\n",
    "- Each residual block is composed of two sub-blocks, and each of these sub-blocks is composed of a Batch Normalization, ReLU, and 3x3 Convolution in sequence.\n",
    "\n",
    "![](../figs/aiart/imagen/residual-block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974fcd22",
   "metadata": {},
   "source": [
    "\n",
    "### Timestep Conditioning\n",
    "\n",
    "- In Imagen, the same denoising U-Net is used at every timestep. \n",
    "- We must therefore devise a way to inject timestep information into the model (i.e. condition on the timestep). \n",
    "- The Imagen authors utilize a technique introduced by the original Transformer paper called positional encoding.\n",
    "\n",
    "![](../figs/aiart/imagen/pos_enc.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2445daac",
   "metadata": {},
   "source": [
    "- A unique timestep encoding vector is generated for each timestep (corresponding to \"word position\" in the original positional embedding implementation). \n",
    "- At different resolutions in the U-Net, this vector is projected to having c components, where c is the number of channels in the U-Net at that resolution. \n",
    "- After projection, each component of the vector is added to the corresponding channel (across its height and width) in the image.\n",
    "\n",
    "![](../figs/aiart/imagen/time_enc.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e202c",
   "metadata": {},
   "source": [
    "### Caption Conditioning\n",
    "\n",
    "- First, the output vectors from the T5 text encoder are pooled and added into the timestep embedding from above. \n",
    "- Next, the model is conditioned on the entire encoding sequence by adding cross attention over the text embeddings at several resolutions. The cross attention is implemented by concatenating the text embedding sequence to the key-value pairs of each self-attention layer.\n",
    "\n",
    "![](../figs/aiart/imagen/caption-conditioning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45543390",
   "metadata": {},
   "source": [
    "### Classifier-Free Guidance\n",
    "\n",
    "- Classifier-Free Guidance is a method of increasing the image fidelity of a Diffusion Model at the cost of image diversity. \n",
    "- Classifier Guidance is a method for trading off the fidelity and diversity of images generated by a Diffusion Model.\n",
    "- The theoretical cost of Classifier Guidance method is diversity, because images will be encouraged to have features that are frequently observed for the given class. \n",
    "- The practical costs of the method are \n",
    "  1. needing to train a classifier in addition to the diffusion model\n",
    "  2. poor image quality when the conditional term is scaled too high (too high of a \"guidance weight\").\n",
    "- https://benanne.github.io/2022/05/26/guidance.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2ae25",
   "metadata": {},
   "source": [
    "- Classifier-Free Guidance works by training a Diffusion Model to be both conditional and unconditional at the same time. \n",
    "- the Diffusion Model is cast as a conditional model and is trained with the conditioning information randomly dropped out a small fraction of the time (by replacing the conditional information with a NULL value). \n",
    "- To use the model in an unconditional way, the NULL value is simply provided as the \"conditional information\" to the model.\n",
    "- Classifier-Free guidance works loosely by interpolating between the unconditional and conditional gradients during inference. \n",
    "- By magnifying the effect of the conditional gradient (i.e. making the \"guidance weight\" greater than 1), better samples can be obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4fbfc",
   "metadata": {},
   "source": [
    "GLIDE images generated by the prompt \"a religious place\"\n",
    "\n",
    "![](../figs/aiart/imagen/guidance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344485d",
   "metadata": {},
   "source": [
    "### Large Guidance Weight Samplers\n",
    "\n",
    "- Imagen depends critically on classifier-free guidance for effective text conditioning.\n",
    "- Classifier-Free Guidance is a very powerful way to improve the caption alignment of generated images, but it has been previously observed that extremely high guidance weights damage fidelity by yielding saturated and unnatural images.\n",
    "- This phenomenon arises from a train-test mismatch.\n",
    "- The pixel values for the training data are scaled to the range [-1, 1], but high guidance weights cause the network outputs to exceed these bounds at given timestep.\n",
    "- High guidance weights are found to be crucial for achieving State-of-the-Art image quality, so avoiding the problem by simply using lower guidance weights is not an option.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859d737",
   "metadata": {},
   "source": [
    "#### Static Thresholding\n",
    "\n",
    "The pixel values at each timestep are clipped to the range [-1, 1]\n",
    "\n",
    "![](../figs/aiart/imagen/static_threshold.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cddabd",
   "metadata": {},
   "source": [
    "#### Dynamic Thresholding\n",
    "\n",
    "- A certain percentile absolute pixel value is chosen. \n",
    "- At each timestep, if that percentile value s exceeds 1, then the pixel values are thresholded to [-s, s] and divided by s.\n",
    "\n",
    "![](../figs/aiart/imagen/dynamic_threshold.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe72a1",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "The effects of increasing the guidance weight for three models\n",
    "\n",
    "![](../figs/aiart/imagen/threshold_comparison.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d85cf",
   "metadata": {},
   "source": [
    "### Super-Resolution Models\n",
    "\n",
    "- Diffusion Model (or \"base model\") outputs 64x64 images. \n",
    "- Imagen uses two conditional diffusion models to bring the image up to 1024x1024 resolution.\n",
    "- The Small-to-Medium (STM) super-resolution model \"takes in\" (is conditioned on) the 64x64 image generated by the base model and super-resolves it to a 256x256 image. \n",
    "- The STM model is yet another diffusion model, and is also conditioned on the caption encoding in addition to the low-resolution image.\n",
    "- The Medium-to-Large (MTL) super-resolution model super-resolves the 256x256 image generated by the STM model to a 1024x1204 image.\n",
    "- The architecture is generally similar to the STM model except that the self-attention layers are removed. Since there are no self-attention layers in this model, explicit cross-attention layers are added to attend over the text embeddings in contrast to the base and STM models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c8cc1",
   "metadata": {},
   "source": [
    "#### Robust Cascaded Diffusion Models\n",
    "\n",
    "- Imagen uses noise conditioning augmentation in the super-resolution models in order to make them aware of the amount of noise added. \n",
    "- This conditioning improves sample quality and the ability of the models to handle artifacts resulting from the lower resolution models. \n",
    "- The authors find this approach to be critical for generating high fidelity images.\u000b\n",
    "\n",
    "![](../figs/aiart/imagen/conditioning_augmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4119c42",
   "metadata": {},
   "source": [
    "### Results and Analysis\n",
    "\n",
    "- Quantitative\n",
    "  - COCO is a dataset used to evaluate text-to-image models, with FID used to measure image fidelity and CLIP used to measure image-caption alignment. \n",
    "  - The authors find that Imagen achieves a State-of-the-Art zero-shot FID of 7.27 on COCO, outperforming DALL-E 2 and even models that were trained on COCO.\n",
    "- Qualitative\n",
    "  - Quality: \"Which image is more photorealistic (looks more real)?”\n",
    "  - Imagen achieves a preference rate of 39.2% for photorealism.\n",
    "  - Caption Similarity: \"Does the caption accurately describe the above image?”\n",
    "  - Imagen is on-par with original reference images for caption similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4fd65",
   "metadata": {},
   "source": [
    "- DrawBench\n",
    "  - a comprehensive and challenging set of prompts that is intended to support the evaluation and comparison of text-to-image models. \n",
    "\n",
    "![](../figs/aiart/imagen/drabench.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af398d90",
   "metadata": {},
   "source": [
    "### Why is Imagen Better than DALL-E 2?\n",
    "\n",
    "- DALL-E 2 uses a contrastive objective to determine how related a text encoding is to an image (essentially CLIP). The text and image encoders tune their parameters such that the cosine similarities of like caption-image pairs are maximized, while the cosine similarities of differing caption-image pairs are minimized.\n",
    "- Sheer Size\n",
    "- The effect of scaling up the text encoder is shockingly high, and that of scaling up the U-Net is shockingly low. \n",
    "\n",
    "![](../figs/aiart/imagen/encoder-unet-size.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f2dc4",
   "metadata": {},
   "source": [
    "- The Image Encoder Crutch\n",
    "  - During CLIP training, both the text encoder and image encoder are tuned to satisfy the demands of the objective function.\n",
    "  - The image encoder in CLIP learns to produce richer encodings more quickly than the text encoder, adapting to the relatively lower performance of the text encoder.\n",
    "- Similar Concepts in Different Data Points\n",
    "- The potential shortcoming of the CLIP text-encoding method is that the blanket objective of maximizing the cosine similarity of corresponding caption-image pairs while minimizing that of differing ones does not account for similar concepts in distinct data points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bd977",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Scaling the text encoder is very effective\n",
    "- Scaling the text encoder is more important than U-Net size\n",
    "- Dynamic thresholding is critical\n",
    "- Noise conditioning augmentation in the super-resolution models is critical\n",
    "- Text conditioning via cross attention is critical\n",
    "- Efficient U-Net is critical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Imagen](https://imagen.research.google/)\n",
    "- [How Imagen Actually Works](https://www.assemblyai.com/blog/how-imagen-actually-works/)\n",
    "- [MinImagen](https://www.assemblyai.com/blog/minimagen-build-your-own-imagen-text-to-image-model/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
