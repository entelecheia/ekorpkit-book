
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Vector Semantics and Representation &#8212; eKorpkit Tutorials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/ekorpkit.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Word Embeddings" href="word_embeddings.html" />
    <link rel="prev" title="Word Segmentation and Association" href="word_segmentation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FN4GFT8HP8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FN4GFT8HP8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/ekorpkit.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">eKorpkit Tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    eKonomic Research Python Toolkit
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/index.html">
   Getting started
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/install.html">
     Installation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/usage.html">
     Usage
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/features/index.html">
   Key features
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/easy_config.html">
     Easy Configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/no_boiler.html">
     No Boilerplate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/workflows.html">
     Workflows
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/share.html">
     Sharable and Reproducible
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/plug.html">
     Pluggable Architece
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/config/index.html">
   Configuring ekorpkit
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/config/eKonf.html">
     Using eKonf class
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/datasets/index.html">
   Datasets
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/corpus.html">
     Build and Load Corpora
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/datasets/dataset.html">
     Build and Load Datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/pipeline.html">
     Corpus task pipelines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/models/index.html">
   Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/automl/index.html">
     Auto ML
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/embeddings/index.html">
     Word Embeddings
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/word2vec_basics.html">
       Word2Vec Basics
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/eval_vectors.html">
       Evaluate pretrained embeddings
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/ngram/index.html">
     N-Grams
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram.html">
       N-Grams
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_lexicons.html">
       N-Gram model for ngram lexicon features
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_words.html">
       N-Gram model for unigram lexicon features
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/sentiment/index.html">
     Sentiment Analyers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/lbsa_en.html">
       Lexicon-based Sentiment Analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/financial_phrasebank_lm.html">
       Evaluate LM with financial phrasebank
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/sentiment.html">
       LM Dictionary vs. finbert vs. T5
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/transformers/index.html">
     Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/pipelines/index.html">
   Pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/pipelines/pipeline.html">
     Instantiating pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/preprocessors/index.html">
   Preprocessors
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/normalizer/index.html">
     Normalizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/normalizer/normalizer.html">
       Normalizers
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/segmenter/index.html">
     Segmenters
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/segmenter/segmenter.html">
       Segmenters
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/index.html">
     Tokenizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/mecab.html">
       Mecab Tokenizer
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/visualizers/index.html">
   Visualizers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/visualizers/plot.html">
     Plots
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/workflow/index.html">
   Workflows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../corpus/ekorpkot_corpus.html">
   The eKorpkit Corpus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Use Cases
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/fomc/index.html">
   FOMC
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/02_FOMC_numerical_data.html">
     Preparing Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/03_FOMC_corpus.html">
     Preparing Textual Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/04_FOMC_EDA_numericals.html">
     EDA on Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/05_FOMC_features.html">
     Visualizing Features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/06_FOMC_AutoML.html">
     Checking Baseline with AutoML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/07_FOMC_sentiments.html">
     Predicting Sentiments of FOMC Corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/08_FOMC_EDA_sentiments.html">
     EDA on Sentiment Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/09_FOMC_AutoML_with_tones.html">
     Predicting the next decisions with tones
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../usecases/bok/index.html">
   Bank of Korea
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/edgar/index.html">
   EDGAR
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/edgar/predict_edgar.html">
     Prediciting Sentiments
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg/index.html">
   ESG
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/snorkel-polarity.html">
     Preparing polarity classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix.html">
     Improving classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/classifiers.html">
     Training Classifiers for ESG Ratings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/econ_news_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       econ_news_kr
      </span>
     </code>
     corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_news.html">
     Predicting ESG Categories and Polarities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix_classifiers.html">
     Preparing classifiers for active learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_for_learning.html">
     Preparing active learning data
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg-en/index.html">
   ESG (English)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg-en/esg_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       JOCo
      </span>
     </code>
     corpus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/cointax/index.html">
   Taxation on Cryptocurrency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/cointax/coin_dataset.html">
     Improving classification datasets
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/analyst.html">
   ESG Topic Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/rethink_esg.html">
   Rethinking ESG
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Introduction to NLP
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="research.html">
     Research Applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="topic.html">
     Topic Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="topic_models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="topic_coherence.html">
     Topic Coherence Measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sentiments.html">
     Sentiment Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tokenization.html">
     Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="word_segmentation.html">
     Word Segmentation and Association
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Vector Semantics and Representation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="word_embeddings.html">
     Word Embeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="language_models.html">
     Language Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lab1-corpus.html">
     Lab 1: Preparing Wikipedia Corpora
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lab2-corpus-eda.html">
     Lab 2: EDA on Corpora
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../deep_nlp/index.html">
   Deep Learning for NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/zeroshot.html">
     Zero Shot, Prompt, and Search Strategies
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/bloom.html">
     What is BLOOM?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/bloom_apps.html">
     Bloom Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/transformers.html">
     Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/bert.html">
     BERT: Bidirectional Encoder Representations from Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/t5.html">
     T5: Text-To-Text Transfer Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/tokenization.html">
     Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/sentencepiece.html">
     SentencePiece Tokenizer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/byt5.html">
     ByT5: Towards a token-free future with pre-trained byte-to-byte models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/lab1-corpus.html">
     Lab 1: Preparing Wikipedia Corpora
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/lab2-corpus-eda.html">
     Lab 2: EDA on Corpora
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/lab3-train-tokenizers.html">
     Lab 3: Training Tokenizers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../aiart/index.html">
   AI Art (Text-to-Image)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/project-themes.html">
     Project Themes - A Brave New World
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle1.html">
     DALL·E 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle2.html">
     DALL·E 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/imagen.html">
     Imagen
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle-mini.html">
     DALL·E Mini
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/disco.html">
     Disco Diffusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/disco_batch.html">
     Disco Diffusion Batch Generation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/stable-diffusion.html">
     Stable Diffusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/prompt-generator.html">
     Prompt Generator for Stable Diffusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/textual-inversion.html">
     Textual Inversion (Dreambooth)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/whisper.html">
     Automatic Speech Recognition (Whisper)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/text2music.html">
     Text to Music
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/image2music.html">
     Image to Music
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mlops/index.html">
   Machine Learning Systems Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ds/index.html">
   Data Science for Economics and Finance
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/index.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/cite.html">
   Citation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/entelecheia/ekorpkit-book/main?urlpath=tree/ekorpkit-book/docs/lectures/intro_nlp/vectorization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/entelecheia/ekorpkit-book/blob/main/ekorpkit-book/docs/lectures/intro_nlp/vectorization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book/issues/new?title=Issue%20on%20page%20%2Fdocs/lectures/intro_nlp/vectorization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/docs/lectures/intro_nlp/vectorization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vector-semantics-and-word-embeddings">
   Vector Semantics and Word Embeddings
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-do-words-mean-and-how-do-we-represent-that">
     What do words mean, and how do we represent that?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#different-approaches-to-lexical-semantics">
     Different approaches to lexical semantics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lexical-semantics">
       Lexical semantics
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#distributional-semantics">
       Distributional semantics
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-represent-words-to-capture-word-similarities">
     How do we represent words to capture word similarities?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-should-word-representations-capture">
     What should word representations capture?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-distributional-hypothesis">
       The Distributional Hypothesis
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-do-we-care-about-word-contexts">
       Why do we care about word contexts?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-ways-nlp-uses-context-for-semantics">
     Two ways NLP uses context for semantics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distributional-similarity">
   Distributional Similarity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basic-idea">
     Basic idea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how">
     How?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-information-retrieval-perspective-the-term-document-matrix">
     The Information Retrieval perspective: The Term-Document Matrix
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#term-document-matrix">
   Term-Document Matrix
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-a-context">
     What is a
     <code class="docutils literal notranslate">
      <span class="pre">
       context
      </span>
     </code>
     ?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-nearby-words-as-contexts">
     Using nearby words as contexts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-word-matrix">
     Word-Word Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-co-occurrences">
     Defining co-occurrences:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representing-co-occurrences">
     Representing co-occurrences:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#getting-co-occurrence-counts">
     Getting co-occurrence counts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#counts-vs-pmi">
     Counts vs PMI
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-pmi-of-w-and-c">
     Computing PMI of
     <span class="math notranslate nohighlight">
      \(w\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(c\)
     </span>
     :
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-a-fixed-window-of-pm-k-words">
       Using a fixed window of
       <span class="math notranslate nohighlight">
        \(\pm k\)
       </span>
       words
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dot-product-as-similarity">
   Dot product as similarity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vector-similarity-cosine">
   Vector similarity: Cosine
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-cosines">
     Visualizing cosines
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Vector Semantics and Representation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vector-semantics-and-word-embeddings">
   Vector Semantics and Word Embeddings
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-do-words-mean-and-how-do-we-represent-that">
     What do words mean, and how do we represent that?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#different-approaches-to-lexical-semantics">
     Different approaches to lexical semantics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lexical-semantics">
       Lexical semantics
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#distributional-semantics">
       Distributional semantics
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-represent-words-to-capture-word-similarities">
     How do we represent words to capture word similarities?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-should-word-representations-capture">
     What should word representations capture?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-distributional-hypothesis">
       The Distributional Hypothesis
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-do-we-care-about-word-contexts">
       Why do we care about word contexts?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-ways-nlp-uses-context-for-semantics">
     Two ways NLP uses context for semantics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distributional-similarity">
   Distributional Similarity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basic-idea">
     Basic idea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how">
     How?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-information-retrieval-perspective-the-term-document-matrix">
     The Information Retrieval perspective: The Term-Document Matrix
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#term-document-matrix">
   Term-Document Matrix
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-a-context">
     What is a
     <code class="docutils literal notranslate">
      <span class="pre">
       context
      </span>
     </code>
     ?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-nearby-words-as-contexts">
     Using nearby words as contexts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-word-matrix">
     Word-Word Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-co-occurrences">
     Defining co-occurrences:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representing-co-occurrences">
     Representing co-occurrences:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#getting-co-occurrence-counts">
     Getting co-occurrence counts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#counts-vs-pmi">
     Counts vs PMI
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-pmi-of-w-and-c">
     Computing PMI of
     <span class="math notranslate nohighlight">
      \(w\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(c\)
     </span>
     :
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-a-fixed-window-of-pm-k-words">
       Using a fixed window of
       <span class="math notranslate nohighlight">
        \(\pm k\)
       </span>
       words
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dot-product-as-similarity">
   Dot product as similarity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vector-similarity-cosine">
   Vector similarity: Cosine
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-cosines">
     Visualizing cosines
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="vector-semantics-and-representation">
<h1>Vector Semantics and Representation<a class="headerlink" href="#vector-semantics-and-representation" title="Permalink to this headline">#</a></h1>
<p><img alt="" src="../../../_images/entelecheia_alphabet_letters.png" /></p>
<section id="vector-semantics-and-word-embeddings">
<h2>Vector Semantics and Word Embeddings<a class="headerlink" href="#vector-semantics-and-word-embeddings" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Lexical semantics is the study of the meaning of words</p></li>
<li><p>Distributional hypothesis: words that occur in similar contexts have similar meanings</p></li>
<li><p>Sparse vectors: one-hot encoding or bag-of-words</p></li>
<li><p>Dense vectors: word embeddings</p></li>
</ul>
<section id="what-do-words-mean-and-how-do-we-represent-that">
<h3>What do words mean, and how do we represent that?<a class="headerlink" href="#what-do-words-mean-and-how-do-we-represent-that" title="Permalink to this headline">#</a></h3>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">cassoulet</span></code></p>
</div></blockquote>
<p>Do we want to represent that …</p>
<ul class="simple">
<li><p>“cassoulet” is a French dish?</p></li>
<li><p>“cassoulet” contains meat and beans?</p></li>
<li><p>“cassoulet” is a stew?</p></li>
</ul>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">bar</span></code></p>
</div></blockquote>
<p>Do we want to represent that …</p>
<ul class="simple">
<li><p>“bar” is a place where you can drink alcohol?</p></li>
<li><p>“bar” is a long rod?</p></li>
<li><p>“bar” is to prevent something from moving?</p></li>
</ul>
<p>About words, we can say that …</p>
<ul class="simple">
<li><p>Concepts or word senses have a complex many-to-many relationship with words</p></li>
<li><p>Words have relations with each other</p>
<ul>
<li><p>Synonyms: “bar” and “pub”</p></li>
<li><p>Antonyms: “bar” and “open”</p></li>
<li><p>Similarity: “bar” and “club”</p></li>
<li><p>Relatedness: “bar” and “restaurant”</p></li>
<li><p>Superordinate: “bar” and “place”</p></li>
<li><p>Subordinate: “bar” and “pub”</p></li>
<li><p>Connotation: “bar” and “prison”</p></li>
</ul>
</li>
</ul>
</section>
<section id="different-approaches-to-lexical-semantics">
<h3>Different approaches to lexical semantics<a class="headerlink" href="#different-approaches-to-lexical-semantics" title="Permalink to this headline">#</a></h3>
<p>NLP draws on two different approaches to lexical semantics:</p>
<ul class="simple">
<li><p><strong>Lexical semantics</strong>:</p>
<ul>
<li><p>The study of the meaning of words</p></li>
<li><p>The lexicographic tradition aims to capture the information represented in lexical entries in dictionaries</p></li>
</ul>
</li>
<li><p><strong>Distributional semantics</strong>:</p>
<ul>
<li><p>The study of the meaning of words based on their distributional properties in large corpora</p></li>
<li><p>The distributional hypothesis: words that occur in similar contexts have similar meanings</p></li>
</ul>
</li>
</ul>
<section id="lexical-semantics">
<h4>Lexical semantics<a class="headerlink" href="#lexical-semantics" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Uses resources such as <code class="docutils literal notranslate"><span class="pre">lexicons</span></code>, <code class="docutils literal notranslate"><span class="pre">thesauri</span></code>, <code class="docutils literal notranslate"><span class="pre">ontologies</span></code> etc. that capture explicit knowledge about word meanings.</p></li>
<li><p>Assumes that words have <code class="docutils literal notranslate"><span class="pre">discrete</span> <span class="pre">word</span> <span class="pre">senses</span></code> that can be represented in a <code class="docutils literal notranslate"><span class="pre">lexicon</span></code>.</p>
<ul>
<li><p>bank 1 = a financial institution</p></li>
<li><p>bank 2 = a river bank</p></li>
</ul>
</li>
<li><p>May capture explicit knowledge about word meanings, but is limited in its ability to capture the meaning of words that are not in the lexicon.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">dog</span></code> is a <code class="docutils literal notranslate"><span class="pre">canine</span></code> (lexicon)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cars</span></code> have <code class="docutils literal notranslate"><span class="pre">wheels</span></code> (lexicon)</p></li>
</ul>
</li>
</ul>
</section>
<section id="distributional-semantics">
<h4>Distributional semantics<a class="headerlink" href="#distributional-semantics" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Uses <code class="docutils literal notranslate"><span class="pre">large</span> <span class="pre">corpora</span> <span class="pre">of</span> <span class="pre">raw</span> <span class="pre">text</span></code> to learn the meaning of words from the contexts in which they occur.</p></li>
<li><p>Maps words to <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">representations</span></code> that capture the <code class="docutils literal notranslate"><span class="pre">distributional</span> <span class="pre">properties</span></code> of the words in the corpus.</p></li>
<li><p>Uses neural networks to learn the dense vector representations of words, <code class="docutils literal notranslate"><span class="pre">word</span> <span class="pre">embeddings</span></code>, from large corpora of raw text.</p></li>
<li><p>If each word is mapped to a single vector, this ignores the fact that words can have multiple meanings or parts of speech.</p></li>
</ul>
</section>
</section>
<section id="how-do-we-represent-words-to-capture-word-similarities">
<h3>How do we represent words to capture word similarities?<a class="headerlink" href="#how-do-we-represent-words-to-capture-word-similarities" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>As <code class="docutils literal notranslate"><span class="pre">atomic</span> <span class="pre">symbols</span></code></p>
<ul>
<li><p>in a traditional n-gram language model</p></li>
<li><p>explicit features in a machine learning model</p></li>
<li><p>this is equivalent to very high-dimensional one-hot vectors:</p>
<ul>
<li><p>aardvark = [1,0,…,0], bear = [0,1,…,0], …, zebra = [0,0,…,1]</p></li>
<li><p>height and tall are as different as aardvark and zebra</p></li>
</ul>
</li>
</ul>
</li>
<li><p>As very high-dimensional <code class="docutils literal notranslate"><span class="pre">sparse</span> <span class="pre">vectors</span></code></p>
<ul>
<li><p>to capture the distributional properties of words</p></li>
</ul>
</li>
<li><p>As low-dimensional <code class="docutils literal notranslate"><span class="pre">dense</span> <span class="pre">vectors</span></code></p>
<ul>
<li><p>word embeddings</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-should-word-representations-capture">
<h3>What should word representations capture?<a class="headerlink" href="#what-should-word-representations-capture" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Vector representations of words were originally used to capture <code class="docutils literal notranslate"><span class="pre">lexical</span> <span class="pre">semantics</span></code> so that words with similar meanings would be represented by vectors that are close together in vector space.</p></li>
<li><p>These representations may also capture some <code class="docutils literal notranslate"><span class="pre">morphological</span></code> and <code class="docutils literal notranslate"><span class="pre">syntactic</span></code> information about words. (part of speech, inflections, stems, etc.)</p></li>
</ul>
<section id="the-distributional-hypothesis">
<h4>The Distributional Hypothesis<a class="headerlink" href="#the-distributional-hypothesis" title="Permalink to this headline">#</a></h4>
<p>Zellig Harris (1954):</p>
<ul class="simple">
<li><p>Words that occur in similar contexts have similar meanings.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">oculist</span></code> and <code class="docutils literal notranslate"><span class="pre">eye</span> <span class="pre">doctor</span></code> occur in almost the same contexts</p></li>
<li><p>If A and B have almost the same environment, then A and B are synonymous.</p></li>
</ul>
<p>John Firth (1957):</p>
<ul class="simple">
<li><p>You shall know a word by the company it keeps.</p></li>
</ul>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">contexts</span></code> in which words occur tell us a lot about the meaning of words.</p>
<p>Words that occur in similar contexts have similar meanings.</p>
</div></blockquote>
</section>
<section id="why-do-we-care-about-word-contexts">
<h4>Why do we care about word contexts?<a class="headerlink" href="#why-do-we-care-about-word-contexts" title="Permalink to this headline">#</a></h4>
<p>What is <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code>?</p>
<ul class="simple">
<li><p>A bottle of <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code> is on the table.</p></li>
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Tezgüino</span></code> makes you drunk.</p></li>
<li><p>We make <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code> out of corn.</p></li>
</ul>
<p>We don’t know what <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code> is, but we can guess that it is a drink because we understand these sentences.</p>
<p>If we have the following sentences:</p>
<ul class="simple">
<li><p>A bottle of <code class="docutils literal notranslate"><span class="pre">wine</span></code> is on the table.</p></li>
<li><p>There is a <code class="docutils literal notranslate"><span class="pre">beer</span></code> bottle on the table</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Beer</span></code> makes you drunk.</p></li>
<li><p>We make <code class="docutils literal notranslate"><span class="pre">bourbon</span></code> out of corn.</p></li>
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">chocolate</span></code></p></li>
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">babies</span></code></p></li>
</ul>
<p>Could we guess that <code class="docutils literal notranslate"><span class="pre">tezgüino</span></code> is a drink like <code class="docutils literal notranslate"><span class="pre">wine</span></code> or <code class="docutils literal notranslate"><span class="pre">beer</span></code>?</p>
<p>However, there are also red herrings:</p>
<ul class="simple">
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">babies</span></code></p></li>
<li><p>Everybody likes <code class="docutils literal notranslate"><span class="pre">chocolate</span></code></p></li>
</ul>
</section>
</section>
<section id="two-ways-nlp-uses-context-for-semantics">
<h3>Two ways NLP uses context for semantics<a class="headerlink" href="#two-ways-nlp-uses-context-for-semantics" title="Permalink to this headline">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Distributional</span> <span class="pre">similarity</span></code>: (vector-space semantics)</p>
<ul class="simple">
<li><p>Assume that words that occur in similar contexts have similar meanings.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">set</span> <span class="pre">of</span> <span class="pre">all</span> <span class="pre">contexts</span></code> in which a word occurs to measure the <code class="docutils literal notranslate"><span class="pre">similarity</span></code> between words.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Word</span> <span class="pre">sense</span> <span class="pre">disambiguation</span></code>:</p>
<ul class="simple">
<li><p>Assume that if a word has multiple meanings, then it will occur in different contexts for each meaning.</p></li>
<li><p>Use the context of a particular occurrence of a word to identify the <code class="docutils literal notranslate"><span class="pre">sense</span></code> of the word in that context.</p></li>
</ul>
</section>
</section>
<section id="distributional-similarity">
<h2>Distributional Similarity<a class="headerlink" href="#distributional-similarity" title="Permalink to this headline">#</a></h2>
<section id="basic-idea">
<h3>Basic idea<a class="headerlink" href="#basic-idea" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Measure the semantic <code class="docutils literal notranslate"><span class="pre">similarities</span> <span class="pre">of</span> <span class="pre">words</span></code> by measuring the <code class="docutils literal notranslate"><span class="pre">similarity</span> <span class="pre">of</span> <span class="pre">their</span> <span class="pre">contexts</span></code> in which they occur</p></li>
</ul>
</section>
<section id="how">
<h3>How?<a class="headerlink" href="#how" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Represent words as <code class="docutils literal notranslate"><span class="pre">sparse</span> <span class="pre">vectors</span></code> such that:</p>
<ul>
<li><p>each <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">element</span></code> (dimension) represents a different <code class="docutils literal notranslate"><span class="pre">context</span></code></p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">value</span></code> of each element is the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the context in which the word occurs, capturing how <code class="docutils literal notranslate"><span class="pre">strongly</span></code> the word is associated with that context</p></li>
</ul>
</li>
<li><p>Compute the <code class="docutils literal notranslate"><span class="pre">semantic</span> <span class="pre">similarity</span> <span class="pre">of</span> <span class="pre">words</span></code> by measuring the <code class="docutils literal notranslate"><span class="pre">similarity</span> <span class="pre">of</span> <span class="pre">their</span> <span class="pre">context</span> <span class="pre">vectors</span></code></p></li>
</ul>
<p>Distributional similarities represent each word <span class="math notranslate nohighlight">\(w\)</span> as a vector <span class="math notranslate nohighlight">\(v_w\)</span> of context counts:</p>
<div class="math notranslate nohighlight">
\[w = (w_1 , \ldots , w_N ) \in R^N\]</div>
<p>in a vector space <span class="math notranslate nohighlight">\(R^N\)</span> where <span class="math notranslate nohighlight">\(N\)</span> is the number of contexts.</p>
<ul class="simple">
<li><p>each dimension <span class="math notranslate nohighlight">\(i\)</span> represents a different context <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
<li><p>each element <span class="math notranslate nohighlight">\(v_{w,i}\)</span> captures how strongly <span class="math notranslate nohighlight">\(w\)</span> is associated with context <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v_{w,i}\)</span> is the co-occurrence count of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
</ul>
</section>
<section id="the-information-retrieval-perspective-the-term-document-matrix">
<h3>The Information Retrieval perspective: The Term-Document Matrix<a class="headerlink" href="#the-information-retrieval-perspective-the-term-document-matrix" title="Permalink to this headline">#</a></h3>
<p>In information retrieval, we search a collection of <span class="math notranslate nohighlight">\(N\)</span> documents for <span class="math notranslate nohighlight">\(M\)</span> terms:</p>
<ul class="simple">
<li><p>We can represent each <code class="docutils literal notranslate"><span class="pre">word</span></code> in the vocabulary <span class="math notranslate nohighlight">\(V\)</span> as an <span class="math notranslate nohighlight">\(N\)</span>-dimensional vector <span class="math notranslate nohighlight">\(v_w\)</span> where <span class="math notranslate nohighlight">\(v_{w,i}\)</span> is the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the word <span class="math notranslate nohighlight">\(w\)</span> in document <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p>Conversely, we can represent each <code class="docutils literal notranslate"><span class="pre">document</span></code> as an <span class="math notranslate nohighlight">\(M\)</span>-dimensional vector <span class="math notranslate nohighlight">\(v_d\)</span> where <span class="math notranslate nohighlight">\(v_{d,j}\)</span> is the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the term <span class="math notranslate nohighlight">\(t_j\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>.</p></li>
</ul>
<p>Finding the <code class="docutils literal notranslate"><span class="pre">most</span> <span class="pre">relevant</span></code> documents for a query <span class="math notranslate nohighlight">\(q\)</span> is equivalent to finding the <code class="docutils literal notranslate"><span class="pre">most</span> <span class="pre">similar</span></code> documents to the query vector <span class="math notranslate nohighlight">\(v_q\)</span>.</p>
<ul class="simple">
<li><p>Queries are also documents, so we can use the same vector representation for queries and documents.</p></li>
<li><p>Use the similarity of the query vector <span class="math notranslate nohighlight">\(v_q\)</span> to the document vectors <span class="math notranslate nohighlight">\(v_d\)</span> to rank the documents.</p></li>
<li><p>Documents are similar to queries if they have similar terms.</p></li>
</ul>
</section>
</section>
<section id="term-document-matrix">
<h2>Term-Document Matrix<a class="headerlink" href="#term-document-matrix" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../../../_images/214.png" /></p>
<p>A term-document matrix is a 2D matrix:</p>
<ul class="simple">
<li><p>each row represents a <code class="docutils literal notranslate"><span class="pre">term</span></code> in the vocabulary</p></li>
<li><p>each column represents a <code class="docutils literal notranslate"><span class="pre">document</span></code></p></li>
<li><p>each element is the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the term in the document</p></li>
</ul>
<p><img alt="" src="../../../_images/38.png" /></p>
<ul class="simple">
<li><p>Each <code class="docutils literal notranslate"><span class="pre">column</span> <span class="pre">vector</span></code> = a <code class="docutils literal notranslate"><span class="pre">document</span></code></p>
<ul>
<li><p>Each entry = the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the term in the document</p></li>
</ul>
</li>
<li><p>Each <code class="docutils literal notranslate"><span class="pre">row</span> <span class="pre">vector</span></code> = a <code class="docutils literal notranslate"><span class="pre">term</span></code></p>
<ul>
<li><p>Each entry = the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> of the term in the document</p></li>
</ul>
</li>
</ul>
<blockquote>
<div><p>Two documents are similar if their vectors are similar.</p>
</div></blockquote>
<blockquote>
<div><p>Two words are similar if their vectors are similar.</p>
</div></blockquote>
<p>For information retrieval, the term-document matrix is useful because it allows us to represent documents as vectors and compute the similarity between documents in terms of the words they contain, or of terms in terms of the documents they occur in.</p>
<p>We can adapt this idea to implement <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">model</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">distributional</span> <span class="pre">hypothesis</span></code> if we treat each context as a column in the matrix and each word as a row.</p>
<section id="what-is-a-context">
<h3>What is a <code class="docutils literal notranslate"><span class="pre">context</span></code>?<a class="headerlink" href="#what-is-a-context" title="Permalink to this headline">#</a></h3>
<p>There are many ways to define a context:</p>
<p><strong>Contexts defined by nearby words:</strong></p>
<ul class="simple">
<li><p>How often does the word <span class="math notranslate nohighlight">\(w_i\)</span> occur within a window of <span class="math notranslate nohighlight">\(k\)</span> words of the word <span class="math notranslate nohighlight">\(w_j\)</span>?</p></li>
<li><p>Or, how often do the words <span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(w_j\)</span> occur in the same document or sentence?</p></li>
<li><p>This yields fairly broad thematic similarities between words.</p></li>
</ul>
<p><strong>Contexts defined by <code class="docutils literal notranslate"><span class="pre">grammtical</span> <span class="pre">relations</span></code>:</strong></p>
<ul class="simple">
<li><p>How often does the word <span class="math notranslate nohighlight">\(w_i\)</span> occur as the <code class="docutils literal notranslate"><span class="pre">subject</span></code> of the word <span class="math notranslate nohighlight">\(w_j\)</span>?</p></li>
<li><p>This requires a <code class="docutils literal notranslate"><span class="pre">grammatical</span> <span class="pre">parser</span></code> to identify the grammatical relations between words.</p></li>
<li><p>This yields more <code class="docutils literal notranslate"><span class="pre">fine-grained</span></code> similarities between words.</p></li>
</ul>
</section>
<section id="using-nearby-words-as-contexts">
<h3>Using nearby words as contexts<a class="headerlink" href="#using-nearby-words-as-contexts" title="Permalink to this headline">#</a></h3>
<ol class="simple">
<li><p>Define a fixed vocabulary of <span class="math notranslate nohighlight">\(N\)</span> context words <span class="math notranslate nohighlight">\(c_1 , \ldots , c_N\)</span></p></li>
</ol>
<ul class="simple">
<li><p>Contexts words should occur frequently enough in the corpus that you can get reliable counts.</p></li>
<li><p>However, you should ignore very frequent words (stopwords) like <code class="docutils literal notranslate"><span class="pre">the</span></code> and <code class="docutils literal notranslate"><span class="pre">a</span></code> because they are not very informative.</p></li>
</ul>
<ol class="simple">
<li><p>Define what <code class="docutils literal notranslate"><span class="pre">nearby</span></code> means:</p></li>
</ol>
<ul class="simple">
<li><p>For example, we can define a <code class="docutils literal notranslate"><span class="pre">window</span></code> of <span class="math notranslate nohighlight">\(k\)</span> words on either side of the word <span class="math notranslate nohighlight">\(w_j\)</span>.</p></li>
</ul>
<ol class="simple">
<li><p>Count the number of times each context word <span class="math notranslate nohighlight">\(c_i\)</span> occurs within a window of <span class="math notranslate nohighlight">\(k\)</span> words of the word <span class="math notranslate nohighlight">\(w_j\)</span>.</p></li>
<li><p>Define how to transform the co-occurrence counts into a vector representation of the word <span class="math notranslate nohighlight">\(w_j\)</span>.</p></li>
</ol>
<ul class="simple">
<li><p>For example, we can use the (positive) <code class="docutils literal notranslate"><span class="pre">PMI</span></code> of the word <span class="math notranslate nohighlight">\(w_j\)</span> and the context word <span class="math notranslate nohighlight">\(c_i\)</span>.</p></li>
</ul>
<ol class="simple">
<li><p>Compute the similarity between words by measuring the similarity of their context vectors.</p></li>
</ol>
<ul class="simple">
<li><p>For example, we can use the cosine similarity of the context vectors.</p></li>
</ul>
</section>
<section id="word-word-matrix">
<h3>Word-Word Matrix<a class="headerlink" href="#word-word-matrix" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../../../_images/43.png" /></p>
<p>Resulting word-word matrix:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(w, c)\)</span> = how often does word w appear in context <span class="math notranslate nohighlight">\(c\)</span>:</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">information</span></code> appeared six times in the context of <code class="docutils literal notranslate"><span class="pre">data</span></code></p></li>
</ul>
<p><img alt="" src="../../../_images/53.png" /></p>
</section>
<section id="defining-co-occurrences">
<h3>Defining co-occurrences:<a class="headerlink" href="#defining-co-occurrences" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Within</span> <span class="pre">a</span> <span class="pre">fixed</span> <span class="pre">window</span></code>: <span class="math notranslate nohighlight">\(c_i\)</span> occurs within <span class="math notranslate nohighlight">\(\pm n\)</span> words of <span class="math notranslate nohighlight">\(w\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Within</span> <span class="pre">the</span> <span class="pre">same</span> <span class="pre">sentence</span></code>: requires sentence boundaries</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">By</span> <span class="pre">grammatical</span> <span class="pre">relations</span></code>: <span class="math notranslate nohighlight">\(c_i\)</span> occurs as a subject/object/modifier/… of verb <span class="math notranslate nohighlight">\(w\)</span> (requires parsing - and separate features for each relation)</p></li>
</ul>
</section>
<section id="representing-co-occurrences">
<h3>Representing co-occurrences:<a class="headerlink" href="#representing-co-occurrences" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_i\)</span> as<code class="docutils literal notranslate"> <span class="pre">binary</span> <span class="pre">features</span></code> (1,0): <span class="math notranslate nohighlight">\(w\)</span> does/does not occur with <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f_i\)</span> as frequencies: <span class="math notranslate nohighlight">\(w\)</span> occurs <span class="math notranslate nohighlight">\(n\)</span> times with <span class="math notranslate nohighlight">\(c_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f_i\)</span> as probabilities: e.g. <span class="math notranslate nohighlight">\(f_i\)</span> is the probability that <span class="math notranslate nohighlight">\(c_i\)</span> is the subject of <span class="math notranslate nohighlight">\(w\)</span>.</p></li>
</ul>
</section>
<section id="getting-co-occurrence-counts">
<h3>Getting co-occurrence counts<a class="headerlink" href="#getting-co-occurrence-counts" title="Permalink to this headline">#</a></h3>
<p>Co-occurrence as a <code class="docutils literal notranslate"><span class="pre">binary</span></code> feature:</p>
<ul class="simple">
<li><p>Does word <span class="math notranslate nohighlight">\(w\)</span> ever appear in the context <span class="math notranslate nohighlight">\(c\)</span>? (1 = yes/0 = no)</p></li>
</ul>
<p><img alt="" src="../../../_images/62.png" /></p>
<p>Co-occurrence as a frequency count:</p>
<ul class="simple">
<li><p>How often does word <span class="math notranslate nohighlight">\(w\)</span> appear in the context <span class="math notranslate nohighlight">\(c\)</span>? (0,1,2,… times)</p></li>
</ul>
<p><img alt="" src="../../../_images/72.png" /></p>
</section>
<section id="counts-vs-pmi">
<h3>Counts vs PMI<a class="headerlink" href="#counts-vs-pmi" title="Permalink to this headline">#</a></h3>
<p>Sometimes, low co-occurrences counts are very informative, and high co-occurrence counts are not:</p>
<ul class="simple">
<li><p>Any word is going to have relatively high co-occurrence counts with very common contexts (e.g. <code class="docutils literal notranslate"><span class="pre">the</span></code> with <code class="docutils literal notranslate"><span class="pre">a</span></code>), but this won’t tell us much about what that word means.</p></li>
<li><p>We need to identify when co-occurrence counts are higher than we would expect by chance.</p></li>
</ul>
<p>We can use pointwise mutual information (PMI) values instead of raw frequency counts:</p>
<div class="math notranslate nohighlight">
\[ PMI(w,c) = \log \frac{p(w,c)}{p(w)p(c)} \]</div>
<p><img alt="" src="../../../_images/83.png" /></p>
</section>
<section id="computing-pmi-of-w-and-c">
<h3>Computing PMI of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(c\)</span>:<a class="headerlink" href="#computing-pmi-of-w-and-c" title="Permalink to this headline">#</a></h3>
<section id="using-a-fixed-window-of-pm-k-words">
<h4>Using a fixed window of <span class="math notranslate nohighlight">\(\pm k\)</span> words<a class="headerlink" href="#using-a-fixed-window-of-pm-k-words" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span>: How many tokens does the corpus contain?</p></li>
<li><p><span class="math notranslate nohighlight">\(f(w) \le N\)</span>: How often does <span class="math notranslate nohighlight">\(w\)</span> occur?</p></li>
<li><p><span class="math notranslate nohighlight">\(f(w, c) \le f(w)\)</span>: How often does <span class="math notranslate nohighlight">\(w\)</span> occur with <span class="math notranslate nohighlight">\(c\)</span> in its window?</p></li>
<li><p><span class="math notranslate nohighlight">\(f(c) = \sum_w f(w, c)\)</span>: How many tokens have <span class="math notranslate nohighlight">\(c\)</span> in their window?</p></li>
</ul>
<div class="math notranslate nohighlight">
\[p(w) = \frac{f{w}}{N}, p(c) = \frac{f(c)}{N}, p(w,c) = \frac{f(w,c)}{N}\]</div>
<div class="math notranslate nohighlight">
\[PMI(w,c) = \log \frac{p(w,c)}{p(w)p(c)}\]</div>
<p>Positive Pointwise Mutual Information</p>
<p>PMI is negative when words co-occur less than expected by chance.</p>
<ul class="simple">
<li><p>This is unreliable without huge corpora:</p></li>
<li><p>With <span class="math notranslate nohighlight">\(P(w ) \approx P(w2 ) \approx 10^{−6}\)</span> , we can’t estimate whether <span class="math notranslate nohighlight">\(P(w_1 , w_2 )\)</span> is significantly different from <span class="math notranslate nohighlight">\(10^{−12}\)</span></p></li>
</ul>
<p>We often just use positive PMI values, and replace all negative PMI values with 0:</p>
<p>Positive Pointwise Mutual Information (PPMI):</p>
<div class="math notranslate nohighlight">
\[
\text{PPMI}(w, c) = PMI, \text{ if } \text{PMI}(w, c) \gt 0
\]</div>
<div class="math notranslate nohighlight">
\[
\text{PPMI}(w, c) = 0, \text{ if } \text{PMI}(w, c) \le 0
\]</div>
<p>PMI and smoothing</p>
<p>PMI is biased towards infrequent events:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(P(w, c) = P(w) = P(c)\)</span>, then <span class="math notranslate nohighlight">\(\text{PMI}(w, c) = \log (\frac{1}{P(w)})\)</span></p></li>
<li><p>So <span class="math notranslate nohighlight">\(\text{PMI}(w, c)\)</span> is larger for rare words <span class="math notranslate nohighlight">\(w\)</span> with low <span class="math notranslate nohighlight">\(P(w)\)</span>.</p></li>
</ul>
<p>Simple remedy: <code class="docutils literal notranslate"><span class="pre">Add-k</span> <span class="pre">smoothing</span></code> of <span class="math notranslate nohighlight">\(P(w, c), P(w), P(c)\)</span> pushes all PMI values towards zero.</p>
<ul class="simple">
<li><p>Add-k smoothing affects low-probability events more, and will therefore reduce the bias of PMI towards infrequent events. (Pantel &amp; Turney 2010)</p></li>
</ul>
</section>
</section>
</section>
<section id="dot-product-as-similarity">
<h2>Dot product as similarity<a class="headerlink" href="#dot-product-as-similarity" title="Permalink to this headline">#</a></h2>
<p>If the vectors consist of simple binary features (0,1), we can use the <code class="docutils literal notranslate"><span class="pre">dot</span> <span class="pre">product</span></code> as <code class="docutils literal notranslate"><span class="pre">similarity</span> <span class="pre">metric</span></code>:</p>
<div class="math notranslate nohighlight">
\[
sim_{dot-prod}(\vec{x}\cdot\vec{y}) = \sum_{i=1}^{N} x_i \times y_i
\]</div>
<p>The dot product is a bad metric if the vector elements are arbitrary features: it prefers <code class="docutils literal notranslate"><span class="pre">long</span></code> vectors</p>
<ul class="simple">
<li><p>If one <span class="math notranslate nohighlight">\(x_i\)</span> is very large (and <span class="math notranslate nohighlight">\(y_i\)</span> nonzero), <span class="math notranslate nohighlight">\(sim(x, y)\)</span> gets very large</p></li>
<li><p>If the number of nonzero <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(y_i\)</span> is very large, <span class="math notranslate nohighlight">\(sim(x, y)\)</span> gets very large.</p></li>
<li><p>Both can happen with frequent words.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{length of }\vec{x}: |\vec{x}|=\sqrt{\sum_{i=1}^{N}x_i^2}
\]</div>
</section>
<section id="vector-similarity-cosine">
<h2>Vector similarity: Cosine<a class="headerlink" href="#vector-similarity-cosine" title="Permalink to this headline">#</a></h2>
<p>One way to define the similarity of two vectors is to use the cosine of their angle.</p>
<p>The cosine of two vectors is their dot product, divided by the product of their lengths:</p>
<div class="math notranslate nohighlight">
\[
sim_{cos}(\vec{x},\vec{y})=\frac{\sum_{i=1}^{N} x_i \times y_i}{\sqrt{\sum_{i=1}^{N}x_i^2}\sqrt{\sum_{i=1}^{N}y_i^2}} = \frac{\vec{x}\cdot\vec{y}}{|\vec{x}||\vec{y}|}
\]</div>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(sim(\mathbf{w}, \mathbf{u}) = 1\)</span>: <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> point in the same direction</p>
</div></blockquote>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(sim(\mathbf{w}, \mathbf{u}) = 0\)</span>: <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> are orthogonal</p>
</div></blockquote>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(sim(\mathbf{w}, \mathbf{u}) = -1\)</span>: <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> point in the opposite direction</p>
</div></blockquote>
<section id="visualizing-cosines">
<h3>Visualizing cosines<a class="headerlink" href="#visualizing-cosines" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../../../_images/cosine-viz.png" /></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/lectures/intro_nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="word_segmentation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Word Segmentation and Association</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="word_embeddings.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Word Embeddings</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Young Joon Lee<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>