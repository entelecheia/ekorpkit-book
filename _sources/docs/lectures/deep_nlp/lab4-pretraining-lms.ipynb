{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Pretraining Language Models\n",
    "\n",
    "![](../figs/deep_nlp/lab/train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset and tokenizer, in this lab, we will train a language model on a large corpus of text from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "%pip install --pre ekorpkit[model]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.utils.notebook:Google Colab not detected.\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_WORKSPACE_ROOT to /content/drive/MyDrive/workspace\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_PROJECT to ekorpkit-book\n",
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0.1.40.post0.dev22\n",
      "is colab? False\n",
      "project_dir: /content/drive/MyDrive/workspace/projects/ekorpkit-book\n",
      "time: 1.45 s (started: 2022-11-22 01:27:10 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ekorpkit import eKonf\n",
    "\n",
    "eKonf.setLogger(\"INFO\")\n",
    "print(\"version:\", eKonf.__version__)\n",
    "\n",
    "is_colab = eKonf.is_colab()\n",
    "print(\"is colab?\", is_colab)\n",
    "if is_colab:\n",
    "    eKonf.mount_google_drive()\n",
    "workspace_dir = \"/content/drive/MyDrive/workspace\"\n",
    "project_name = \"ekorpkit-book\"\n",
    "project_dir = eKonf.set_workspace(workspace=workspace_dir, project=project_name)\n",
    "print(\"project_dir:\", project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e221b829f9574fbf988c75775e1603e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 70.7 ms (started: 2022-11-21 00:59:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id 'entelecheia' will be used during this lab\n",
      "time: 893 ms (started: 2022-11-21 01:06:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "token = HfFolder.get_token()\n",
    "if token is None:\n",
    "    token = os.environ[\"HF_USER_ACCESS_TOKEN\"]\n",
    "\n",
    "if token is None:\n",
    "    raise ValueError(\"Please login to huggingface_hub\")\n",
    "\n",
    "user_id = HfApi().whoami(token)[\"name\"]\n",
    "\n",
    "print(f\"user id '{user_id}' will be used during this lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Normalization\n",
    "\n",
    "One little thing to note is that we will need to normalize our text before training our language model. This is because the same character can be represented in different ways. For example, the character \"é\" can be represented as \"e\" followed by a combining accent character, or as a single character.\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "Use `NFKC` normalization to normalize your text before training your language model.\n",
    "\n",
    "\n",
    "### Unicode Normalization Forms\n",
    "\n",
    "There are four normalization forms:\n",
    "\n",
    "- **NFC**: Normalization Form Canonical Composition\n",
    "- **NFD**: Normalization Form Canonical Decomposition\n",
    "- **NFKC**: Normalization Form Compatibility Composition\n",
    "- **NFKD**: Normalization Form Compatibility Decomposition\n",
    "\n",
    "In the above forms, \"C\" stands for \"Canonical\" and \"K\" stands for \"Compatibility\". The \"C\" forms are the most commonly used. The \"K\" forms are used when you need to convert characters to their compatibility representation. For example, the \"K\" forms will convert \"ﬁ\" to \"fi\".\n",
    "\n",
    "There two main differences between the two sets of forms:\n",
    "\n",
    "- The length of the string is changed or not: NFC and NFKC always produce a string of the same length or shorter, while NFD and NFKD may produce a string that is longer.\n",
    "- The original string is changed or not: NFC and NFD always produce a string that is identical to the original string, while NFKC and NFKD may produce a string that is different from the original string.\n",
    "\n",
    "### Unicode Normalization in Python\n",
    "\n",
    "In Python, you can use the `unicodedata` module to normalize your text. The `unicodedata.normalize` function takes two arguments:\n",
    "\n",
    "- `form`: The normalization form to use. This can be one of the following: `NFC`, `NFD`, `NFKC`, `NFKD`.\n",
    "- `unistr`: The string to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ａｂｃＡＢＣ１２３가나다…, 13\n",
      "NFC: ａｂｃＡＢＣ１２３가나다…, 13\n",
      "NFD: ａｂｃＡＢＣ１２３가나다…, 16\n",
      "NFKC: abcABC123가나다..., 15\n",
      "NFKD: abcABC123가나다..., 18\n",
      "time: 23.8 ms (started: 2022-11-19 10:16:57 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "text = \"ａｂｃＡＢＣ１２３가나다…\"\n",
    "print(f\"Original: {text}, {len(text)}\")\n",
    "for form in [\"NFC\", \"NFD\", \"NFKC\", \"NFKD\"]:\n",
    "    ntext = unicodedata.normalize(form, text)\n",
    "    print(f\"{form}: {ntext}, {len(ntext)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Pretraining\n",
    "\n",
    "In this lab, we will train a BERT-like model using masked-language modeling, one of the two pretraining tasks used in the original BERT paper.\n",
    "\n",
    "### What is BERT?\n",
    "\n",
    "BERT is a large-scale language model that was trained on the English Wikipedia using a masked-language modeling objective. The model was then fine-tuned on a variety of downstream tasks, including question answering, natural language inference, and sentiment analysis. BERT was the first large-scale language model to be pre-trained using a deep bidirectional architecture and outperformed previous language models on a variety of tasks.\n",
    "\n",
    "BERT was originally pre-trained on 1 Million Steps with a global batch size of 256.\n",
    "\n",
    "> \"We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus.\"\n",
    "\n",
    "For more information, see the lecture notes on BERT.\n",
    "\n",
    "### Masked-Language Modeling (MLM)\n",
    "\n",
    "Masked-language modeling is a pretraining task where we mask some of the input tokens and train the model to predict the original value of the masked tokens. For example, if we have the sentence \"The dog ate the apple\", we can mask the word \"ate\" and train the model to predict the original value of the masked token. The model will then learn to predict the original value of the masked tokens based on the context of the sentence.\n",
    "\n",
    "Example:\n",
    "\n",
    "> Input: \"The dog [MASK] the apple\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "Before training our language model, we need to preprocess our dataset. We will use our tokenizer to tokenize our dataset and then convert the tokens to their IDs. If we have a sentence that is longer than the maximum sequence length, we will truncate the sentence. If the sentence is shorter than the maximum sequence length, we will pad the sentence with the padding token.\n",
    "\n",
    "Unlike the original BERT paper, we will not use the WordPiece tokenization algorithm. Instead, we will use the `unigram` tokenization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "is_fast: True\n",
      "Vocab size: 30000\n",
      "{'input_ids': [1, 8, 14690, 10, 8, 968, 8, 6871, 8, 42, 8, 2777, 72, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 67 ms (started: 2022-11-19 10:55:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer_path = \"tokenizers/enko_wiki/enko_wiki_unigram_huggingface_vocab_30000.json\"\n",
    "tokenizer_path = project_dir + \"/\" + tokenizer_path\n",
    "context_length = 512\n",
    "\n",
    "unigram_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(f\"Vocab size: {unigram_tokenizer.get_vocab_size()}\")\n",
    "unigram_tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", unigram_tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", unigram_tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=unigram_tokenizer,\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_length=True,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "\n",
    "print(f\"is_fast: {tokenizer.is_fast}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(tokenizer(\"Hello, my dog is cute\"))\n",
    "tokenizer.save_pretrained(project_dir + \"/tokenizers/enko_wiki\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fast: True\n",
      "time: 59.6 ms (started: 2022-11-21 01:03:21 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    project_dir + \"/tokenizers/enko_wiki\"\n",
    ")\n",
    "print(f\"is_fast: {tokenizer.is_fast}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 8, 14690, 10, 8, 235, 8, 202, 8, 15219, 489, 2, 8, 37, 8, 235, 8, 15219, 8, 11241, 8, 80, 8, 65, 9, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.2 ms (started: 2022-11-21 01:07:07 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeddc62f7db4f55a039916fbd57d972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-f34802e795f4ed05\n",
      "WARNING:datasets.builder:Reusing dataset text (/workspace/data/tbts/.cache/huggingface/datasets/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3618972\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.02 s (started: 2022-11-19 09:39:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_dir = project_dir + \"/data/tokenizers/enko_filtered_chunk\"\n",
    "\n",
    "dataset = load_dataset(\"text\", data_dir=data_dir, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.3 ms (started: 2022-11-19 09:55:12 +00:00)\n"
     ]
    }
   ],
   "source": [
    "text_column = \"text\"\n",
    "\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[text_column],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = 20\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=[text_column], num_proc=num_proc\n",
    ")\n",
    "tokenized_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= context_length:\n",
    "        total_length = (total_length // context_length) * context_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + context_length] for i in range(0, total_length, context_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=num_proc)\n",
    "\n",
    "# shuffle dataset\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=1234)\n",
    "\n",
    "print(f\"the dataset contains in total {len(tokenized_dataset)*context_length} tokens\")\n",
    "# the dataset contains in total 137,816,832 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 137,816,832 tokens in our dataset. For reference, the original BERT paper used 3.2 billion tokens, and GPT-3 uses 300 billion tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a New Model\n",
    "\n",
    "We will initialize a new model using the `bert-base-uncased` configuration. We will then save the configuration to a file so that we can use it later when we load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.4 s (started: 2022-11-19 11:29:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "tk_path = project_dir + \"/tokenizers/enko_wiki\"\n",
    "\n",
    "# Load codeparrot tokenizer trained for Python code tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(tk_path)\n",
    "\n",
    "# Configuration\n",
    "config_kwargs = {\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"mask_token_id\": tokenizer.mask_token_id,\n",
    "    \"cls_token_id\": tokenizer.cls_token_id,\n",
    "    \"sep_token_id\": tokenizer.sep_token_id,\n",
    "    \"unk_token_id\": tokenizer.unk_token_id,\n",
    "}\n",
    "\n",
    "# # Load model with config and push to hub\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\", **config_kwargs)\n",
    "model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "model_path = project_dir + \"/models/enko_wiki_bert_base_uncased\"\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model have 109.1 million parameters just like the original BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT size: 109.1M parameters\n",
      "time: 2.07 s (started: 2022-11-19 11:23:15 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"BERT size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a DataCollator\n",
    "\n",
    "Before we can start training, we need to set up a data collator that will be used to collate the batches of data. We will use the `DataCollatorForLanguageModeling` data collator, which will take care of masking the tokens and padding the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 01:24:31.069901: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.59 s (started: 2022-11-21 01:24:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Tokenized Dataset\n",
    "\n",
    "Our dataset is already tokenized, there are 268,366 examples in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 268366\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 269 ms (started: 2022-11-21 01:24:33 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset_dir = project_dir + \"/data/tokenized_datasets/enko_filtered\"\n",
    "\n",
    "tokenized_dataset = Dataset.load_from_disk(dataset_dir)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output of the first batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 512])\n",
      "token_type_ids shape: torch.Size([5, 512])\n",
      "attention_mask shape: torch.Size([5, 512])\n",
      "labels shape: torch.Size([5, 512])\n",
      "time: 70 ms (started: 2022-11-21 01:24:35 +00:00)\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_dataset[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We will configure the training arguments and then set up a trainer to train our model.\n",
    "\n",
    "### Configure the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.43 s (started: 2022-11-19 11:34:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "time: 47.9 ms (started: 2022-11-19 11:34:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "acc_state = {str(k): str(v) for k, v in accelerator.state.__dict__.items()}\n",
    "device = accelerator.device\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "trainer = accelerator.prepare(trainer)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 6h 33m 0.0s to train our model for 40 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "We will load our model and test it on a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:652] 2022-11-22 19:14:02,688 >> loading configuration file /content/drive/MyDrive/workspace/projects/ekorpkit-book/models/enko_wiki_bert_base_uncased/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 19:14:02,690 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/workspace/projects/ekorpkit-book/models/enko_wiki_bert_base_uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 4,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 6,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2155] 2022-11-22 19:14:02,691 >> loading weights file /content/drive/MyDrive/workspace/projects/ekorpkit-book/models/enko_wiki_bert_base_uncased/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2608] 2022-11-22 19:14:04,867 >> All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:2616] 2022-11-22 19:14:04,873 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/workspace/projects/ekorpkit-book/models/enko_wiki_bert_base_uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 19:14:04,888 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 19:14:04,888 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 19:14:04,889 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 19:14:04,889 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.3 s (started: 2022-11-22 19:14:02 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_path = project_dir + \"/models/enko_wiki_bert_base_uncased\"\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.1881457269191742, 'token': 1183, 'token_str': '만든', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 만든 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.14176464080810547, 'token': 3567, 'token_str': '제작한', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 제작한 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.04182405769824982, 'token': 15022, 'token_str': '롤플레잉', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 롤플레잉 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.035137832164764404, 'token': 3225, 'token_str': '개발한', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 개발한 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.033710286021232605, 'token': 3171, 'token_str': '아시안', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 아시안 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "time: 223 ms (started: 2022-11-22 19:14:05 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "example = \"처음으로 대중적으로 <mask> 롤플레잉 게임이다.\"\n",
    "for prediction in fill_mask(example):\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of MLM Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.utils.notebook:Google Colab not detected.\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_WORKSPACE_ROOT to /content/drive/MyDrive/workspace\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_PROJECT to ekorpkit-book\n",
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0.1.40.post0.dev25\n",
      "is colab? False\n",
      "project_dir: /content/drive/MyDrive/workspace/projects/ekorpkit-book\n",
      "time: 1.35 s (started: 2022-11-24 01:14:09 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ekorpkit import eKonf\n",
    "\n",
    "eKonf.setLogger(\"INFO\")\n",
    "print(\"version:\", eKonf.__version__)\n",
    "\n",
    "is_colab = eKonf.is_colab()\n",
    "print(\"is colab?\", is_colab)\n",
    "if is_colab:\n",
    "    eKonf.mount_google_drive()\n",
    "workspace_dir = \"/content/drive/MyDrive/workspace\"\n",
    "project_name = \"ekorpkit-book\"\n",
    "project_dir = eKonf.set_workspace(workspace=workspace_dir, project=project_name)\n",
    "print(\"project_dir:\", project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 01:05:45.351000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n",
      "WARNING:ekorpkit.models.transformer.trainers.mlm:Process rank: -1, device: cuda:0, n_gpu: 8, distributed training: False, 16-bits training: True\n",
      "INFO:ekorpkit.models.transformer.trainers.mlm:Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=8,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=1000,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=None,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "optim=adamw_hf,\n",
      "output_dir=/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/models/enkowiki/enkowiki_bert-base-uncased,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=enkowiki,\n",
      "save_on_each_node=False,\n",
      "save_steps=5000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=2000801245,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=1000,\n",
      "weight_decay=0.1,\n",
      "xpu_backend=None,\n",
      ")\n",
      "INFO:ekorpkit.base:No method defined to call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.62 s (started: 2022-11-24 01:05:45 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.models.transformer.trainers import MlmTrainer\n",
    "\n",
    "data_dir = project_dir + \"/data/tokenizers/enko_filtered_chunk\"\n",
    "tokenizer_path = \"tokenizers/enko_wiki/enko_wiki_unigram_huggingface_vocab_30000.json\"\n",
    "tokenizer_path = project_dir + \"/\" + tokenizer_path\n",
    "\n",
    "cfg = eKonf.compose(\"model/transformer=mlm.trainer\")\n",
    "cfg.name = \"enkowiki\"\n",
    "cfg.model.config_name = \"bert-base-uncased\"\n",
    "cfg.tokenizer.path = tokenizer_path\n",
    "cfg.dataset.train_file = data_dir\n",
    "cfg.dataset.max_seq_length = 512\n",
    "cfg.use_accelerator = True\n",
    "cfg.training.num_train_epochs = 10\n",
    "\n",
    "trainer = MlmTrainer(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.io.file:Processing [61] files from ['*.txt']\n",
      "[INFO|configuration_utils.py:654] 2022-11-22 12:04:16,761 >> loading configuration file config.json from cache at /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 12:04:16,763 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:ekorpkit.models.transformer.trainers.mlm:Update model config with {'vocab_size': 30000, 'pad_token_id': 4, 'mask_token_id': 3, 'unk_token_id': 7, 'sep_token_id': 6, 'cls_token_id': 5}\n",
      "INFO:ekorpkit.models.transformer.trainers.mlm:Is a fast tokenizer? True\n",
      "INFO:ekorpkit.models.transformer.trainers.mlm:Vocab size: 30000\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-22 12:04:16,833 >> tokenizer config file saved in /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-22 12:04:16,833 >> Special tokens file saved in /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 12:04:16,856 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 12:04:16,856 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 12:04:16,856 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 12:04:16,857 >> loading file tokenizer_config.json\n",
      "INFO:ekorpkit.models.transformer.trainers.mlm:Training new model from scratch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e7828f4a984770af67c0ece9730886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-f34802e795f4ed05\n",
      "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
      "INFO:datasets.info:Loading Dataset info from /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad\n",
      "WARNING:datasets.builder:Reusing dataset text (/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n",
      "INFO:datasets.info:Loading Dataset info from /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a1d62a6a394439b2dc6fcec42c0184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-f34802e795f4ed05\n",
      "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
      "INFO:datasets.info:Loading Dataset info from /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad\n",
      "WARNING:datasets.builder:Reusing dataset text (/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n",
      "INFO:datasets.info:Loading Dataset info from /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c505473904e04456b0cc1cfdcf9f67b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on every text in dataset:   0%|          | 0/181 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets.arrow_dataset:Caching processed dataset at /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-e6a9a7d78be420bf.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27fbca4a39247abbc65f6db2e2841d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on every text in dataset:   0%|          | 0/3439 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets.arrow_dataset:Caching processed dataset at /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-627ec14eedd38508.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d735bdeaa5374f91af6f207169b4836a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 512:   0%|          | 0/181 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets.arrow_dataset:Caching processed dataset at /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-e8f0776955bfab1c.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb438209d5994b5d9a01743d6e572c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 512:   0%|          | 0/3439 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets.arrow_dataset:Caching processed dataset at /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/cache/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-9092d54dda154dbb.arrow\n",
      "[INFO|trainer.py:557] 2022-11-22 12:16:50,065 >> Using cuda_amp half precision backend\n",
      "INFO:ekorpkit.models.transformer.trainers.mlm:Accelerator state: {'fork_launched': 'False', 'backend': 'None', 'deepspeed_plugin': 'None', 'distributed_type': 'DistributedType.NO', 'num_processes': '1', 'process_index': '0', 'local_process_index': '0', 'device': 'cuda', 'mixed_precision': 'no', 'initialized': 'True'}\n",
      "INFO:ekorpkit.models.transformer.trainers.mlm:Accelerator device: cuda\n",
      "INFO:ekorpkit.batch:Saving config to /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased/configs/enko_wiki_bert_base_uncased(1)_config.yaml\n",
      "[INFO|trainer.py:725] 2022-11-22 12:16:50,102 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:1608] 2022-11-22 12:16:50,118 >> ***** Running training *****\n",
      "[INFO|trainer.py:1609] 2022-11-22 12:16:50,118 >>   Num examples = 255023\n",
      "[INFO|trainer.py:1610] 2022-11-22 12:16:50,118 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1611] 2022-11-22 12:16:50,118 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1612] 2022-11-22 12:16:50,119 >>   Total train batch size (w. parallel, distributed & accumulation) = 2048\n",
      "[INFO|trainer.py:1613] 2022-11-22 12:16:50,119 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:1614] 2022-11-22 12:16:50,119 >>   Total optimization steps = 1240\n",
      "[INFO|trainer.py:1615] 2022-11-22 12:16:50,120 >>   Number of trainable parameters = 109112880\n",
      "[INFO|integrations.py:680] 2022-11-22 12:16:50,121 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mentelecheia\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/projects/ekorpkit-book/ekorpkit-book/docs/lectures/deep_nlp/wandb/run-20221122_121652-37kfjnp7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/entelecheia/ekorpkit-book/runs/37kfjnp7\" target=\"_blank\">enko_wiki_bert_base_uncased</a></strong> to <a href=\"https://wandb.ai/entelecheia/ekorpkit-book\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:281] 2022-11-22 12:16:58,103 >> You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 1:46:59, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.957800</td>\n",
       "      <td>3.366854</td>\n",
       "      <td>0.481761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-11-22 13:44:30,021 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2929] 2022-11-22 13:44:30,024 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2931] 2022-11-22 13:44:30,025 >>   Num examples = 13417\n",
      "[INFO|trainer.py:2934] 2022-11-22 13:44:30,025 >>   Batch size = 256\n",
      "[INFO|trainer.py:1859] 2022-11-22 14:04:34,523 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2678] 2022-11-22 14:04:34,526 >> Saving model checkpoint to /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased\n",
      "[INFO|configuration_utils.py:447] 2022-11-22 14:04:34,528 >> Configuration saved in /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-22 14:04:35,035 >> Model weights saved in /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-22 14:04:35,037 >> tokenizer config file saved in /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-22 14:04:35,037 >> Special tokens file saved in /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased/special_tokens_map.json\n",
      "INFO:ekorpkit.models.transformer.trainers.mlm:*** Evaluate ***\n",
      "[INFO|trainer.py:725] 2022-11-22 14:04:35,047 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2929] 2022-11-22 14:04:35,050 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2931] 2022-11-22 14:04:35,051 >>   Num examples = 13417\n",
      "[INFO|trainer.py:2934] 2022-11-22 14:04:35,051 >>   Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        9.99\n",
      "  total_flos               = 624867928GF\n",
      "  train_loss               =      4.5229\n",
      "  train_runtime            =  1:47:44.40\n",
      "  train_samples            =      255023\n",
      "  train_samples_per_second =     394.503\n",
      "  train_steps_per_second   =       0.192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       9.99\n",
      "  eval_accuracy           =     0.5945\n",
      "  eval_loss               =     2.3393\n",
      "  eval_runtime            = 0:00:26.68\n",
      "  eval_samples            =      13417\n",
      "  eval_samples_per_second =    502.858\n",
      "  eval_steps_per_second   =      1.986\n",
      "  perplexity              =    10.3743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:449] 2022-11-22 14:05:02,607 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5945472915782851}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2h 46s (started: 2022-11-22 12:04:15 +00:00)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:652] 2022-11-23 01:56:36,260 >> loading configuration file /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 01:56:36,261 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token_id\": 5,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token_id\": 3,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 4,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 6,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token_id\": 7,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2155] 2022-11-23 01:56:36,262 >> loading weights file /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2608] 2022-11-23 01:56:37,801 >> All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:2616] 2022-11-23 01:56:37,802 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-23 01:56:37,817 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-23 01:56:37,818 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-23 01:56:37,818 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-23 01:56:37,818 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.10175482928752899, 'token': 3225, 'token_str': '개발한', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 개발한 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.06834898144006729, 'token': 15022, 'token_str': '롤플레잉', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 롤플레잉 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.06569146364927292, 'token': 390, 'token_str': '게임', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 게임 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.03806138411164284, 'token': 3567, 'token_str': '제작한', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 제작한 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.03329821303486824, 'token': 2549, 'token_str': '사용되는', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 사용되는 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "time: 1.73 s (started: 2022-11-23 01:56:36 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "example = \"처음으로 대중적으로 <mask> 롤플레잉 게임이다.\"\n",
    "for prediction in trainer.fill_mask(example):\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.batch:Using existing path: /content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling\n",
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n",
      "INFO:ekorpkit.batch:Merging config with args: {}\n",
      "INFO:ekorpkit.batch:Batch name: enko_wiki_bert_base_uncased, Batch num: 2\n",
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_target_': 'ekorpkit.models.transformer.trainers.MlmTrainer',\n",
      " 'auto': {},\n",
      " 'autoload': False,\n",
      " 'batch': {'batch_name': 'enko_wiki_bert_base_uncased',\n",
      "           'batch_num': None,\n",
      "           'random_seed': True,\n",
      "           'resume_latest': False,\n",
      "           'resume_run': False,\n",
      "           'run_to_resume': 'latest',\n",
      "           'seed': None},\n",
      " 'data': {'dataset_config_name': None,\n",
      "          'dataset_name': None,\n",
      "          'line_by_line': False,\n",
      "          'max_eval_samples': None,\n",
      "          'max_seq_length': 512,\n",
      "          'max_train_samples': None,\n",
      "          'mlm_probability': 0.15,\n",
      "          'overwrite_cache': False,\n",
      "          'pad_to_max_length': False,\n",
      "          'preprocessing_num_workers': None,\n",
      "          'train_file': '/content/drive/MyDrive/workspace/projects/ekorpkit-book/data/tokenizers/enko_filtered_chunk',\n",
      "          'validation_file': None,\n",
      "          'validation_split_percentage': 5},\n",
      " 'device': 'cuda',\n",
      " 'model': {'cache_dir': None,\n",
      "           'config_name': 'bert-base-uncased',\n",
      "           'config_overrides': None,\n",
      "           'model_name_or_path': None,\n",
      "           'model_revision': 'main',\n",
      "           'model_type': None,\n",
      "           'tokenizer_name': None,\n",
      "           'use_auth_token': False,\n",
      "           'use_fast_tokenizer': True},\n",
      " 'name': 'enko_wiki_bert_base_uncased',\n",
      " 'path': {'batch_dir': '/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/enko_wiki_bert_base_uncased',\n",
      "          'batch_name': 'enko_wiki_bert_base_uncased',\n",
      "          'cache_dir': '/content/drive/MyDrive/workspace/.cache',\n",
      "          'library_dir': '/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/libs',\n",
      "          'model_dir': '/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/models',\n",
      "          'name': 'language-modeling',\n",
      "          'output_dir': '/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs',\n",
      "          'root': '/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling',\n",
      "          'task_name': 'language-modeling',\n",
      "          'tmp_dir': '/content/drive/MyDrive/workspace/.tmp',\n",
      "          'verbose': False},\n",
      " 'project': {'name': 'ekorpkit-book',\n",
      "             'path': {'archive': '/content/drive/MyDrive/workspace/data/archive',\n",
      "                      'cache': '/content/drive/MyDrive/workspace/.cache',\n",
      "                      'corpus': '/content/drive/MyDrive/workspace/data/datasets/corpus',\n",
      "                      'data': '/content/drive/MyDrive/workspace/data',\n",
      "                      'dataset': '/content/drive/MyDrive/workspace/data/datasets',\n",
      "                      'ekorpkit': '/workspace/projects/ekorpkit/ekorpkit',\n",
      "                      'home': '/root',\n",
      "                      'library': '/content/drive/MyDrive/workspace/data/libs',\n",
      "                      'log': '/content/drive/MyDrive/workspace/projects/ekorpkit-book/logs',\n",
      "                      'model': '/content/drive/MyDrive/workspace/data/models',\n",
      "                      'output': '/content/drive/MyDrive/workspace/projects/ekorpkit-book/outputs',\n",
      "                      'project': '/content/drive/MyDrive/workspace/projects/ekorpkit-book',\n",
      "                      'resource': '/workspace/projects/ekorpkit/ekorpkit/resources',\n",
      "                      'runtime': '/workspace/projects/ekorpkit-book/ekorpkit-book/docs/lectures/deep_nlp',\n",
      "                      'tmp': '/content/drive/MyDrive/workspace/.tmp',\n",
      "                      'workspace': '/content/drive/MyDrive/workspace'},\n",
      "             'project_name': 'ekorpkit-book',\n",
      "             'root': '/content/drive/MyDrive/workspace/projects/ekorpkit-book',\n",
      "             'task_name': 'language-modeling'},\n",
      " 'secret': {'hf_user_access_token': None, 'wandb_api_key': None},\n",
      " 'tokenizer': {'bos_token': '<s>',\n",
      "               'cls_token': '<cls>',\n",
      "               'eos_token': '</s>',\n",
      "               'mask_token': '<mask>',\n",
      "               'model_max_length': None,\n",
      "               'model_type': None,\n",
      "               'name': None,\n",
      "               'pad_token': '<pad>',\n",
      "               'padding_side': 'right',\n",
      "               'path': '/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/enko_wiki_unigram_huggingface_vocab_30000.json',\n",
      "               'return_length': True,\n",
      "               'sep_token': '<sep>',\n",
      "               'truncation': True,\n",
      "               'unk_token': '<unk>'},\n",
      " 'training': {'do_eval': True,\n",
      "              'do_train': True,\n",
      "              'eval_steps': 1000,\n",
      "              'evaluation_strategy': 'steps',\n",
      "              'fp16': True,\n",
      "              'gradient_accumulation_steps': 8,\n",
      "              'learning_rate': 0.0005,\n",
      "              'logging_steps': 1000,\n",
      "              'lr_scheduler_type': 'cosine',\n",
      "              'num_train_epochs': 10,\n",
      "              'output_dir': None,\n",
      "              'overwrite_output_dir': True,\n",
      "              'per_device_eval_batch_size': 32,\n",
      "              'per_device_train_batch_size': 32,\n",
      "              'push_to_hub': False,\n",
      "              'report_to': 'wandb',\n",
      "              'run_name': 'enko_wiki_bert_base_uncased',\n",
      "              'save_steps': 5000,\n",
      "              'warmup_steps': 1000,\n",
      "              'weight_decay': 0.1},\n",
      " 'use_accelerator': True,\n",
      " 'verbose': False}\n",
      "time: 262 ms (started: 2022-11-23 06:17:35 +00:00)\n"
     ]
    }
   ],
   "source": [
    "trainer.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training bnwiki_bert using MLM Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.utils.notebook:Google Colab not detected.\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_WORKSPACE_ROOT to /content/drive/MyDrive/workspace\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_PROJECT to ekorpkit-book\n",
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0.1.40.post0.dev29\n",
      "is colab? False\n",
      "project_dir: /content/drive/MyDrive/workspace/projects/ekorpkit-book\n",
      "time: 71.8 ms (started: 2022-11-24 11:24:31 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ekorpkit import eKonf\n",
    "\n",
    "eKonf.setLogger(\"INFO\")\n",
    "print(\"version:\", eKonf.__version__)\n",
    "\n",
    "is_colab = eKonf.is_colab()\n",
    "print(\"is colab?\", is_colab)\n",
    "if is_colab:\n",
    "    eKonf.mount_google_drive()\n",
    "workspace_dir = \"/content/drive/MyDrive/workspace\"\n",
    "project_name = \"ekorpkit-book\"\n",
    "project_dir = eKonf.set_workspace(workspace=workspace_dir, project=project_name)\n",
    "print(\"project_dir:\", project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n",
      "INFO:ekorpkit.base:No method defined to call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/tokenizers/bnwiki/bnwiki_unigram_huggingface_vocab_30000.json\n",
      "/content/drive/MyDrive/workspace/projects/ekorpkit-book/language-modeling/outputs/bnwiki/sentence_sample/sentence_chunks_sampled.txt\n",
      "time: 1.84 s (started: 2022-11-24 11:24:34 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.tokenizers.trainer import TokenizerTrainer\n",
    "\n",
    "\n",
    "cfg = eKonf.compose(\"tokenizer=trainer\")\n",
    "cfg.name = \"bnwiki\"\n",
    "cfg.training.use_sample = False\n",
    "cfg.model.model_type = \"unigram\"\n",
    "cfg.model.vocab_size = 30000\n",
    "\n",
    "tk_trainer = TokenizerTrainer(**cfg)\n",
    "print(tk_trainer.model_path)\n",
    "print(tk_trainer.sample_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.34 s (started: 2022-11-24 11:24:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.models.transformer.trainers import MlmTrainer\n",
    "\n",
    "# data_file = \"bnwiki_filtered.parquet\"\n",
    "\n",
    "cfg = eKonf.compose(\"model/transformer=mlm.trainer\")\n",
    "cfg.name = \"bnwiki\"\n",
    "cfg.model.config_name = \"bert-base-uncased\"\n",
    "cfg.tokenizer.path = str(tk_trainer.model_path)\n",
    "cfg.dataset.train_file = str(tk_trainer.sample_filepath)\n",
    "cfg.dataset.max_seq_length = 512\n",
    "cfg.dataset.num_workers = 8\n",
    "cfg.use_accelerator = True\n",
    "cfg.training.num_train_epochs = 50\n",
    "cfg.training.eval_steps =100\n",
    "cfg.training.warmup_steps = 50\n",
    "\n",
    "# trainer = MlmTrainer(**cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "ekorpkit \\\n",
    "    project.name=ekorpkit-book \\\n",
    "    dir.workspace=/content/drive/MyDrive/workspace \\\n",
    "    run=transformer \\\n",
    "    +model.transformer=mlm.trainer \\\n",
    "    model.transformer.name=bnwiki \\\n",
    "    model.transformer.model.config_name=bert-base-uncased \\\n",
    "    model.transformer.tokenizer.name=bnwiki_unigram_huggingface_vocab_30000.json \\\n",
    "    model.transformer.dataset.train_file=enko_filtered.parquet \\\n",
    "    model.transformer.dataset.max_seq_length=512 \\\n",
    "    model.transformer.dataset.num_workers=8 \\\n",
    "    model.transformer.use_accelerator=true \\\n",
    "    model.transformer.training.num_train_epochs=50 \\\n",
    "    model.transformer.training.eval_steps=100 \\\n",
    "    model.transformer.training.warmup_steps=50 \\\n",
    "    model.transformer.auto=train\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform predictions\n",
    "example = \"এই মসজিদটির ভিত্তিপ্রস্তর হয়েছিলো <mask> ২০১৫ সালের জুন মাসে।\"\n",
    "for prediction in trainer.fill_mask(example):\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- [Unicode equivalence](https://en.wikipedia.org/wiki/Unicode_equivalence)\n",
    "- [Training a causal language model from scratch](https://huggingface.co/course/chapter7/6?fw=pt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
