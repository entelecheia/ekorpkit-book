{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "![](../figs/intro_nlp/tokenization/entelecheia_puzzle_pieces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "NLP systems have three main components that help machines understand natural language:\n",
    "\n",
    "- **Tokenization**: Splitting a string into a list of tokens.\n",
    "- **Embedding**: Mapping tokens to vectors.\n",
    "- **Model**: A neural network that takes token vectors as input and outputs predictions.\n",
    "\n",
    "Tokenization is the first step in the NLP pipeline. \n",
    "\n",
    "- Tokenization is the process of splitting a string into a list of tokens. \n",
    "- For example, the sentence \"I like to eat apples\" can be tokenized into the list of tokens `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`. \n",
    "- The tokens can be words, characters, or subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39810160",
   "metadata": {},
   "source": [
    "## What is Tokenization?\n",
    "\n",
    "- Tokenization is the process of representing a text in smaller units called tokens.\n",
    "- In a very simple case, we can simply map every word in the text to a numerical index.\n",
    "- For example, the sentence \"I like to eat apples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`. \n",
    "- Then, each token can be mapped to a unique index, such as:\n",
    "  > `{\"I\": 0, \"like\": 1, \"to\": 2, \"eat\": 3, \"apples\": 4}`.\n",
    "- There are more linguistic features to consider when tokenizing a text, such as punctuation, capitalization, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cc5c5",
   "metadata": {},
   "source": [
    "## Why do we need tokenization?\n",
    "\n",
    "- \"How can we make a machine read a sentence?\"\n",
    "- Machines don’t know any language, nor do they understand sounds or phonetics.\n",
    "- They need to be taught from scratch.\n",
    "- The first step is to break down the sentence into smaller units that the machine can process.\n",
    "- Tokenization determines how the input is represented to the model.\n",
    "- This decision has a huge impact on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29cf2e",
   "metadata": {},
   "source": [
    "## How do we identify words in text?\n",
    "\n",
    "For a language like English, this seems like a simple task. We can simply split the text by spaces. \n",
    "\n",
    "```\n",
    "A word is any sequence of alphabetical characters between whitespaces that’s not a punctuation mark?\n",
    "```\n",
    "\n",
    "\n",
    "However, there are many cases where this is not true.\n",
    "\n",
    "- What about contractions? \n",
    "  - \"I'm\" is a single word, but it is split into two tokens.\n",
    "- What about abbreviations? \n",
    "  - \"U.S.\" is a single word, but it is split into two tokens.\n",
    "- What about hyphenated words? \n",
    "  - \"self-driving\", \"R2-D2\" are single words, but they are split into two tokens.\n",
    "- What about complex names? \n",
    "  - \"New York\" is a single word, but it is split into two tokens.\n",
    "- What about languages like Chinese that have no spaces between words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fcaf3e",
   "metadata": {},
   "source": [
    "### Words aren’t just defined by blanks\n",
    "\n",
    "Problem 1: Compounding\n",
    "\n",
    "```\n",
    "“ice cream”, “website”, “web site”, “New York-based”\n",
    "```\n",
    "\n",
    "Problem 2: Other writing systems have no blanks\n",
    "\n",
    "```\n",
    "Chinese: 我开始写⼩说 = 我 开始 写 ⼩说 (I start(ed) writing novel(s))\n",
    "```\n",
    "\n",
    "Problem 3: Contractions and Clitics\n",
    "\n",
    "```\n",
    "English: “doesn’t” , “I’m” ,\n",
    "Italian: “dirglielo” = dir + gli(e) + lo (tell + him + it)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421276ae",
   "metadata": {},
   "source": [
    "### Tokenization Standards\n",
    "\n",
    "Any actual NLP system will assume a particular tokenization standard.\n",
    "\n",
    "- NLP systems are usually trained on particular corpora (text datasets) that everybody uses.\n",
    "- These corpora often define a de facto standard.\n",
    "\n",
    "Penn Treebank 3 standard:\n",
    "\n",
    "- Input:\n",
    "  > `\"The San Francisco-based restaurant,\" they said, \"doesn’t charge $10\".`\n",
    "- Output:\n",
    "  > `“_ The _ San _ Francisco-based _ restaurant _ , _” _ they_ said* ,* \"_ does _ n’t _ charge_ $_ 10 _ \" _ . _`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d9c79",
   "metadata": {},
   "source": [
    "### What about sentence boundaries?\n",
    "\n",
    "How can we identify that this is two sentences?\n",
    "\n",
    "```\n",
    "Mr. Smith went to D.C. Ms. Xu went to Chicago instead.\n",
    "```\n",
    "\n",
    "- We can use a period to identify the end of a sentence.\n",
    "- However, this is not always true.\n",
    "- Abbreviations, such as \"Mr.\", \"D.C.\", \"Ms.\", \"U.S.\", \"etc.\" can be followed by a period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1463193",
   "metadata": {},
   "source": [
    "How many sentences are in this text?\n",
    "\n",
    "```\n",
    "\"The San Francisco-based restaurant,\" they said, \"doesn’t charge $10\".\n",
    "```\n",
    "\n",
    "Answer: just one, because the comma is not a sentence boundary.\n",
    "\n",
    "Similarly, we typically treat this also just as one sentence:\n",
    "\n",
    "```\n",
    "They said: ”The San Francisco-based restaurant doesn’t charge $10\".\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565cdd6",
   "metadata": {},
   "source": [
    "### Spelling variants, typos, etc.\n",
    "\n",
    "The same word can be written in different ways:\n",
    "\n",
    "- with different `capitalizations`:\n",
    "  - lowercase “cat” (in standard running text)\n",
    "  - capitalized “Cat” (as first word in a sentence, or in titles/headlines),\n",
    "  - all-caps “CAT” (e.g. in headlines)\n",
    "- with different abbreviation or hyphenation styles:\n",
    "  - US-based, US based, U.S.-based, U.S. based\n",
    "  - US-EU relations, U.S./E.U. relations, …\n",
    "- with spelling variants (e.g. regional variants of English):\n",
    "  - labor vs labour, materialize vs materialise,\n",
    "- with typos (teh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d58bbc0",
   "metadata": {},
   "source": [
    "## How many different words are there in English?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5f64f",
   "metadata": {},
   "source": [
    "### Counting words: tokens vs. types\n",
    "\n",
    "When counting words in text, we distinguish between word types and word tokens:\n",
    "\n",
    "- The vocabulary of a language is the set of (unique) word types:\n",
    "  > V = {a, aardvark, …., zyzzva}\n",
    "- The tokens in a document include all occurrences of the word types in that document or corpus\n",
    "- The frequency of a word (type) in a document  \n",
    "  = the number of occurrences (tokens) of that type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705c650",
   "metadata": {},
   "source": [
    "How large is the vocabulary of English (or any other language)?\n",
    "\n",
    "- Vocabulary size = the number of distinct word types\n",
    "  > Google N-gram corpus: 1 trillion tokens, 13 million word types that appear 40+ times\n",
    "\n",
    "If you count words in text, you will find that ...\n",
    "\n",
    "- a few words (mostly closed-class) are very frequent (the, be, to, of, and, a, in, that,…)\n",
    "- most words (all open class) are very rare.\n",
    "- even if you’ve read a lot of text, you will keep finding words you haven’t seen before. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e7cc9",
   "metadata": {},
   "source": [
    "### Zipf’s law: the long tail\n",
    "\n",
    "In a natural language:\n",
    "\n",
    "- A small number of events (e.g. words) occur with high frequency\n",
    "- A large number of events occur with very low frequency\n",
    "\n",
    "![](../figs/intro_nlp/tokenization/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7563b2a",
   "metadata": {},
   "source": [
    "#### Implications of Zipf’s Law for NLP\n",
    "\n",
    "The good:\n",
    "\n",
    "```\n",
    "Any text will contain a number of words that are very common.\n",
    "We have seen these words often enough that we know (almost) everything about them.\n",
    "These words will help us get at the structure (and possibly meaning) of this text.\n",
    "```\n",
    "\n",
    "The bad:\n",
    "\n",
    "```\n",
    "Any text will contain a number of words that are rare.\n",
    "We know something about these words, but haven’t seen them often enough to know everything about them.\n",
    "They may occur with a meaning or a part of speech we haven’t seen before.\n",
    "```\n",
    "\n",
    "The ugly:\n",
    "\n",
    "```\n",
    "Any text will contain a number of words that are unknown to us.\n",
    "We have never seen them before, but we still need to get at the structure (and meaning) of these texts.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f40ad7",
   "metadata": {},
   "source": [
    "#### Dealing with the bad and the ugly\n",
    "\n",
    "NLP systems need to be able to generalize from the known to the unknown.\n",
    "\n",
    "There are two main strategies:\n",
    "\n",
    "- Linguistic knowledge\n",
    "  - a finite set of grammatical rules is enough to generate an infinite number of languages\n",
    "- Machine learning or statistical methods\n",
    "  - learn representations of words from large amounts of data that often work well for unseen words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14416e2c",
   "metadata": {},
   "source": [
    "## How do we represent words?\n",
    "\n",
    "Option 1: Words are atomic symbols\n",
    "\n",
    "- Each (surface) word is a unique symbol\n",
    "- Add some generalization rules to map different surface forms to the same symbol\n",
    "  - `Normalization`: map all variants of the same word (form) to the same canonical variant \n",
    "    > e.g. lowercase everything, normalize spellings, perhaps spell-check)\n",
    "  - `Lemmatization`: map each word to its lemma (esp. in English, the lemma is still a word in the language, but lemmatized text is no longer grammatical)\n",
    "  - `Stemming`: remove endings that differ among word forms (no guarantee that the resulting symbol is an actual word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c0ac7",
   "metadata": {},
   "source": [
    "Option 2: Represent the structure of each word\n",
    "\n",
    "```\n",
    "\"books\" => \"book N pl\" (or \"book V 3rd sg\")\n",
    "```\n",
    "\n",
    "- This requries a morphological analyzer\n",
    "- The output is often a lemma (e.g. \"book\") and morphological features (e.g. \"N pl\" for noun plural, \"V 3rd sg\" for verb 3rd person singular)\n",
    "- This is particularly useful for languages with rich morphology (e.g. Turkish, Finnish, Hungarian, etc.)\n",
    "- Less useful for languages with little morphology (e.g. English, German, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8de82",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
