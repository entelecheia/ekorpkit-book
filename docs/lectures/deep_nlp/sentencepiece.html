
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>SentencePiece Tokenizer &#8212; eKorpkit Tutorials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/ekorpkit.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="AI Art (Text-to-Image)" href="../aiart/index.html" />
    <link rel="prev" title="Tokenization" href="tokenization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FN4GFT8HP8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FN4GFT8HP8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/ekorpkit.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">eKorpkit Tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    eKonomic Research Python Toolkit
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/index.html">
   Getting started
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/install.html">
     Installation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/usage.html">
     Usage
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/features/index.html">
   Key features
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/easy_config.html">
     Easy Configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/no_boiler.html">
     No Boilerplate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/workflows.html">
     Workflows
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/share.html">
     Sharable and Reproducible
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/plug.html">
     Pluggable Architece
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/config/index.html">
   Configuring ekorpkit
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/config/eKonf.html">
     Using eKonf class
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/datasets/index.html">
   Datasets
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/corpus.html">
     Build and Load Corpora
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/datasets/dataset.html">
     Build and Load Datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/pipeline.html">
     Corpus task pipelines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/models/index.html">
   Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/automl/index.html">
     Auto ML
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/embeddings/index.html">
     Word Embeddings
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/word2vec_basics.html">
       Word2Vec Basics
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/eval_vectors.html">
       Evaluate pretrained embeddings
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/ngram/index.html">
     N-Grams
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram.html">
       N-Grams
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_lexicons.html">
       N-Gram model for ngram lexicon features
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_words.html">
       N-Gram model for unigram lexicon features
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/sentiment/index.html">
     Sentiment Analyers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/lbsa_en.html">
       Lexicon-based Sentiment Analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/financial_phrasebank_lm.html">
       Evaluate LM with financial phrasebank
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/sentiment.html">
       LM Dictionary vs. finbert vs. T5
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/transformers/index.html">
     Transformers
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/art/index.html">
     AI Art (Text-to-Image)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/disco.html">
       Disco Diffusion
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/dalle-mini.html">
       DALL·E Mini
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/pipelines/index.html">
   Pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/pipelines/pipeline.html">
     Instantiating pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/preprocessors/index.html">
   Preprocessors
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/normalizer/index.html">
     Normalizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/normalizer/normalizer.html">
       Normalizers
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/segmenter/index.html">
     Segmenters
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/segmenter/segmenter.html">
       Segmenters
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/index.html">
     Tokenizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
    <label for="toctree-checkbox-14">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/mecab.html">
       Mecab Tokenizer
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/visualizers/index.html">
   Visualizers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/visualizers/plot.html">
     Plots
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/workflow/index.html">
   Workflows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../corpus/ekorpkot_corpus.html">
   The eKorpkit Corpus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Use Cases
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/fomc/index.html">
   FOMC
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/02_FOMC_numerical_data.html">
     Preparing Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/03_FOMC_corpus.html">
     Preparing Textual Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/04_FOMC_EDA_numericals.html">
     EDA on Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/05_FOMC_features.html">
     Visualizing Features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/06_FOMC_AutoML.html">
     Checking Baseline with AutoML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/07_FOMC_sentiments.html">
     Predicting Sentiments of FOMC Corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/08_FOMC_EDA_sentiments.html">
     EDA on Sentiment Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/09_FOMC_AutoML_with_tones.html">
     Predicting the next decisions with tones
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../usecases/bok/index.html">
   Bank of Korea
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/edgar/index.html">
   EDGAR
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/edgar/predict_edgar.html">
     Prediciting Sentiments
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg/index.html">
   ESG
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/snorkel-polarity.html">
     Preparing polarity classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix.html">
     Improving classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/classifiers.html">
     Training Classifiers for ESG Ratings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/econ_news_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       econ_news_kr
      </span>
     </code>
     corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_news.html">
     Predicting ESG Categories and Polarities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix_classifiers.html">
     Preparing classifiers for active learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_for_learning.html">
     Preparing active learning data
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg-en/index.html">
   ESG (English)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg-en/esg_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       JOCo
      </span>
     </code>
     corpus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/cointax/index.html">
   Taxation on Cryptocurrency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/cointax/coin_dataset.html">
     Improving classification datasets
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/analyst.html">
   ESG Topic Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/rethink_esg.html">
   Rethinking ESG
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../intro_nlp/index.html">
   Introduction to NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/research.html">
     Research Applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/topic.html">
     Topic Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/topic_models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/topic_coherence.html">
     Topic Coherence Measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/sentiments.html">
     Sentiment Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/tokenization.html">
     Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/word_segmentation.html">
     Word Segmentation and Association
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/vectorization.html">
     Vector Semantics and Representation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/language_models.html">
     Language Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/lm_evaluation.html">
     Language Model Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Deep Learning for NLP
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="zeroshot.html">
     Zero Shot, Prompt, and Search Strategies
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloom.html">
     What is BLOOM?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloom_apps.html">
     Bloom Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="transformers.html">
     Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert.html">
     BERT: Bidirectional Encoder Representations from Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="t5.html">
     T5: Text-To-Text Transfer Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tokenization.html">
     Tokenization
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     SentencePiece Tokenizer
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../aiart/index.html">
   AI Art (Text-to-Image)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
  <label for="toctree-checkbox-23">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/project-themes.html">
     Project Themes - A Brave New World
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle1.html">
     DALL·E 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle2.html">
     DALL·E 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/imagen.html">
     Imagen
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mlops/index.html">
   Machine Learning Systems Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ds/index.html">
   Data Science for Economics and Finance
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/index.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/cite.html">
   Citation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/entelecheia/ekorpkit-book/main?urlpath=tree/ekorpkit-book/docs/lectures/deep_nlp/sentencepiece.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/entelecheia/ekorpkit-book/blob/main/ekorpkit-book/docs/lectures/deep_nlp/sentencepiece.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book/issues/new?title=Issue%20on%20page%20%2Fdocs/lectures/deep_nlp/sentencepiece.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/docs/lectures/deep_nlp/sentencepiece.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-sentencepiece">
   What is SentencePiece?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#technical-highlights">
   Technical highlights
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparisons-with-other-implementations">
     Comparisons with other implementations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#high-level-differences-between-sentencepiece-and-other-tokenizers">
   High level differences between SentencePiece and other tokenizers
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-number-of-unique-tokens-is-predetermined">
     The number of unique tokens is predetermined
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trains-from-raw-sentences">
     Trains from raw sentences
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#whitespace-is-treated-as-a-basic-symbol">
     Whitespace is treated as a basic symbol
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#subword-regularization-and-bpe-dropout">
     Subword regularization and BPE-dropout
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subword-regularization">
   Subword Regularization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-train-sentencepiece">
   How to train SentencePiece
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-the-optimal-segmentation">
     Finding the optimal segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-the-model">
     Fitting the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#subword-sampling">
     Subword sampling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>SentencePiece Tokenizer</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-sentencepiece">
   What is SentencePiece?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#technical-highlights">
   Technical highlights
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparisons-with-other-implementations">
     Comparisons with other implementations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#high-level-differences-between-sentencepiece-and-other-tokenizers">
   High level differences between SentencePiece and other tokenizers
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-number-of-unique-tokens-is-predetermined">
     The number of unique tokens is predetermined
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trains-from-raw-sentences">
     Trains from raw sentences
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#whitespace-is-treated-as-a-basic-symbol">
     Whitespace is treated as a basic symbol
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#subword-regularization-and-bpe-dropout">
     Subword regularization and BPE-dropout
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subword-regularization">
   Subword Regularization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-train-sentencepiece">
   How to train SentencePiece
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-the-optimal-segmentation">
     Finding the optimal segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-the-model">
     Fitting the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#subword-sampling">
     Subword sampling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="sentencepiece-tokenizer">
<h1>SentencePiece Tokenizer<a class="headerlink" href="#sentencepiece-tokenizer" title="Permalink to this headline">#</a></h1>
<p><img alt="" src="../../../_images/entelecheia_alphabets.png" /></p>
<section id="what-is-sentencepiece">
<h2>What is SentencePiece?<a class="headerlink" href="#what-is-sentencepiece" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training.</p></li>
<li><p>SentencePiece implements <strong>subword units</strong> (e.g., <strong>byte-pair-encoding (BPE)</strong> [<a class="reference external" href="https://www.aclweb.org/anthology/P16-1162">Sennrich et al.</a>]) and <strong>unigram language model</strong> [<a class="reference external" href="https://arxiv.org/abs/1804.10959">Kudo.</a>])
with the extension of direct training from raw sentences.</p></li>
<li><p>SentencePiece is a pure end-to-end system that does not depend on language-specific pre/postprocessing.</p></li>
<li><p>SentencePiece is a general-purpose tokenizer that can be used for any language.</p></li>
</ul>
<p>To be surprised, SentencePiece is not a tokenizer itself, but a tool to train a tokenizer.</p>
<ul class="simple">
<li><p>It is a method to select the best subword units from the corpus optimizing the tokenization process.</p></li>
<li><p>It implements the Subword Regularization algorithm.</p></li>
</ul>
</section>
<section id="technical-highlights">
<h2>Technical highlights<a class="headerlink" href="#technical-highlights" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>Purely data driven</strong>: SentencePiece trains tokenization and detokenization
models from sentences. Pre-tokenization (<a class="reference external" href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl">Moses tokenizer</a>/<a class="reference external" href="http://taku910.github.io/mecab/">MeCab</a>/<a class="reference external" href="http://www.phontron.com/kytea/">KyTea</a>) is not always required.</p></li>
<li><p><strong>Language independent</strong>: SentencePiece treats the sentences just as sequences of Unicode characters. There is no language-dependent logic.</p></li>
<li><p><strong>Multiple subword algorithms</strong>: <strong>BPE</strong>  [<a class="reference external" href="https://www.aclweb.org/anthology/P16-1162">Sennrich et al.</a>] and <strong>unigram language model</strong> [<a class="reference external" href="https://arxiv.org/abs/1804.10959">Kudo.</a>] are supported.</p></li>
<li><p><strong>Subword regularization</strong>: SentencePiece implements subword sampling for <a class="reference external" href="https://arxiv.org/abs/1804.10959">subword regularization</a> and <a class="reference external" href="https://arxiv.org/abs/1910.13267">BPE-dropout</a> which help to improve the robustness and accuracy of NMT models.</p></li>
<li><p><strong>Fast and lightweight</strong>: Segmentation speed is around 50k sentences/sec, and memory footprint is around 6MB.</p></li>
<li><p><strong>Self-contained</strong>: The same tokenization/detokenization is obtained as long as the same model file is used.</p></li>
<li><p><strong>Direct vocabulary id generation</strong>: SentencePiece manages vocabulary to id mapping and can directly generate vocabulary id sequences from raw sentences.</p></li>
<li><p><strong>NFKC-based normalization</strong>: SentencePiece performs NFKC-based text normalization.</p></li>
</ul>
<section id="comparisons-with-other-implementations">
<h3>Comparisons with other implementations<a class="headerlink" href="#comparisons-with-other-implementations" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Feature</p></th>
<th class="text-align:center head"><p>SentencePiece</p></th>
<th class="text-align:center head"><p>subword-nmt</p></th>
<th class="text-align:center head"><p>WordPiece</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Supported algorithm</p></td>
<td class="text-align:center"><p>BPE, unigram, char, word</p></td>
<td class="text-align:center"><p>BPE</p></td>
<td class="text-align:center"><p>BPE*</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>OSS?</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Google internal</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Subword regularization</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>No</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Python Library (pip)</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>N/A</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>C++ Library</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>N/A</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Pre-segmentation required?</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Customizable normalization (e.g., NFKC)[Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>N/A</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Direct id generation</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>N/A</p></td>
</tr>
</tbody>
</table>
<p>Note that BPE algorithm used in WordPiece is slightly different from the original BPE.</p>
</section>
</section>
<section id="high-level-differences-between-sentencepiece-and-other-tokenizers">
<h2>High level differences between SentencePiece and other tokenizers<a class="headerlink" href="#high-level-differences-between-sentencepiece-and-other-tokenizers" title="Permalink to this headline">#</a></h2>
<section id="the-number-of-unique-tokens-is-predetermined">
<h3>The number of unique tokens is predetermined<a class="headerlink" href="#the-number-of-unique-tokens-is-predetermined" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Neural Machine Translation models typically operate with a fixed vocabulary.</p></li>
<li><p>Unlike most unsupervised word segmentation algorithms, which assume an infinite vocabulary, SentencePiece trains the segmentation model such that the final vocabulary size is fixed, e.g., 8k, 16k, or 32k.</p></li>
</ul>
<p>Note that SentencePiece specifies the final vocabulary size for training, which is different from
<a class="reference external" href="https://github.com/rsennrich/subword-nmt">subword-nmt</a> that uses the number of merge operations.
The number of merge operations is a BPE-specific parameter and not applicable to other segmentation algorithms, including unigram, word and character.</p>
</section>
<section id="trains-from-raw-sentences">
<h3>Trains from raw sentences<a class="headerlink" href="#trains-from-raw-sentences" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Previous sub-word implementations assume that the input sentences are pre-tokenized.</p></li>
<li><p>This constraint was required for efficient training, but makes the preprocessing complicated as we have to run language dependent tokenizers in advance.</p></li>
<li><p>The implementation of SentencePiece is fast enough to train the model from raw sentences.</p></li>
<li><p>This is useful for training the tokenizer and detokenizer for Chinese and Japanese where no explicit spaces exist between words.</p></li>
</ul>
</section>
<section id="whitespace-is-treated-as-a-basic-symbol">
<h3>Whitespace is treated as a basic symbol<a class="headerlink" href="#whitespace-is-treated-as-a-basic-symbol" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>The first step of Natural Language processing is text tokenization.</p></li>
<li><p>For example, a standard English tokenizer would segment the text “Hello world.” into the following three tokens.</p>
<blockquote>
<div><p>[Hello] [World] [.]</p>
</div></blockquote>
</li>
<li><p>One observation is that the original input and tokenized sequence are <strong>NOT reversibly convertible</strong>.</p></li>
<li><p>For instance, the information that is no space between “World” and “.” is dropped from the tokenized sequence, since e.g., <code class="docutils literal notranslate"><span class="pre">Tokenize(“World.”)</span> <span class="pre">==</span> <span class="pre">Tokenize(“World</span> <span class="pre">.”)</span></code></p></li>
<li><p>SentencePiece treats the input text just as a sequence of Unicode characters. Whitespace is also handled as a normal symbol.</p></li>
</ul>
<ul>
<li><p>To handle the whitespace as a basic token explicitly, SentencePiece first escapes the whitespace with a meta symbol “▁” (U+2581) as follows.</p>
<blockquote>
<div><p>Hello▁World.</p>
</div></blockquote>
</li>
<li><p>Then, this text is segmented into small pieces, for example:</p>
<blockquote>
<div><p>[Hello] [▁Wor] [ld] [.]</p>
</div></blockquote>
</li>
<li><p>Since the whitespace is preserved in the segmented text, we can detokenize the text without any ambiguities.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">detokenized</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pieces</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;▁&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This feature makes it possible to perform detokenization without relying on language-specific resources.</p></li>
</ul>
<p>Note that we cannot apply the same lossless conversions when splitting the sentence with standard word segmenters, since they treat the whitespace as a special symbol. Tokenized sequences do not preserve the necessary information to restore the original sentence.</p>
<ul class="simple">
<li><p>(en) Hello world.   → [Hello] [World] [.]   (A space between Hello and World)</p></li>
<li><p>(ja) こんにちは世界。  → [こんにちは] [世界] [。] (No space between こんにちは and 世界)</p></li>
</ul>
</section>
<section id="subword-regularization-and-bpe-dropout">
<h3>Subword regularization and BPE-dropout<a class="headerlink" href="#subword-regularization-and-bpe-dropout" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Subword regularization [<a class="reference external" href="https://arxiv.org/abs/1804.10959">Kudo.</a>] and BPE-dropout <a class="reference external" href="https://arxiv.org/abs/1910.13267">Provilkov et al</a> are simple regularization methods that virtually augment training data with on-the-fly subword sampling, which helps to improve the accuracy as well as robustness of NMT models.</p></li>
<li><p>To enable subword regularization, you would like to integrate SentencePiece library (C++/Python) into the NMT system to sample one segmentation for each parameter update, which is different from the standard off-line data preparations.</p></li>
<li><p>You can find that ‘New York’ is segmented differently on each <code class="docutils literal notranslate"><span class="pre">SampleEncode</span> <span class="pre">(C++)</span></code> or <code class="docutils literal notranslate"><span class="pre">encode</span> <span class="pre">with</span> <span class="pre">enable_sampling=True</span> <span class="pre">(Python)</span></code> calls.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">pip</span> install sentencepiece
<span class="o">%</span><span class="k">pip</span> install --pre ekorpkit
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sentencepiece</span> <span class="k">as</span> <span class="nn">spm</span>

<span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceTrainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="s2">&quot;--input=../data/sentencepiece/botchan.txt --model_prefix=m --vocab_size=2000&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=../data/sentencepiece/botchan.txt --model_prefix=m --vocab_size=2000
sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : 
trainer_spec {
  input: ../data/sentencepiece/botchan.txt
  input_format: 
  model_prefix: m
  model_type: UNIGRAM
  vocab_size: 2000
  self_test_sample_size: 0
  character_coverage: 0.9995
  input_sentence_size: 0
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 0
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  hard_vocab_limit: 1
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: -1
  unk_piece: &lt;unk&gt;
  bos_piece: &lt;s&gt;
  eos_piece: &lt;/s&gt;
  pad_piece: &lt;pad&gt;
  unk_surface:  ⁇ 
  enable_differential_privacy: 0
  differential_privacy_noise_level: 0
  differential_privacy_clipping_threshold: 0
}
normalizer_spec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(181) LOG(INFO) Loading corpus: ../data/sentencepiece/botchan.txt
trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences
trainer_interface.cc(422) LOG(INFO) Adding meta_piece: &lt;unk&gt;
trainer_interface.cc(422) LOG(INFO) Adding meta_piece: &lt;s&gt;
trainer_interface.cc(422) LOG(INFO) Adding meta_piece: &lt;/s&gt;
trainer_interface.cc(427) LOG(INFO) Normalizing sentences...
trainer_interface.cc(536) LOG(INFO) all chars count=274252
trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.
trainer_interface.cc(557) LOG(INFO) Alphabet size=69
trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957
trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.
unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...
unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces
trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288
trainer_interface.cc(607) LOG(INFO) Done! 9165
unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71868 num_tokens=20446 num_tokens/piece=5.21183
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3922 obj=8.66277 num_tokens=20447 num_tokens/piece=5.21341
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2941 obj=8.95617 num_tokens=22741 num_tokens/piece=7.7324
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2941 obj=8.88103 num_tokens=22745 num_tokens/piece=7.73376
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2205 obj=9.26224 num_tokens=25461 num_tokens/piece=11.5469
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2205 obj=9.17719 num_tokens=25457 num_tokens/piece=11.5451
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2200 obj=9.17892 num_tokens=25475 num_tokens/piece=11.5795
unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2200 obj=9.17823 num_tokens=25475 num_tokens/piece=11.5795
trainer_interface.cc(685) LOG(INFO) Saving model: m.model
trainer_interface.cc(697) LOG(INFO) Saving vocabs: m.vocab
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">()</span>
<span class="n">s</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;m.model&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="n">s</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
            <span class="s2">&quot;New York&quot;</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_sampling</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">nbest_size</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;▁New&#39;, &#39;▁Y&#39;, &#39;or&#39;, &#39;k&#39;]
[&#39;▁N&#39;, &#39;e&#39;, &#39;w&#39;, &#39;▁Y&#39;, &#39;o&#39;, &#39;r&#39;, &#39;k&#39;]
[&#39;▁New&#39;, &#39;▁Y&#39;, &#39;o&#39;, &#39;r&#39;, &#39;k&#39;]
[&#39;▁N&#39;, &#39;e&#39;, &#39;w&#39;, &#39;▁Y&#39;, &#39;o&#39;, &#39;r&#39;, &#39;k&#39;]
[&#39;▁&#39;, &#39;N&#39;, &#39;e&#39;, &#39;w&#39;, &#39;▁Y&#39;, &#39;or&#39;, &#39;k&#39;]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="subword-regularization">
<h2>Subword Regularization<a class="headerlink" href="#subword-regularization" title="Permalink to this headline">#</a></h2>
<ul>
<li><p>Given a sequence of unigrams <span class="math notranslate nohighlight">\(X = (x_1, x_2, \cdots, x_n)\)</span>, the probability of the sequence <span class="math notranslate nohighlight">\(X\)</span> is given by the product of the unigram conditional probabilities by the Bayes chain rule:</p>
<div class="math notranslate nohighlight">
\[
  P(X) = p(x_1) p(x_2 | x_1) \cdots p(x_n | x_1, \cdots, x_{n-1}) = \prod_{i=1}^n p(x_i | x_1, \cdots, x_{i-1})
  \]</div>
</li>
<li><p>In the problem of Neural Machine Translation, the probability of <span class="math notranslate nohighlight">\(P(Y|X)\)</span> is given by the product of the conditional probabilities of the target sequence <span class="math notranslate nohighlight">\(Y\)</span> given the source sequence <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[
  P(Y|X;\theta) = \prod_{i=1}^n P(y_i | \mathbf{x}, y_{&lt;i}; \theta)
  \]</div>
<p>where the lower case variables represent the actual tokens, and the upper case variables represent the sequence of tokens. <span class="math notranslate nohighlight">\(\theta\)</span> is the model parameter.</p>
</li>
</ul>
<ul>
<li><p>This formula is not actually correct, since <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> can be formed by an exponentailly large number of possible subword sequences.</p></li>
<li><p>For example, the word <code class="docutils literal notranslate"><span class="pre">hello</span></code> can be segmented in a number of ways, e.g., <code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">e</span> <span class="pre">l</span> <span class="pre">l</span> <span class="pre">o</span></code>, <code class="docutils literal notranslate"><span class="pre">he</span> <span class="pre">ll</span> <span class="pre">o</span></code>, <code class="docutils literal notranslate"><span class="pre">hel</span> <span class="pre">lo</span></code>, <code class="docutils literal notranslate"><span class="pre">hell</span> <span class="pre">o</span></code>, <code class="docutils literal notranslate"><span class="pre">hello</span></code>.</p></li>
<li><p>Therefore, we should replace <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> on the left with a specific sequence of subwords, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, respectively.</p></li>
<li><p>The cost function for NMT is given by sum of the expected log-likelihood of the target sequence <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> given the source sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \mathcal{L}(\theta) = - \sum_{s=1}^{|D|} \mathbb{E}_{\substack{\mathbf{x} \sim P(\mathbf{x}|X^{(s)}) \\ \mathbf{y} \sim P(\mathbf{y}|Y^{(s)})}} \log P(\mathbf{y}|\mathbf{x};\theta)
  \end{split}\]</div>
</li>
<li><p>This formula looks intimidating, but it is actually quite simple.</p></li>
<li><p>In practice, we can approximate the expected log-likelihood of a single training example <span class="math notranslate nohighlight">\((\mathbf{x}, \mathbf{y})\)</span>.</p></li>
</ul>
</section>
<section id="how-to-train-sentencepiece">
<h2>How to train SentencePiece<a class="headerlink" href="#how-to-train-sentencepiece" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Assume that we have a large collection of bigrams, greater than what we ultimately want to use in our model.</p></li>
<li><p>To train a SentencePiece model, we want to maximize the probability of obtaining a particular tokenization <span class="math notranslate nohighlight">\(X = (x_1, x_2, \cdots, x_n)\)</span> of the corpus, given the unigram probabilities <span class="math notranslate nohighlight">\(p(x_i), p(x_2), \cdots, p(x_n)\)</span>.</p></li>
<li><p>The actual tokenization <span class="math notranslate nohighlight">\(X\)</span> is not observed, we only observe the un-tokenized sequence <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>This is a classic problem of maximum likelihood estimation, and we can solve it by the EM algorithm.</p></li>
<li><p>The problem is that thte <span class="math notranslate nohighlight">\(x_i\)</span> are all of different lengths, and we cannot apply the EM algorithm directly.</p></li>
<li><p>Instead, we should use a Bayesian approach to solve this problem.</p></li>
</ul>
<p>The SentencePiece training objective is given by the following equation:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta) = - \sum_{s=1}^{|D|} \log P(X^{(s)}) = - \sum_{s=1}^{|D|} \log (\sum_{\mathbf{x} \in S(\mathbf{x})} P(\mathbf{x}) )
\]</div>
<p>where the <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a unigram sequence, and <span class="math notranslate nohighlight">\(S(\mathbf{x})\)</span> is the set of all possible sequences that can be generated from <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>The steps are as follows:</p>
<ol>
<li><p>Initialize the unigram probabilities <span class="math notranslate nohighlight">\(p(x_i)\)</span>. The frequency of each unigram is used as the initial value.</p></li>
<li><p>M-step: Compute the most likely sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> given the current unigram probabilities <span class="math notranslate nohighlight">\(p(x_i)\)</span>.</p></li>
<li><p>E-step: Given the current most likely sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, update the unigram probabilities <span class="math notranslate nohighlight">\(p(x_i)\)</span>. In Bayesian setting, the unigram probabilities are defined as:</p>
<div class="math notranslate nohighlight">
\[
   p(x_i | \mathbf{x}) = \frac{c_i}{\sum_{j=1}^{|V|} c_j} \implies \frac{e^{\psi(c_i)}}{\sum_{j=1}^{|V|} e^{\psi(c_j)}} = \frac{e^{\psi(c_i)}}{e^{\psi(\sum_{j=1}^{|V|} c_j)}} = \frac{e^{\psi(c_i)}}{e^{\psi(c_1) + \cdots + \psi(c_n)}}
   \]</div>
<p>where <span class="math notranslate nohighlight">\(c_i\)</span> is the frequency of the unigram <span class="math notranslate nohighlight">\(x_i\)</span> in the corpus, <span class="math notranslate nohighlight">\(|V|\)</span> is the size of the vocabulary, and <span class="math notranslate nohighlight">\(\psi(c_i)\)</span> is the digamma function.</p>
</li>
<li><p>Repeat the M-step and E-step until the unigram probabilities converge. The log-likelihood is monotonically increasing, so we can stop the training when the log-likelihood does not increase for a certain number of iterations.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ekorpkit</span> <span class="kn">import</span> <span class="n">eKonf</span>
<span class="kn">from</span> <span class="nn">ekorpkit.tokenizers.trainers.unigram</span> <span class="kn">import</span> <span class="n">UnigramTrainer</span>

<span class="n">sp</span> <span class="o">=</span> <span class="n">UnigramTrainer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Initialize the unigram probabilities <span class="math notranslate nohighlight">\(p(x_i)\)</span>. The frequency of each unigram is used as the initial value.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eKonf</span><span class="o">.</span><span class="n">viewsource</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">initialize_vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    def initialize_vocab(self, words):
        word_freqs = self.get_word_freqs(words)
        sorted_subwords, characters = self.initialize_subwords(word_freqs)

        alphabet = {char: 0 for char in self.initial_alphabet if char not in characters}
        tokens = list(characters.items()) + sorted_subwords + list(alphabet.items())
        tokens = {token: freq for token, freq in tokens}
        tokens = collections.Counter(tokens)
        return tokens, characters
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>M-step: Compute the most likely sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> given the current unigram probabilities <span class="math notranslate nohighlight">\(p(x_i)\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eKonf</span><span class="o">.</span><span class="n">viewsource</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">M_step</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    def M_step(self, text, trie):
        loss, p = self.forward_step(text, trie)
        tokenization = self.backward_step(text, p)
        return tokenization, loss
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>E-step: Given the current most likely sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, update the unigram probabilities <span class="math notranslate nohighlight">\(p(x_i)\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eKonf</span><span class="o">.</span><span class="n">viewsource</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">E_step</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    def E_step(self, tokenization, trie):
        # get the new token counts based on updated tokenization
        counts = collections.Counter(tokenization)
        norm = sum(list(counts.values()))

        # we are returning the log probabilties here (alpha=0 prior)
        logsum = digamma(norm)
        for k, v in counts.items():
            counts[k] = digamma(v) - logsum

        for k, v in counts.items():
            trie.set_value(k, v)
        return trie
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Repeat the M-step and E-step until the unigram probabilities converge.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eKonf</span><span class="o">.</span><span class="n">viewsource</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">EM_round</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    def EM_round(self, text, tokens, delta=0.01, n_iterations=10):
        tokenization, old_loss = self.M_step(text, self.trie)
        for step in range(n_iterations):
            print(f&quot;EM iter {step}: &quot;, end=&quot;&quot;)
            loss, tokenization, trie = self.EM_step(text, tokenization, self.trie)
            print(f&quot;Loss={loss:.2f}&quot;)
            if abs(old_loss - loss) &lt; delta:
                break
            old_loss = loss
</pre></div>
</div>
</div>
</div>
<section id="finding-the-optimal-segmentation">
<h3>Finding the optimal segmentation<a class="headerlink" href="#finding-the-optimal-segmentation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>If all of the subwords were of the same length, we could apply the Viterbi algorithm to find the optimal segmentation.</p></li>
<li><p>The Viterbi algorithm is a dynamic programming algorithm that finds the most likely sequence of states, given a sequence of observations and a model of the transition probabilities between states and the emission probabilities of the observations given the states.</p>
<ul>
<li><p>You have some hidden states <span class="math notranslate nohighlight">\(z_1, z_2, \cdots, z_n\)</span>, and you want to transition from <span class="math notranslate nohighlight">\(z_1 \rightarrow z_2 \rightarrow \cdots \rightarrow z_n\)</span>, and you know the transition matrix <span class="math notranslate nohighlight">\(A_{ij}\)</span>, giving the probability of transitioning from <span class="math notranslate nohighlight">\(z_i^{(1)}\)</span> to <span class="math notranslate nohighlight">\(z_j^{(2)}\)</span>, where <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are the hidden states, and the superscript indicates the sequence order.</p></li>
</ul>
</li>
<li><p>The problem is that <span class="math notranslate nohighlight">\(A\)</span> is not between adjacent states.</p></li>
</ul>
<ul>
<li><p>Consider tokenizing the word <code class="docutils literal notranslate"><span class="pre">hello</span></code> given the subwords {<code class="docutils literal notranslate"><span class="pre">he</span></code>, <code class="docutils literal notranslate"><span class="pre">h</span></code>, <code class="docutils literal notranslate"><span class="pre">ll</span></code>, <code class="docutils literal notranslate"><span class="pre">e</span></code>, <code class="docutils literal notranslate"><span class="pre">o</span></code>, <code class="docutils literal notranslate"><span class="pre">hell</span></code>}.</p></li>
<li><p>We can generate the following figure:</p>
<p><img alt="" src="../../../_images/transition.png" /></p>
<ul class="simple">
<li><p>Each arrow represents a transition, and the weight of the arrow is the probability of the transition.</p></li>
<li><p>The goal is to pick arrows that we arrive at <code class="docutils literal notranslate"><span class="pre">&lt;eos&gt;</span></code> (end of sequence) with the highest probability.</p></li>
</ul>
</li>
<li><p>This problem has optimal substructure, so we can apply dynamic programming to solve it.</p></li>
<li><p>For example, assume that we are at the state (4).</p>
<ul>
<li><p>There are three arrows that can lead to the state (4), a red, a blue, and a green arrow.</p></li>
<li><p>The highest probability at the state (4) is just the best path from the previous state, plus the probability of the transition.</p>
<div class="math notranslate nohighlight">
\[
    p_i = \max_{j \le i} (p_j p_{j \rightarrow i})
    \]</div>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>The Trie structure is used to find the optimal segmentation.</p>
<ul>
<li><p>The Trie structure is a tree structure that is used to store a set of strings.</p></li>
<li><p>The following figure shows the Trie structure for the subwords {<code class="docutils literal notranslate"><span class="pre">h</span></code>, <code class="docutils literal notranslate"><span class="pre">he</span></code>, <code class="docutils literal notranslate"><span class="pre">hell</span></code>, <code class="docutils literal notranslate"><span class="pre">hello</span></code>}:</p>
<p><img alt="" src="../../../_images/trie.png" /></p>
</li>
<li><p>The root node is the start of the sequence, <code class="docutils literal notranslate"><span class="pre">&lt;sos&gt;</span></code>.</p></li>
<li><p>Any time we encounter an <code class="docutils literal notranslate"><span class="pre">end</span></code> node, it means that everything in the path from <code class="docutils literal notranslate"><span class="pre">&lt;sos&gt;</span></code> to <code class="docutils literal notranslate"><span class="pre">end</span></code> is a valid subword.</p></li>
<li><p>The root node <code class="docutils literal notranslate"><span class="pre">&lt;sos&gt;</span></code> will begin with exactly one branch for every unique first character in the vocabulary.</p></li>
<li><p>As we grow the available subwords, we create more branches.</p></li>
<li><p>The Trie is going to be the fundamental data structure that the tokenizer uses to store and retrieve the subwords.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eKonf</span><span class="o">.</span><span class="n">viewsource</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">initialize_trie</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    def initialize_trie(self, tokens):
        trie = Trie()
        norm = sum(list(tokens.values()))
        logsum = digamma(norm)

        maxlen = 0
        for tok, val in tokens.items():
            trie.add(tok, digamma(val) - logsum)
            maxlen = max(maxlen, len(tok))

        return trie, maxlen
</pre></div>
</div>
</div>
</div>
</section>
<section id="fitting-the-model">
<h3>Fitting the model<a class="headerlink" href="#fitting-the-model" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>One of the algorithm for finding the optimal sequence from the Trie is a forwards-backwards algorithm.</p>
<ul>
<li><p>It is a special subset of the sum-product algorithm for training directed graphical models.</p></li>
<li><p>More sophisticated algorithms include the Forward-DP Backward-A* algorithm, and the Forward-Filtering and Backward-Sampling algorithm (FFBS).</p></li>
</ul>
</li>
<li><p>When we compute the forward step, we also store the length of the longest subword that ends at the current position.</p></li>
<li><p>This allows us to backtrack from the end of the sequence to the beginning, and find the optimal segmentation, since the length of the arrow is the length of the subword.</p></li>
<li><p>The EM step puts together the E step and the M step, where the E step is updating the Trie, and the M step is finding the optimal segmentation using the forwards-backwards algorithm explained above.</p></li>
<li><p>Then, fitting the model is just a matter of repeating the EM step until the log-likelihood converges.</p></li>
<li><p>One more thing to consider is to get the desired vocabulary size by pruning the vocabulary.</p>
<ul>
<li><p>First, prepare more subword tokens than the desired vocabulary size.</p></li>
<li><p>After each EM step, remove the least probable, say 10%, of the subwords.</p></li>
<li><p>Repeat this process until the desired vocabulary size is reached.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eKonf</span><span class="o">.</span><span class="n">viewsource</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">forward_step</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    def forward_step(self, text, trie):
        N = len(text)

        # d[i] contains the maximum log_prob of any tokenization
        # of text[:i], initialized to 0 (i.e. log(0)=-infty)
        d = [-np.inf] * (N + 1)

        # p[i] (stands for parent) contains the number of characters of
        # the final token in the most likely sequence that ends at index i
        p = [None] * (N + 1)
        d[0] = 0

        for i in range(1, N + 1):

            # find all possible final words. Have to look back
            # a distance set by the length of the longest token
            for j in range(max(i - self.maxlen, 0), i):

                final_token = text[j:i]
                final_value = trie.get_value(final_token)

                # if the current ending word has a higher log-probability,
                # save that value and store the word (i.e. # chars to backtrack)
                if final_value and d[j] + final_value &gt; d[i]:
                    d[i] = d[j] + final_value
                    p[i] = len(final_token)
            if p[i] is None:
                raise ValueError(f&quot;Encountered unknown token &#39;{text[i-1]}&#39;.&quot;)

        loss = d[-1]
        return loss, p
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eKonf</span><span class="o">.</span><span class="n">viewsource</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">backward_step</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    def backward_step(self, text, p):
        idx = len(p)
        tokenization = []
        while idx &gt; 1:
            # move back the number of steps p tells you to
            next_idx = idx - p[idx - 1]

            # extract the final token
            tok = text[next_idx - 1 : idx - 1]
            tokenization.append(tok)

            idx = next_idx
        tokenization = list(reversed(tokenization))
        return tokenization
</pre></div>
</div>
</div>
</div>
</section>
<section id="subword-sampling">
<h3>Subword sampling<a class="headerlink" href="#subword-sampling" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>To find alternative segmentations, we can save the n-best paths in the forward-backward step.</p></li>
<li><p>Now, we can sample from the n-best paths to find alternative segmentations.</p></li>
</ul>
</section>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ekorpkit</span> <span class="kn">import</span> <span class="n">eKonf</span>

<span class="n">cfg</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="s2">&quot;path&quot;</span><span class="p">)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">uri</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/us_equities_news_sampled.zip&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;us_equities_news_sampled.parquet&quot;</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">cached_path</span><span class="p">)</span>
<span class="n">texts</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">text</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ekorpkit.tokenizers.sentencepiece</span> <span class="kn">import</span> <span class="n">SentencePieceUnigramTokenizer</span>

<span class="n">texts</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">text</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="n">sp</span> <span class="o">=</span> <span class="n">SentencePieceUnigramTokenizer</span><span class="p">()</span>
<span class="n">sp</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span>
    <span class="n">texts</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">min_frequency</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">initial_alphabet</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;!&quot;</span><span class="p">,</span> <span class="s2">&quot;?&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Round 1. Vocab size: 9641 ---
EM iter 0: Loss=-596166.15
EM iter 1: Loss=-593240.93
EM iter 2: Loss=-592453.62
EM iter 3: Loss=-592143.08
EM iter 4: Loss=-592028.75
--- Round 2. Vocab size: 7231 ---
EM iter 0: Loss=-616021.42
EM iter 1: Loss=-615565.28
EM iter 2: Loss=-615379.74
EM iter 3: Loss=-615290.66
EM iter 4: Loss=-615283.06
--- Round 3. Vocab size: 5424 ---
EM iter 0: Loss=-640810.86
EM iter 1: Loss=-640411.09
EM iter 2: Loss=-640241.08
EM iter 3: Loss=-640186.43
EM iter 4: Loss=-640155.71
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sp</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;../data/tokenizers&quot;</span><span class="p">,</span> <span class="s2">&quot;sp_unigram&quot;</span><span class="p">,</span> <span class="n">pretty</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;../data/tokenizers/sp_unigram/vocab.json&#39;,
 &#39;../data/tokenizers/sp_unigram/config.json&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ekorpkit.tokenizers.sentencepiece</span> <span class="kn">import</span> <span class="n">SentencePieceUnigramTokenizer</span>

<span class="n">sp</span> <span class="o">=</span> <span class="n">SentencePieceUnigramTokenizer</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;../data/tokenizers&quot;</span><span class="p">,</span> <span class="s2">&quot;sp_unigram&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequence</span> <span class="o">=</span> <span class="s2">&quot;Investment opportunities in the company.&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">sequence</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize</span><span class="p">(</span><span class="n">sequence</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">generalized_forward_step</span><span class="p">(</span><span class="s2">&quot;▁company.&quot;</span><span class="p">,</span> <span class="n">sp</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">trie</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>investment opportunities in the company.
[&#39;▁Investment&#39;, &#39;▁opportunities&#39;, &#39;▁in&#39;, &#39;▁the&#39;, &#39;▁company.&#39;]
[None, [1], [2], [3], [4], [5], [6], [7], [8], [1]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">nbest_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenized_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 [&#39;▁&#39;, &#39;investment&#39;, &#39;▁opportun&#39;, &#39;iti&#39;, &#39;es&#39;, &#39;▁i&#39;, &#39;n&#39;, &#39;▁&#39;, &#39;th&#39;, &#39;e&#39;, &#39;▁co&#39;, &#39;mpany&#39;, &#39;.&#39;]
1 [&#39;▁&#39;, &#39;in&#39;, &#39;vestment&#39;, &#39;▁opportuni&#39;, &#39;ties&#39;, &#39;▁&#39;, &#39;in&#39;, &#39;▁&#39;, &#39;th&#39;, &#39;e&#39;, &#39;▁&#39;, &#39;company&#39;, &#39;.&#39;]
2 [&#39;▁&#39;, &#39;investment&#39;, &#39;▁o&#39;, &#39;pportuni&#39;, &#39;t&#39;, &#39;ies&#39;, &#39;▁&#39;, &#39;i&#39;, &#39;n&#39;, &#39;▁the&#39;, &#39;▁&#39;, &#39;company&#39;, &#39;.&#39;]
3 [&#39;▁investment&#39;, &#39;▁&#39;, &#39;op&#39;, &#39;port&#39;, &#39;un&#39;, &#39;it&#39;, &#39;i&#39;, &#39;es&#39;, &#39;▁&#39;, &#39;i&#39;, &#39;n&#39;, &#39;▁the&#39;, &#39;▁&#39;, &#39;c&#39;, &#39;o&#39;, &#39;mpany&#39;, &#39;.&#39;]
4 [&#39;▁&#39;, &#39;investmen&#39;, &#39;t&#39;, &#39;▁&#39;, &#39;opport&#39;, &#39;un&#39;, &#39;itie&#39;, &#39;s&#39;, &#39;▁in&#39;, &#39;▁&#39;, &#39;th&#39;, &#39;e&#39;, &#39;▁&#39;, &#39;compan&#39;, &#39;y&#39;, &#39;.&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/google/sentencepiece">SentencePiece</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15">SentencePiece Tokenizer Demystified</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/lectures/deep_nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="tokenization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Tokenization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../aiart/index.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">AI Art (Text-to-Image)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Young Joon Lee<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>