{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "![](../figs/deep_nlp/tokenization/entelecheia_puzzle_pieces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc7030",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "NLP systems have three main components that help machines understand natural language:\n",
    "\n",
    "- **Tokenization**: Splitting a string into a list of tokens.\n",
    "- **Embedding**: Mapping tokens to vectors.\n",
    "- **Model**: A neural network that takes token vectors as input and outputs predictions.\n",
    "\n",
    "Tokenization is the first step in the NLP pipeline. \n",
    "\n",
    "- Tokenization is the process of splitting a string into a list of tokens. \n",
    "- For example, the sentence \"I like to eat apples\" can be tokenized into the list of tokens `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`. \n",
    "- The tokens can be words, characters, or subwords.\n",
    "\n",
    "> In deep learning, tokenization is the process of converting a sequence of characters into a sequence of tokens, then converting each token into a numerical vector to be used as input to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e6390",
   "metadata": {},
   "source": [
    "## What is Tokenization?\n",
    "\n",
    "- Tokenization is the process of representing a text in smaller units called tokens.\n",
    "- In a very simple case, we can simply map every word in the text to a numerical index.\n",
    "- For example, the sentence \"I like to eat apples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`. \n",
    "- Then, each token can be mapped to a unique index, such as:\n",
    "  > `{\"I\": 0, \"like\": 1, \"to\": 2, \"eat\": 3, \"apples\": 4}`.\n",
    "- There are more linguistic features to consider when tokenizing a text, such as punctuation, capitalization, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc320d",
   "metadata": {},
   "source": [
    "## Why do we need tokenization?\n",
    "\n",
    "- \"How can we make a machine read a sentence?\"\n",
    "- Machines donâ€™t know any language, nor do they understand sounds or phonetics.\n",
    "- They need to be taught from scratch.\n",
    "- The first step is to break down the sentence into smaller units that the machine can process.\n",
    "- Tokenization determines how the input is represented to the model.\n",
    "- This decision has a huge impact on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9deb0f",
   "metadata": {},
   "source": [
    "## Tokenization Methods\n",
    "\n",
    "- Word-level tokenization: Split a sentence into words.\n",
    "- Character-level tokenization: Split a sentence into characters.\n",
    "- Subword-level tokenization: Split a sentence into subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4007e",
   "metadata": {},
   "source": [
    "##  Word (White Space) Tokenization\n",
    "\n",
    "- The simplest tokenization method is to split a sentence into words.\n",
    "- This is also called white space tokenization.\n",
    "- The sentence \"I like to eat apples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`.\n",
    "- This method is very fast and easy to implement.\n",
    "- However, it has some limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedde7b",
   "metadata": {},
   "source": [
    "### Problems with Word tokenizer\n",
    "\n",
    "- Out-of-vocabulary (OOV) words: \n",
    "  - The risk of missing words that are not in the vocabulary.\n",
    "  - The model will not recognize the variants of words that were not in the training set.\n",
    "  - For example, even though the words `pine` and `apple` exist in the training set, the model will not recognize the word `pineapple`.\n",
    "- Punctuation and abbreviations: \n",
    "  - The tokenizer will not recognize punctuation and abbreviations.\n",
    "  - For example, the word `don't` will be tokenized as `[\"do\", \"n't\"]`.\n",
    "- Slang and informal language: \n",
    "  - The tokenizer will not recognize slang and informal language.\n",
    "  - For example, the word `gonna` will be tokenized as `[\"gon\", \"na\"]`.\n",
    "  - `tl;dr` will be tokenized as `[\"tl\", \";\", \"dr\"]`.\n",
    "- What if language does not use spaces for separating words?\n",
    "  - Chinese, Japanese, and Korean do not use spaces to separate words.\n",
    "  - The tokenizer will not work for these languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f5243",
   "metadata": {},
   "source": [
    "## Character Tokenization\n",
    "\n",
    "- To solve the problems of word tokenization, we can split a sentence into characters.\n",
    "- The sentence \"I like to eat apples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \" \", \"l\", \"i\", \"k\", \"e\", \" \", \"t\", \"o\", \" \", \"e\", \"a\", \"t\", \" \", \"a\", \"p\", \"p\", \"l\", \"e\", \"s\"]`.\n",
    "- However, this method has its own problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c38ec",
   "metadata": {},
   "source": [
    "### Problems with Character tokenizer\n",
    "\n",
    "- The number of tokens is very large.\n",
    "  - This requires more computation and memory.\n",
    "- Limit the application of the model.\n",
    "  - Only certain types of models can be used.\n",
    "  - It is inefficient for the certain types of applications, such as NER.\n",
    "- It would be difficult to understand the relationship between the tokens.\n",
    "  - For example, the tokens `[\"a\", \"p\", \"p\", \"l\", \"e\"]` do not represent the word `apple`.\n",
    "  - The tokens `[\"a\", \"p\", \"p\", \"l\", \"e\"]` do not have any relationship with the tokens `[\"a\", \"p\", \"p\", \"l\", \"e\", \"s\"]`.\n",
    "- Incorrect spelling could be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d9aa2",
   "metadata": {},
   "source": [
    "## Subword Tokenization\n",
    "\n",
    "- With character-level tokenization, we risk losing the semantic features of the words. \n",
    "- With word-level tokenization, we have out-of-vocabulary (OOV) words or very large vocabulary sizes.\n",
    "- To solve the problems of word tokenization and character tokenization, an algorithm should be able to:\n",
    "  - Retain the semantic features of the words.\n",
    "  - Tokenize any words without the need for a huge vocabulary.\n",
    "- Subword tokenization is a method that can solve these problems.\n",
    "- For example, the sentence \"I like to eat pineapples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \"like\", \"to\", \"eat\", \"pine\", \"##app\", \"##les\"]`.\n",
    "- The model only learns a few subwords that can be used to represent any word.\n",
    "- This solves the problem of OOV words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ac9c3",
   "metadata": {},
   "source": [
    "### How to decide which subwords to use?\n",
    "\n",
    "- There are several algorithms that can be used to decide which subwords to use.\n",
    "  - Byte Pair Encoding (BPE)\n",
    "  - Unigram Language Model\n",
    "  - Subword Sampling\n",
    "  - Byte-level BPE\n",
    "  - WordPiece\n",
    "  - SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fd915",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE)\n",
    "\n",
    "Sennrich et al. (2016) proposed a method called Byte Pair Encoding (BPE) to learn subword units. \n",
    "{cite}`sennrich-etal-2016-neural`\n",
    "\n",
    "- Byte Pair Encoding algorithm is originally used for compressing text.\n",
    "- It splits words into sequences of characters and iteratively combines the most frequent character pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c0a95",
   "metadata": {},
   "source": [
    " ### Token Learning from Dataset\n",
    "\n",
    "  - Count the frequency of each word shown in the corpus. \n",
    "  - For each word, append a special stop token `</w>` at the end of the word. \n",
    "  - Then, split the word into characters. \n",
    "  - Initially, the tokens of the word are all of its characters plus the additional `</w>` token. \n",
    "  - For example, the tokens for word `low` are [`l`, `o`, `w`, `</w>`] in order. \n",
    "  - So after counting all the words in the dataset, we will get a vocabulary for the tokenized word with its corresponding counts\n",
    "\n",
    "```\n",
    "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b470e11",
   "metadata": {},
   "source": [
    "- In each iteration, count the frequency of each consecutive byte pair, find out the most frequent one, and merge the two byte pair tokens to one token.\n",
    "\n",
    "- For the above example, in the first iteration of the merge, because byte pair `e` and `s` occurred 6 + 3 = 9 times which is the most frequent, merge these into a new token `es`. \n",
    "- Note that token `s` is also gone in this particular example.\n",
    "\n",
    "```\n",
    "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20dc35",
   "metadata": {},
   "source": [
    "- In the second iteration of merge, token `es` and `t` occurred 6 + 3 = 9 times, which is the most frequent. \n",
    "- Merge these to into a new token `est`. \n",
    "- Note that token `es` and `t` are also gone.\n",
    "\n",
    "```\n",
    "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae4514",
   "metadata": {},
   "source": [
    "- In the third iteration of the merge, token `est` and `</w>` pair is the most frequent, etc.\n",
    "- Do this until we have the desired number of tokens or reach the maximum number of iterations.\n",
    "\n",
    "Stop token `</w>` is also important. \n",
    "- Without `</w>`, say if there is a token `st`, this token could be in the word `st ar`, or the wold `wide st`.\n",
    "- Those two words are very different in meaning, but the token `st` is the same.\n",
    "- With `</w>`, if there is a token `st</w>`, the model immediately know that it is the token for the wold `wide st</w>` but not `st ar</w>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd9f95",
   "metadata": {},
   "source": [
    "To summarize, the algorithm is as follows:\n",
    "1. Extract the words from the given dataset along with their count. \n",
    "2. Define the vocabulary size. \n",
    "3. Split the words into a character sequence. \n",
    "4. Add all the unique characters in our character sequence to the vocabulary. \n",
    "5. Select and merge the symbol pair that has a high frequency. \n",
    "6. Repeat step 5 until the vocabulary size is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [NLP Tokenization](https://medium.com/nerd-for-tech/nlp-tokenization-2fdec7536d17)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
