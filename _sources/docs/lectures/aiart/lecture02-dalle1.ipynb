{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899602f2-b9ee-458f-bb12-8da95bfb76d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# DALL·E 1\n",
    "\n",
    "- Paper: https://arxiv.org/abs/2102.12092\n",
    "- Blog post: https://openai.com/blog/dall-e/\n",
    "- Code: https://github.com/openai/dall-e (it’s not complete and covers only the dVAE part)\n",
    "- Model: Not available\n",
    "- Alternative code (PyTorch): https://github.com/lucidrains/DALLE-pytorch \n",
    "- Alternative code (JAX/Flax): https://github.com/borisdayma/dalle-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d145cd0-2c51-4d1a-add2-1afee681c21a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![dalle2](../figs/aiart_1_dalle1.png)\n",
    "\n",
    "- The first version of DALL·E was a GPT-3 style transformer decoder that autoregressively generated a 256×256 image based on textual input and an optional beginning of the image.\n",
    "- A text is encoded by BPE-tokens (max. 256), and an image is encoded by special image tokens (1024 of them) produced by a discrete variational autoencoder (dVAE). \n",
    "- dVAE encodes a 256×256 image into a grid of 32×32 tokens with a vocabulary of 8192 possible values. \n",
    "- Because of the dVAE, some details and high-frequency features are lost in generated images, so some blurriness and smoothness are the features of the DALL·E-generated images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c974d-81c7-40a6-9f42-f38bed950931",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## DALL·E 1 Charateristics\n",
    "\n",
    "### Controlling Attributes\n",
    "\n",
    "![attributes](../figs/aiart_1_dalle1_attributes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fc5e6-edd6-44a8-b452-c05786223e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Drawing Multiple Objects\n",
    "\n",
    "![objects](../figs/aiart_1_dalle1_objects.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f62cb7-532a-4907-8d85-53134ec0135e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Visualizing Perspective and Three-Dimensionality\n",
    "\n",
    "![perspective](../figs/aiart_1_dalle1_perspective.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a6be5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Visualizing Internal and External Structure\n",
    "\n",
    "![structure](../figs/aiart_1_dalle1_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb82a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Inferring Contextual Details\n",
    "\n",
    "![context](../figs/aiart_1_dalle1_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d3aa10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## DALL·E 1 Architecture\n",
    "\n",
    "- The transformer is a large model with 12B parameters. \n",
    "- It consisted of 64 sparse transformer blocks with a complicated set of attention mechanisms inside, consisting of \n",
    "  - classical text-to-text masked attention, \n",
    "  - image-to-text attention, and \n",
    "  - image-to-image sparse attention. \n",
    "- All three attention types are merged into a single attention operation. \n",
    "- The model was trained on a dataset of 250M image-text pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd22977",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### VQ-VAE\n",
    "\n",
    "![vqvae](../figs/aiart_1_dalle1_vqvae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29788c78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- VQ-VAE is a type of variational autoencoder that uses vector quantization to obtain a discrete latent representation.\n",
    "- This is in contrast to the continuous latent space that other variational autoencoders have.\n",
    "- The objective function of a VQ-VAE, when trained on an image dataset, can be written as:\n",
    "  \n",
    "  $\\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\beta \\cdot D_{KL}[q(z|x) || p(z)]$\n",
    "  \n",
    "  where $p(x)$ is the data distribution, $q$ is the approximate posterior over latent variables and $D_{KL}$ denotes the Kullback-Leibler divergence. \n",
    "- This objective function encourages the model to learn an efficient codebook that minimises reconstruction error while also matching the prior distribution over codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64471ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**A latent space**\n",
    "\n",
    "A latent space is obtained by encoding the input image into the nearest codebook entry. This process is called vector quantization and results in a discrete latent space.\n",
    "\n",
    "![latent](../figs/aiart_1_dalle1_latent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ac1db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Autoencoders\n",
    "\n",
    "![autoencoder](../figs/aiart_1_dalle1_autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da313ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Autoencoder is a neural network that is trained to predict its input.\n",
    "- The objective function of an autoencoder can be written as:\n",
    "\n",
    "\n",
    "  $\\mathcal{L} = \\mathbb{E}_{p(x)}[\\log p(x|z)]$\n",
    "\n",
    "  \n",
    "  where $p(x)$ is the data distribution and $p(x|z)$ is the model distribution. \n",
    "- This objective function encourages the model to learn a latent space that captures the underlying structure of the data.\n",
    "\n",
    "- A VQ-VAE can be seen as a type of autoencoder where the latent space is constrained to be discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e095e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Variational Autoencoders (VAE)\n",
    "\n",
    "![vae](../figs/aiart_1_dalle1_vae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f133baa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Variational Autoencoders (VAE) is a type of autoencoder where the latent space is continuous. \n",
    "- The objective function of a VAE can be written as:\n",
    "\n",
    "\n",
    "  $\\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}[q(z|x) || p(z)]$\n",
    "\n",
    "  \n",
    "  where $p(x)$ is the data distribution, $q$ is the approximate posterior over latent variables and $D_{KL}$ denotes the Kullback-Leibler divergence. \n",
    "- This objective function encourages the model to learn a latent space that captures the underlying structure of the data while also matching the prior distribution over latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3272ed49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Discrete Spaces\n",
    "\n",
    "![discrete](../figs/aiart_1_dalle1_discrete.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4140b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Discrete spaces are more efficient to represent than continuous spaces. \n",
    "- This is because a discrete space can be represented with a finite number of bits, whereas a continuous space requires an infinite number of bits. \n",
    "- In addition, discrete spaces are easier to manipulate and reason about than continuous spaces. \n",
    "- For these reasons, VQ-VAE is more efficient than VAE at learning latent representations of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c5a4dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Uncertainty in the Posterior\n",
    "\n",
    "![uncetainty1](../figs/aiart_1_dalle1_uncetainty1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5992b79",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Uncertainty in the Posterior**\n",
    "\n",
    "![uncetainty2](../figs/aiart_1_dalle1_uncetainty2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94c1cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Uncertainty in the posterior is added by soft-sampling codebook vectors from the Gumbel-Softmax distribution. \n",
    "- This results in a softened latent space which can be seen as a continuous approximation of the discrete latent space.\n",
    "\n",
    "\n",
    "- The Gumbel-Softmax distribution is a type of distribution that allows for sampling from a discrete space while still allowing for gradients to flow through the samples. \n",
    "- This is useful for training models with discrete latent spaces, such as VQ-VAE. \n",
    "- The Gumbel-Softmax distribution is defined as:\n",
    "\n",
    "\n",
    "  $G(z;\\mu,\\beta) = \\frac{\\exp((z - \\mu)/\\beta)}{\\sum_{k=1}^K \\exp((z_k - \\mu)/\\beta)}$\n",
    "\n",
    "  \n",
    "  where $\\mu$ is the mean, $\\beta$ is the temperature and $K$ is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af3421",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Comparison of original images (top) and reconstructions from the dVAE (bottom)**\n",
    "\n",
    "![dvae](../figs/aiart_1_dalle1_dvae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b9517",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Decoder\n",
    "\n",
    "A GPT-3 like transformer decoder consumes a sequence of text tokens and (optional) image tokens (here a single image token with id 42) and produces a continuation of an image (here the next image token with id 1369)\n",
    "\n",
    "![decoder](../figs/aiart_1_dalle1_decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ba94f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Sampling From a Trained DALL-E\n",
    "\n",
    "![sampling](../figs/aiart_1_dalle1_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0900c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![sampling](../figs/aiart_1_dalle1_sampling2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ca75c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## DALL·E 1 Results\n",
    "\n",
    "Several image generation examples from the original paper\n",
    "\n",
    "![examples](../figs/aiart_1_dalle1_examples.png)\n",
    "\n",
    "The trained model generated several samples (up to 512!) based on the text provided, then all these samples were ranked by a special model called CLIP, and the top-ranked one was chosen as the result of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f99f13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![eval](../figs/aiart_1_dalle1_eval.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
