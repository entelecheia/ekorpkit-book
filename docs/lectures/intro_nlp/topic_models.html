
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Topic Models &#8212; eKorpkit Tutorials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/ekorpkit.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Deep Learning for NLP" href="../deep_nlp/index.html" />
    <link rel="prev" title="Topic Modeling" href="topic.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FN4GFT8HP8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FN4GFT8HP8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/ekorpkit.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">eKorpkit Tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    eKonomic Research Python Toolkit
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/index.html">
   Getting started
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/install.html">
     Installation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/usage.html">
     Usage
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/features/index.html">
   Key features
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/easy_config.html">
     Easy Configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/no_boiler.html">
     No Boilerplate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/workflows.html">
     Workflows
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/share.html">
     Sharable and Reproducible
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/plug.html">
     Pluggable Architece
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/config/index.html">
   Configuring ekorpkit
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/config/eKonf.html">
     Using eKonf class
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/datasets/index.html">
   Datasets
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/corpus.html">
     Build and Load Corpora
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/datasets/dataset.html">
     Build and Load Datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/pipeline.html">
     Corpus task pipelines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/models/index.html">
   Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/automl/index.html">
     Auto ML
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/embeddings/index.html">
     Word Embeddings
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/word2vec_basics.html">
       Word2Vec Basics
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/eval_vectors.html">
       Evaluate pretrained embeddings
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/ngram/index.html">
     N-Grams
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram.html">
       N-Grams
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_lexicons.html">
       N-Gram model for ngram lexicon features
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_words.html">
       N-Gram model for unigram lexicon features
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/sentiment/index.html">
     Sentiment Analyers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/lbsa_en.html">
       Lexicon-based Sentiment Analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/financial_phrasebank_lm.html">
       Evaluate LM with financial phrasebank
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/sentiment.html">
       LM Dictionary vs. finbert vs. T5
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/transformers/index.html">
     Transformers
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/art/index.html">
     AI Art (Text-to-Image)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/disco.html">
       Disco Diffusion
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/dalle-mini.html">
       DALL·E Mini
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/pipelines/index.html">
   Pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/pipelines/pipeline.html">
     Instantiating pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/preprocessors/index.html">
   Preprocessors
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/normalizer/index.html">
     Normalizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/normalizer/normalizer.html">
       Normalizers
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/segmenter/index.html">
     Segmenters
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/segmenter/segmenter.html">
       Segmenters
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/index.html">
     Tokenizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
    <label for="toctree-checkbox-14">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/mecab.html">
       Mecab Tokenizer
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/visualizers/index.html">
   Visualizers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/visualizers/plot.html">
     Plots
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/workflow/index.html">
   Workflows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../corpus/ekorpkot_corpus.html">
   The eKorpkit Corpus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Use Cases
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/fomc/index.html">
   FOMC
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/02_FOMC_numerical_data.html">
     Preparing Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/03_FOMC_corpus.html">
     Preparing Textual Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/04_FOMC_EDA_numericals.html">
     EDA on Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/05_FOMC_features.html">
     Visualizing Features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/06_FOMC_AutoML.html">
     Checking Baseline with AutoML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/07_FOMC_sentiments.html">
     Predicting Sentiments of FOMC Corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/08_FOMC_EDA_sentiments.html">
     EDA on Sentiment Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/09_FOMC_AutoML_with_tones.html">
     Predicting the next decisions with tones
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../usecases/bok/index.html">
   Bank of Korea
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/edgar/index.html">
   EDGAR
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/edgar/predict_edgar.html">
     Prediciting Sentiments
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg/index.html">
   ESG
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/snorkel-polarity.html">
     Preparing polarity classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix.html">
     Improving classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/classifiers.html">
     Training Classifiers for ESG Ratings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/econ_news_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       econ_news_kr
      </span>
     </code>
     corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_news.html">
     Predicting ESG Categories and Polarities
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg-en/index.html">
   ESG (English)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg-en/esg_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       JOCo
      </span>
     </code>
     corpus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/cointax/index.html">
   Taxation on Cryptocurrency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/cointax/coin_dataset.html">
     Improving classification datasets
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/analyst.html">
   ESG Topic Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/rethink_esg.html">
   Rethinking ESG
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Introduction to NLP
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="research.html">
     Research Applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="topic.html">
     Topic Modeling
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Topic Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../deep_nlp/index.html">
   Deep Learning for NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/zeroshot.html">
     Zero Shot, Prompt, and Search Strategies
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/bloom.html">
     What is BLOOM?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/bloom_apps.html">
     Bloom Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/transformers.html">
     Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../aiart/index.html">
   AI Art (Text-to-Image)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
  <label for="toctree-checkbox-23">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/project-themes.html">
     Project Themes - A Brave New World
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle1.html">
     DALL·E 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ds/index.html">
   Data Science for Economics and Finance
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/index.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/cite.html">
   Citation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/entelecheia/ekorpkit-book/main?urlpath=tree/ekorpkit-book/docs/lectures/intro_nlp/topic_models.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/entelecheia/ekorpkit-book/blob/main/ekorpkit-book/docs/lectures/intro_nlp/topic_models.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book/issues/new?title=Issue%20on%20page%20%2Fdocs/lectures/intro_nlp/topic_models.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/docs/lectures/intro_nlp/topic_models.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-tomotopy">
   What is tomotopy?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-started">
   Getting Started
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-of-tomotopy">
   Performance of tomotopy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-save-and-load">
   Model Save and Load
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#documents-in-the-model-and-out-of-the-model">
   Documents in the Model and out of the Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-for-unseen-documents">
   Inference for Unseen Documents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#corpus-and-transform">
   Corpus and transform
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parallel-sampling-algorithms">
   Parallel Sampling Algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pining-topics-using-word-priors">
   Pining Topics using Word Priors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples">
   Examples
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#install-or-upgrade-of-ekorpkit">
     Install or upgrade of ekorpkit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-a-dataset">
     Load a dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda-basics">
     LDA Basics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda-visualization">
     LDA Visualization
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Topic Models </h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-tomotopy">
   What is tomotopy?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-started">
   Getting Started
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-of-tomotopy">
   Performance of tomotopy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-save-and-load">
   Model Save and Load
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#documents-in-the-model-and-out-of-the-model">
   Documents in the Model and out of the Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-for-unseen-documents">
   Inference for Unseen Documents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#corpus-and-transform">
   Corpus and transform
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parallel-sampling-algorithms">
   Parallel Sampling Algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pining-topics-using-word-priors">
   Pining Topics using Word Priors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples">
   Examples
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#install-or-upgrade-of-ekorpkit">
     Install or upgrade of ekorpkit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-a-dataset">
     Load a dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda-basics">
     LDA Basics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda-visualization">
     LDA Visualization
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="topic-models-jupyter-book-badge">
<h1>Topic Models <a class="reference external" href="https://entelecheia.github.io/ekorpkit-book/"><img alt="Jupyter Book Badge" src="https://jupyterbook.org/badge.svg" /></a><a class="headerlink" href="#topic-models-jupyter-book-badge" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://github.com/bab2min/tomotopy"><img alt="tomoto" src="../../../_images/tomoto.png" /></a></p>
<p>Package tomotopy <span id="id1">[<a class="reference internal" href="../../about/index.html#id13" title="Minchul Lee. Bab2min/tomotopy: 0.12.3. July 2022. URL: https://doi.org/10.5281/zenodo.6868418, doi:10.5281/zenodo.6868418.">Lee, 2022</a>]</span></p>
<section id="what-is-tomotopy">
<h2>What is tomotopy?<a class="headerlink" href="#what-is-tomotopy" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> is a Python extension of <code class="docutils literal notranslate"><span class="pre">tomoto</span></code> (Topic Modeling Tool) which is a Gibbs-sampling based topic model library written in C++.
The current version of <code class="docutils literal notranslate"><span class="pre">tomoto</span></code> supports several major topic models including</p>
<ul class="simple">
<li><p>Latent Dirichlet Allocation (<code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel</span></code>)</p></li>
<li><p>Labeled LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.LLDAModel</span></code>)</p></li>
<li><p>Partially Labeled LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.PLDAModel</span></code>)</p></li>
<li><p>Supervised LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.SLDAModel</span></code>)</p></li>
<li><p>Dirichlet Multinomial Regression (<code class="docutils literal notranslate"><span class="pre">tomotopy.DMRModel</span></code>)</p></li>
<li><p>Generalized Dirichlet Multinomial Regression (<code class="docutils literal notranslate"><span class="pre">tomotopy.GDMRModel</span></code>)</p></li>
<li><p>Hierarchical Dirichlet Process (<code class="docutils literal notranslate"><span class="pre">tomotopy.HDPModel</span></code>)</p></li>
<li><p>Hierarchical LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.HLDAModel</span></code>)</p></li>
<li><p>Multi Grain LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.MGLDAModel</span></code>)</p></li>
<li><p>Pachinko Allocation (<code class="docutils literal notranslate"><span class="pre">tomotopy.PAModel</span></code>)</p></li>
<li><p>Hierarchical PA (<code class="docutils literal notranslate"><span class="pre">tomotopy.HPAModel</span></code>)</p></li>
<li><p>Correlated Topic Model (<code class="docutils literal notranslate"><span class="pre">tomotopy.CTModel</span></code>)</p></li>
<li><p>Dynamic Topic Model (<code class="docutils literal notranslate"><span class="pre">tomotopy.DTModel</span></code>)</p></li>
<li><p>Pseudo-document based Topic Model (<code class="docutils literal notranslate"><span class="pre">tomotopy.PTModel</span></code>).</p></li>
</ul>
</section>
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">#</a></h2>
<p>You can install tomotopy easily using pip. (<a class="reference external" href="https://pypi.org/project/tomotopy/">https://pypi.org/project/tomotopy/</a>)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install --upgrade pip
pip install tomotopy
</pre></div>
</div>
<p>The supported OS and Python versions are:</p>
<ul class="simple">
<li><p>Linux (x86-64) with Python &gt;= 3.6</p></li>
<li><p>macOS &gt;= 10.13 with Python &gt;= 3.6</p></li>
<li><p>Windows 7 or later (x86, x86-64) with Python &gt;= 3.6</p></li>
<li><p>Other OS with Python &gt;= 3.6: Compilation from source code required (with c++14 compatible compiler)</p></li>
</ul>
<p>After installing, you can start tomotopy by just importing.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tp</span><span class="o">.</span><span class="n">isa</span><span class="p">)</span> <span class="c1"># prints &#39;avx2&#39;, &#39;avx&#39;, &#39;sse2&#39; or &#39;none&#39;</span>
</pre></div>
</div>
<p>Currently, tomotopy can exploits AVX2, AVX or SSE2 SIMD instruction set for maximizing performance.
When the package is imported, it will check available instruction sets and select the best option.
If <code class="docutils literal notranslate"><span class="pre">tp.isa</span></code> tells <code class="docutils literal notranslate"><span class="pre">none</span></code>, iterations of training may take a long time.
But, since most of modern Intel or AMD CPUs provide SIMD instruction set, the SIMD acceleration could show a big improvement.</p>
<p>Here is a sample code for simple LDA training of texts from ‘sample.txt’ file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;sample.txt&#39;</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{}</span><span class="se">\t</span><span class="s1">Log-likelihood: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top 10 words of topic #</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="n">mdl</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="performance-of-tomotopy">
<h2>Performance of tomotopy<a class="headerlink" href="#performance-of-tomotopy" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> uses Collapsed Gibbs-Sampling(CGS) to infer the distribution of topics and the distribution of words.
Generally CGS converges more slowly than Variational Bayes(VB) that <a class="reference external" href="https://radimrehurek.com/gensim/models/ldamodel.html"><code class="docutils literal notranslate"><span class="pre">gensim's</span> <span class="pre">LdaModel</span></code></a> uses, but its iteration can be computed much faster.
In addition, <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> can take advantage of multicore CPUs with a SIMD instruction set, which can result in faster iterations.</p>
<p>Following chart shows the comparison of LDA model’s running time between <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> and <code class="docutils literal notranslate"><span class="pre">gensim</span></code>.
The input data consists of 1000 random documents from English Wikipedia with 1,506,966 words (about 10.1 MB).
<code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> trains 200 iterations and <code class="docutils literal notranslate"><span class="pre">gensim</span></code> trains 10 iterations.</p>
<p><img alt="" src="../../../_images/tmt_i5.png" /></p>
</section>
<section id="model-save-and-load">
<h2>Model Save and Load<a class="headerlink" href="#model-save-and-load" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> provides <code class="docutils literal notranslate"><span class="pre">save</span></code> and <code class="docutils literal notranslate"><span class="pre">load</span></code> method for each topic model class,
so you can save the model into the file whenever you want, and re-load it from the file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>

<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">HDPModel</span><span class="p">()</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;sample.txt&#39;</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{}</span><span class="se">\t</span><span class="s1">Log-likelihood: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>

<span class="c1"># save into file</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;sample_hdp_model.bin&#39;</span><span class="p">)</span>

<span class="c1"># load from file</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">HDPModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;sample_hdp_model.bin&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">mdl</span><span class="o">.</span><span class="n">is_live_topic</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="k">continue</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top 10 words of topic #</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># the saved model is HDP model, </span>
<span class="c1"># so when you load it by LDA model, it will raise an exception</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;sample_hdp_model.bin&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>When you load the model from a file, a model type in the file should match the class of methods.</p>
</section>
<section id="documents-in-the-model-and-out-of-the-model">
<h2>Documents in the Model and out of the Model<a class="headerlink" href="#documents-in-the-model-and-out-of-the-model" title="Permalink to this headline">#</a></h2>
<p>We can use Topic Model for two major purposes.
The basic one is to discover topics from a set of documents as a result of trained model,
and the more advanced one is to infer topic distributions for unseen documents by using trained model.</p>
<p>We named the document in the former purpose (used for model training) as <strong>document in the model</strong>,
and the document in the later purpose (unseen document during training) as <strong>document out of the model</strong>.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code>, these two different kinds of document are generated differently.
A <strong>document in the model</strong> can be created by <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.add_doc</span></code> method.
<code class="docutils literal notranslate"><span class="pre">add_doc</span></code> can be called before <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.train</span></code> starts.
In other words, after <code class="docutils literal notranslate"><span class="pre">train</span></code> called, <code class="docutils literal notranslate"><span class="pre">add_doc</span></code> cannot add a document into the model because the set of document used for training has become fixed.</p>
<p>To acquire the instance of the created document, you should use <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.docs</span></code> like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span> <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Failed to add doc&quot;</span><span class="p">)</span>
<span class="n">doc_inst</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="c1"># doc_inst is an instance of the added document</span>
</pre></div>
</div>
<p>A <strong>document out of the model</strong> is generated by <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.make_doc</span></code> method. <code class="docutils literal notranslate"><span class="pre">make_doc</span></code> can be called only after <code class="docutils literal notranslate"><span class="pre">train</span></code> starts.
If you use <code class="docutils literal notranslate"><span class="pre">make_doc</span></code> before the set of document used for training has become fixed, you may get wrong results.
Since <code class="docutils literal notranslate"><span class="pre">make_doc</span></code> returns the instance directly, you can use its return value for other manipulations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># add_doc ...</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">doc_inst</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">make_doc</span><span class="p">(</span><span class="n">unseen_doc</span><span class="p">)</span> <span class="c1"># doc_inst is an instance of the unseen document</span>
</pre></div>
</div>
</section>
<section id="inference-for-unseen-documents">
<h2>Inference for Unseen Documents<a class="headerlink" href="#inference-for-unseen-documents" title="Permalink to this headline">#</a></h2>
<p>If a new document is created by <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.make_doc</span></code>, its topic distribution can be inferred by the model.
Inference for unseen document should be performed using <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.infer</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># add_doc ...</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">doc_inst</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">make_doc</span><span class="p">(</span><span class="n">unseen_doc</span><span class="p">)</span>
<span class="n">topic_dist</span><span class="p">,</span> <span class="n">ll</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">doc_inst</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Topic Distribution for Unseen Docs: &quot;</span><span class="p">,</span> <span class="n">topic_dist</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Log-likelihood of inference: &quot;</span><span class="p">,</span> <span class="n">ll</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">infer</span></code> method can infer only one instance of <code class="docutils literal notranslate"><span class="pre">tomotopy.Document</span></code> or a <code class="docutils literal notranslate"><span class="pre">list</span></code> of instances of <code class="docutils literal notranslate"><span class="pre">tomotopy.Document</span></code>.
See more at <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.infer</span></code>.</p>
</section>
<section id="corpus-and-transform">
<h2>Corpus and transform<a class="headerlink" href="#corpus-and-transform" title="Permalink to this headline">#</a></h2>
<p>Every topic model in <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> has its own internal document type.
A document can be created and added into suitable for each model through each model’s <code class="docutils literal notranslate"><span class="pre">add_doc</span></code> method.
However, trying to add the same list of documents to different models becomes quite inconvenient,
because <code class="docutils literal notranslate"><span class="pre">add_doc</span></code> should be called for the same list of documents to each different model.
Thus, <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> provides <code class="docutils literal notranslate"><span class="pre">tomotopy.utils.Corpus</span></code> class that holds a list of documents.
<code class="docutils literal notranslate"><span class="pre">tomotopy.utils.Corpus</span></code> can be inserted into any model by passing as argument <code class="docutils literal notranslate"><span class="pre">corpus</span></code> to <code class="docutils literal notranslate"><span class="pre">__init__</span></code> or <code class="docutils literal notranslate"><span class="pre">add_corpus</span></code> method of each model.
So, inserting <code class="docutils literal notranslate"><span class="pre">tomotopy.utils.Corpus</span></code> just has the same effect to inserting documents the corpus holds.</p>
<p>Some topic models requires different data for its documents.
For example, <code class="docutils literal notranslate"><span class="pre">tomotopy.DMRModel</span></code> requires argument <code class="docutils literal notranslate"><span class="pre">metadata</span></code> in <code class="docutils literal notranslate"><span class="pre">str</span></code> type,
but <code class="docutils literal notranslate"><span class="pre">tomotopy.PLDAModel</span></code> requires argument <code class="docutils literal notranslate"><span class="pre">labels</span></code> in <code class="docutils literal notranslate"><span class="pre">List[str]</span></code> type.
Since <code class="docutils literal notranslate"><span class="pre">tomotopy.utils.Corpus</span></code> holds an independent set of documents rather than being tied to a specific topic model,
data types required by a topic model may be inconsistent when a corpus is added into that topic model.
In this case, miscellaneous data can be transformed to be fitted target topic model using argument <code class="docutils literal notranslate"><span class="pre">transform</span></code>.</p>
<p>See more details in the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tomotopy</span> <span class="kn">import</span> <span class="n">DMRModel</span>
<span class="kn">from</span> <span class="nn">tomotopy.utils</span> <span class="kn">import</span> <span class="n">Corpus</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">Corpus</span><span class="p">()</span>
<span class="n">corpus</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="s2">&quot;a b c d e&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">a_data</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">corpus</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="s2">&quot;e f g h i&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">a_data</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">corpus</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="s2">&quot;i j k l m&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">a_data</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DMRModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span> 
<span class="c1"># You lose `a_data` field in `corpus`, </span>
<span class="c1"># and `metadata` that `DMRModel` requires is filled with the default value, empty str.</span>

<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span>

<span class="k">def</span> <span class="nf">transform_a_data_to_metadata</span><span class="p">(</span><span class="n">misc</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;metadata&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">misc</span><span class="p">[</span><span class="s1">&#39;a_data&#39;</span><span class="p">])}</span>
<span class="c1"># this function transforms `a_data` to `metadata`</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DMRModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_a_data_to_metadata</span><span class="p">)</span>
<span class="c1"># Now docs in `model` has non-default `metadata`, that generated from `a_data` field.</span>

<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;2&#39;</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;3&#39;</span>
</pre></div>
</div>
</section>
<section id="parallel-sampling-algorithms">
<h2>Parallel Sampling Algorithms<a class="headerlink" href="#parallel-sampling-algorithms" title="Permalink to this headline">#</a></h2>
<p>Since version 0.5.0, <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> allows you to choose a parallelism algorithm.
The algorithm provided in versions prior to 0.4.2 is <code class="docutils literal notranslate"><span class="pre">COPY_MERGE</span></code>, which is provided for all topic models.
The new algorithm <code class="docutils literal notranslate"><span class="pre">PARTITION</span></code>, available since 0.5.0, makes training generally faster and more memory-efficient, but it is available at not all topic models.</p>
<p>The following chart shows the speed difference between the two algorithms based on the number of topics.</p>
<p><img alt="" src="../../../_images/algo_comp.png" /></p>
</section>
<section id="pining-topics-using-word-priors">
<h2>Pining Topics using Word Priors<a class="headerlink" href="#pining-topics-using-word-priors" title="Permalink to this headline">#</a></h2>
<p>Since version 0.6.0, a new method <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.set_word_prior</span></code> has been added. It allows you to control word prior for each topic.
For example, we can set the weight of the word ‘church’ to 1.0 in topic 0, and the weight to 0.1 in the rest of the topics by following codes.
This means that the probability that the word ‘church’ is assigned to topic 0 is 10 times higher than the probability of being assigned to another topic.
Therefore, most of ‘church’ is assigned to topic 0, so topic 0 contains many words related to ‘church’.
This allows to manipulate some topics to be placed at a specific topic number.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># add documents into `mdl`</span>

<span class="c1"># setting word prior</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">set_word_prior</span><span class="p">(</span><span class="s1">&#39;church&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)])</span>
</pre></div>
</div>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">#</a></h2>
<section id="install-or-upgrade-of-ekorpkit">
<h3>Install or upgrade of ekorpkit<a class="headerlink" href="#install-or-upgrade-of-ekorpkit" title="Permalink to this headline">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Install ekorpkit package first.</p>
<p>Set logging level to Warning, if you don’t want to see verbose logging.</p>
<p>If you run this notebook in Colab, set Hardware accelerator to GPU.</p>
</div>
<div class="toggle docutils container">
<p>!pip install -U –pre ekorpkit[topic]</p>
<p>exit()</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ekorpkit</span> <span class="kn">import</span> <span class="n">eKonf</span>

<span class="n">eKonf</span><span class="o">.</span><span class="n">setLogger</span><span class="p">(</span><span class="s2">&quot;WARNING&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;version:&quot;</span><span class="p">,</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;is notebook?&quot;</span><span class="p">,</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">is_notebook</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;is colab?&quot;</span><span class="p">,</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">is_colab</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;environment variables:&quot;</span><span class="p">)</span>
<span class="n">eKonf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">eKonf</span><span class="o">.</span><span class="n">env</span><span class="p">()</span><span class="o">.</span><span class="n">dict</span><span class="p">())</span>

<span class="n">data_dir</span> <span class="o">=</span> <span class="s2">&quot;../data/topic_models&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>version: 0.1.39+5.g6b58da9
is notebook? True
is colab? False
environment variables:
{&#39;CUDA_DEVICE_ORDER&#39;: None,
 &#39;CUDA_VISIBLE_DEVICES&#39;: None,
 &#39;EKORPKIT_CONFIG_DIR&#39;: &#39;/workspace/projects/ekorpkit-book/config&#39;,
 &#39;EKORPKIT_DATA_DIR&#39;: None,
 &#39;EKORPKIT_LOG_LEVEL&#39;: &#39;WARNING&#39;,
 &#39;EKORPKIT_PROJECT&#39;: &#39;ekorpkit-book&#39;,
 &#39;EKORPKIT_WORKSPACE_ROOT&#39;: &#39;/workspace&#39;,
 &#39;KMP_DUPLICATE_LIB_OK&#39;: &#39;TRUE&#39;,
 &#39;NUM_WORKERS&#39;: 230}
</pre></div>
</div>
</div>
</div>
</section>
<section id="load-a-dataset">
<h3>Load a dataset<a class="headerlink" href="#load-a-dataset" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cfg</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="s1">&#39;path&#39;</span><span class="p">)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">uri</span> <span class="o">=</span> <span class="s1">&#39;https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/us_equities_news_sampled.zip&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;us_equities_news_sampled.parquet&quot;</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">cached_path</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>split</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>251155</td>
      <td>Investing com Asian stock markets were broadly...</td>
      <td>train</td>
    </tr>
    <tr>
      <th>1</th>
      <td>270611</td>
      <td>Solid execution product diversity and strong b...</td>
      <td>train</td>
    </tr>
    <tr>
      <th>2</th>
      <td>237917</td>
      <td>Chip name Micron Technology Inc NASDAQ MU is h...</td>
      <td>train</td>
    </tr>
    <tr>
      <th>3</th>
      <td>406989</td>
      <td>June is typically a boring month for gold and ...</td>
      <td>train</td>
    </tr>
    <tr>
      <th>4</th>
      <td>231535</td>
      <td>A prudent investment decision involves buying ...</td>
      <td>train</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="lda-basics">
<h3>LDA Basics<a class="headerlink" href="#lda-basics" title="Permalink to this headline">#</a></h3>
<p>LDA class provides Latent Dirichlet Allocation(LDA) topic model and its implementation is based on following papers:</p>
<ul class="simple">
<li><p>Blei, D.M., Ng, A.Y., &amp;Jordan, M.I. (2003).Latent dirichlet allocation.Journal of machine Learning research, 3(Jan), 993 - 1022.</p></li>
<li><p>Newman, D., Asuncion, A., Smyth, P., &amp;Welling, M. (2009).Distributed algorithms for topic models.Journal of Machine Learning Research, 10(Aug), 1801 - 1828.</p></li>
</ul>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>

<span class="n">save_path</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">join_path</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s2">&quot;lda_basic.mdl&quot;</span><span class="p">)</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">tw</span><span class="o">=</span><span class="n">tp</span><span class="o">.</span><span class="n">TermWeight</span><span class="o">.</span><span class="n">ONE</span><span class="p">,</span> <span class="n">min_cf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rm_top</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">][:</span><span class="mi">1000</span><span class="p">]):</span>
    <span class="n">ch</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">ch</span><span class="p">)</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">burn_in</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Num docs:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">),</span> <span class="s1">&#39;, Vocab size:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">),</span> <span class="s1">&#39;, Num words:&#39;</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">num_words</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Removed top words:&#39;</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">removed_top_words</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{}</span><span class="se">\t</span><span class="s1">Log-likelihood: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>

<span class="n">mdl</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Num docs: 1000 , Vocab size: 11716 , Num words: 468615
Removed top words: [&#39;the&#39;, &#39;to&#39;, &#39;of&#39;, &#39;and&#39;, &#39;in&#39;]
Iteration: 0	Log-likelihood: -8.149834490154362
Iteration: 100	Log-likelihood: -8.08736953477213
Iteration: 200	Log-likelihood: -8.051370628337063
Iteration: 300	Log-likelihood: -8.025257625400224
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Topic #</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #0
		earnings	0.03124929964542389
		a	0.028039269149303436
		Zacks	0.02219722606241703
		is	0.020621005445718765
		quarter	0.018191618844866753
		for	0.017067573964595795
		Earnings	0.015026379376649857
		ESP	0.014128423295915127
		an	0.013452290557324886
		that	0.012859341688454151
Topic #1
		a	0.026198897510766983
		said	0.025242939591407776
		s	0.019541189074516296
		its	0.015248588286340237
		for	0.011897781863808632
		on	0.011563551612198353
		it	0.010953155346214771
		percent	0.00960773415863514
		as	0.008685766719281673
		company	0.008452088572084904
Topic #2
		a	0.030532576143741608
		is	0.016013480722904205
		The	0.015097678638994694
		on	0.012399696744978428
		for	0.011318178847432137
		at	0.010512854903936386
		it	0.008428314700722694
		day	0.007102582603693008
		Tesla	0.007090953178703785
		stock	0.006995012052357197
Topic #3
		a	0.01793372444808483
		is	0.016452036798000336
		for	0.012529638595879078
		its	0.012028549797832966
		s	0.011740843765437603
		with	0.010846556164324284
		technology	0.009976244531571865
		NASDAQ	0.009379253722727299
		will	0.009110728278756142
		The	0.008410641923546791
Topic #4
		S	0.019718732684850693
		on	0.019661812111735344
		a	0.01943672075867653
		0	0.017082324251532555
		1	0.016551939770579338
		percent	0.014398054219782352
		s	0.013101842254400253
		The	0.012660715728998184
		U	0.01251065544784069
		after	0.010465435683727264
Topic #5
		a	0.028191950172185898
		for	0.021664651110768318
		is	0.014714432880282402
		with	0.011219375766813755
		options	0.010716661810874939
		his	0.008673887699842453
		that	0.008546214550733566
		volatility	0.007253522053360939
		on	0.007054032292217016
		as	0.006343849468976259
Topic #6
		0	0.03876658156514168
		at	0.02934960648417473
		or	0.02234751358628273
		1	0.021844562143087387
		was	0.01921565644443035
		which	0.015937551856040955
		NASDAQ	0.014646284282207489
		The	0.01412549801170826
		on	0.014075559563934803
		Inc	0.013850836083292961
Topic #7
		a	0.026682842522859573
		said	0.01741178147494793
		on	0.015312546864151955
		that	0.013975335285067558
		s	0.012738214805722237
		The	0.011103400960564613
		for	0.010098491795361042
		was	0.00858111772686243
		by	0.008440990000963211
		U	0.008267499506473541
Topic #8
		year	0.03777458146214485
		quarter	0.02896769717335701
		million	0.028481706976890564
		1	0.01698167622089386
		from	0.015365163795650005
		2	0.012678230181336403
		billion	0.012417101301252842
		a	0.012012973427772522
		3	0.011970488354563713
		over	0.011747700162231922
Topic #9
		is	0.026326481252908707
		or	0.02442336454987526
		a	0.022607386112213135
		Zacks	0.020143095403909683
		for	0.015291192568838596
		stocks	0.014799728989601135
		that	0.013360192067921162
		are	0.012704906985163689
		investment	0.011826545000076294
		Research	0.010195302776992321
Topic #10
		a	0.026791885495185852
		that	0.023886676877737045
		is	0.020946161821484566
		it	0.012942749075591564
		for	0.011327537707984447
		we	0.011231127195060253
		s	0.011211437173187733
		are	0.011083795689046383
		I	0.010176045820116997
		be	0.009823672473430634
Topic #11
		a	0.020864764228463173
		for	0.020766371861100197
		with	0.015346375294029713
		is	0.013524716719985008
		s	0.011478163301944733
		The	0.010997447185218334
		from	0.007882636971771717
		on	0.007497502025216818
		that	0.006727233063429594
		Boeing	0.006626029498875141
Topic #12
		Apple	0.02030530944466591
		a	0.01906707137823105
		s	0.017792053520679474
		NASDAQ	0.015772251412272453
		its	0.01572321355342865
		is	0.013062838464975357
		for	0.012008496560156345
		on	0.01025841198861599
		company	0.009939656592905521
		The	0.009550408460199833
Topic #13
		s	0.021421290934085846
		sales	0.01631803810596466
		its	0.01597566343843937
		a	0.015387967228889465
		company	0.014013081789016724
		on	0.0105489082634449
		is	0.010017824359238148
		has	0.0078099193051457405
		for	0.007408237084746361
		The	0.0072761401534080505
Topic #14
		a	0.023136673495173454
		is	0.012710190378129482
		The	0.012222971767187119
		dividend	0.010886242613196373
		has	0.010031736455857754
		fund	0.008607557974755764
		as	0.008360200561583042
		with	0.008185301907360554
		its	0.007902964949607849
		ETF	0.007413247134536505
Topic #15
		a	0.0332772321999073
		is	0.022882360965013504
		has	0.016356931999325752
		for	0.01582656055688858
		stock	0.015669718384742737
		this	0.012685603462159634
		that	0.012254289351403713
		s	0.012019027024507523
		Zacks	0.011170845478773117
		earnings	0.010324726812541485
Topic #16
		Zacks	0.022936400026082993
		a	0.020220259204506874
		has	0.016317356377840042
		company	0.016230208799242973
		s	0.013876333832740784
		The	0.013500397093594074
		for	0.01269469689577818
		Rank	0.01241616252809763
		is	0.010008460842072964
		growth	0.009430031292140484
Topic #17
		oil	0.019801318645477295
		a	0.018382295966148376
		is	0.01412523165345192
		for	0.012624233961105347
		The	0.011699890717864037
		production	0.010812296532094479
		gas	0.010303483344614506
		Energy	0.010145186446607113
		energy	0.008825100027024746
		from	0.008525466546416283
Topic #18
		a	0.019231945276260376
		China	0.018903110176324844
		S	0.015108540654182434
		U	0.013544078916311264
		for	0.011517252773046494
		s	0.01038326695561409
		is	0.009811291471123695
		on	0.00963989831507206
		as	0.00934892799705267
		The	0.008886564522981644
Topic #19
		a	0.02060071751475334
		The	0.01461031660437584
		is	0.01301425788551569
		for	0.011943803168833256
		on	0.011200555600225925
		s	0.010091605596244335
		as	0.009640030562877655
		1	0.009573404677212238
		at	0.008796103298664093
		that	0.008279383182525635
</pre></div>
</div>
</div>
</div>
</section>
<section id="lda-visualization">
<h3>LDA Visualization<a class="headerlink" href="#lda-visualization" title="Permalink to this headline">#</a></h3>
<p>This example shows how to perform a Latent Dirichlet Allocation using tomotopy and visualize the result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pyLDAvis</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">porter_stemmer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">PorterStemmer</span><span class="p">()</span><span class="o">.</span><span class="n">stem</span>
<span class="n">english_stops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">porter_stemmer</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
<span class="n">pat</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;^[a-z]{2,}$&#39;</span><span class="p">)</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">Corpus</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tp</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">SimpleTokenizer</span><span class="p">(</span><span class="n">porter_stemmer</span><span class="p">),</span> 
    <span class="n">stopwords</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">english_stops</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">pat</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">corpus</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
<span class="c1"># save preprocessed corpus for reuse</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">join_path</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s2">&quot;preprocessed_corpus.cps&quot;</span><span class="p">)</span>
<span class="n">corpus</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">rm_top</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">corpus</span><span class="o">=</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Num docs:</span><span class="si">{}</span><span class="s1">, Num Vocabs:</span><span class="si">{}</span><span class="s1">, Total Words:</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">),</span> <span class="n">mdl</span><span class="o">.</span><span class="n">num_words</span>
<span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Removed Top words: &#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">mdl</span><span class="o">.</span><span class="n">removed_top_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Num docs:22098, Num Vocabs:20231, Total Words:5861553
Removed Top words:  year compani stock earn zack quarter market share expect million report said nyse estim growth price billion rank also trade nasdaq revenu invest investor new inc last rate industri like sale buy per increas month time current one consensu percent
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s train the model</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{:04}</span><span class="s1">, LL per word: </span><span class="si">{:.4}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{:04}</span><span class="s1">, LL per word: </span><span class="si">{:.4}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>

<span class="n">mdl</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0000, LL per word: -11.76
Iteration: 0020, LL per word: -8.53
Iteration: 0040, LL per word: -8.273
Iteration: 0060, LL per word: -8.196
Iteration: 0080, LL per word: -8.154
Iteration: 0100, LL per word: -8.133
Iteration: 0120, LL per word: -8.117
Iteration: 0140, LL per word: -8.105
Iteration: 0160, LL per word: -8.097
Iteration: 0180, LL per word: -8.091
Iteration: 0200, LL per word: -8.084
Iteration: 0220, LL per word: -8.079
Iteration: 0240, LL per word: -8.075
Iteration: 0260, LL per word: -8.074
Iteration: 0280, LL per word: -8.071
Iteration: 0300, LL per word: -8.067
Iteration: 0320, LL per word: -8.066
Iteration: 0340, LL per word: -8.061
Iteration: 0360, LL per word: -8.06
Iteration: 0380, LL per word: -8.059
Iteration: 0400, LL per word: -8.055
Iteration: 0420, LL per word: -8.054
Iteration: 0440, LL per word: -8.053
Iteration: 0460, LL per word: -8.053
Iteration: 0480, LL per word: -8.052
Iteration: 0500, LL per word: -8.051
Iteration: 0520, LL per word: -8.049
Iteration: 0540, LL per word: -8.049
Iteration: 0560, LL per word: -8.048
Iteration: 0580, LL per word: -8.048
Iteration: 0600, LL per word: -8.05
Iteration: 0620, LL per word: -8.046
Iteration: 0640, LL per word: -8.048
Iteration: 0660, LL per word: -8.048
Iteration: 0680, LL per word: -8.046
Iteration: 0700, LL per word: -8.048
Iteration: 0720, LL per word: -8.048
Iteration: 0740, LL per word: -8.048
Iteration: 0760, LL per word: -8.047
Iteration: 0780, LL per word: -8.047
Iteration: 0800, LL per word: -8.048
Iteration: 0820, LL per word: -8.047
Iteration: 0840, LL per word: -8.048
Iteration: 0860, LL per word: -8.047
Iteration: 0880, LL per word: -8.047
Iteration: 0900, LL per word: -8.048
Iteration: 0920, LL per word: -8.047
Iteration: 0940, LL per word: -8.047
Iteration: 0960, LL per word: -8.045
Iteration: 0980, LL per word: -8.046
Iteration: 1000, LL per word: -8.046
&lt;Basic Info&gt;
| LDAModel (current version: 0.12.3)
| 22098 docs, 5861553 words
| Total Vocabs: 71297, Used Vocabs: 20231
| Entropy of words: 7.90829
| Entropy of term-weighted words: 7.90829
| Removed Vocabs: year compani stock earn zack quarter market share expect million report said nyse estim growth price billion rank also trade nasdaq revenu invest investor new inc last rate industri like sale buy per increas month time current one consensu percent
|
&lt;Training Info&gt;
| Iterations: 1000, Burn-in steps: 0
| Optimization Interval: 10
| Log-likelihood per word: -8.04616
|
&lt;Initial Parameters&gt;
| tw: TermWeight.ONE
| min_cf: 0 (minimum collection frequency of words)
| min_df: 5 (minimum document frequency of words)
| rm_top: 40 (the number of top words to be removed)
| k: 30 (the number of topics between 1 ~ 32767)
| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)
| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)
| seed: 1845346494 (random seed)
| trained in version 0.12.3
|
&lt;Parameters&gt;
| alpha (Dirichlet prior on the per-document topic distributions)
|  [0.0305041  0.03837155 0.03806888 0.15033804 0.03779588 0.02238496
|   0.04674659 0.09717994 0.06488691 0.06417299 0.02571286 0.06602185
|   0.06956474 0.02840271 0.03985003 0.07475647 0.06216731 0.08010441
|   0.21915881 0.03911861 0.01526014 0.02923114 0.11285637 0.10376224
|   0.052956   0.092475   0.03947552 0.04790112 0.02535166 0.04443888]
| eta (Dirichlet prior on the per-topic word distribution)
|  0.01
|
&lt;Topics&gt;
| #0 (106794) : ep revis beat surpris esp
| #1 (104946) : game point first two season
| #2 (135861) : oil energi ga product barrel
| #3 (468133) : gain strong term past posit
| #4 (93291) : product food brand consum may
| #5 (124404) : gold mine metal silver properti
| #6 (137225) : dividend yield cash insur ratio
| #7 (269291) : appl technolog servic googl data
| #8 (149044) : bank financi loan credit goldman
| #9 (157560) : china chines tariff countri global
| #10 (78979) : boe air aircraft defens jet
| #11 (201906) : court reuter investig case use
| #12 (259548) : index gain futur dow fell
| #13 (107755) : fund etf sector asset index
| #14 (97522) : energi airlin power solar electr
| #15 (239181) : trump presid would tax state
| #16 (147627) : retail store amazon brand onlin
| #17 (400553) : fed economi econom data bank
| #18 (666510) : get go even look make
| #19 (143296) : valu score ratio look past
| #20 (67007) : point close rose fell perform
| #21 (149661) : drug patient treatment studi approv
| #22 (273527) : deal reuter would busi execut
| #23 (238765) : analyst profit reuter forecast fell
| #24 (195379) : day week move chart higher
| #25 (320151) : oper cent adjust ago net
| #26 (93764) : health medic healthcar care system
| #27 (163422) : esp surpris beat result posit
| #28 (149443) : research secur inform recommend may
| #29 (121008) : car vehicl tesla motor gm
|
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">topic_term_dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_word_dist</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">)])</span>
<span class="n">doc_topic_dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">get_topic_dist</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">])</span>
<span class="n">doc_topic_dists</span> <span class="o">/=</span> <span class="n">doc_topic_dists</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">doc_lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">])</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">)</span>
<span class="n">term_frequency</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">used_vocab_freq</span>

<span class="n">prepared_data</span> <span class="o">=</span> <span class="n">pyLDAvis</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
    <span class="n">topic_term_dists</span><span class="p">,</span> 
    <span class="n">doc_topic_dists</span><span class="p">,</span> 
    <span class="n">doc_lengths</span><span class="p">,</span> 
    <span class="n">vocab</span><span class="p">,</span> 
    <span class="n">term_frequency</span><span class="p">,</span>
    <span class="n">start_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># tomotopy starts topic ids with 0, pyLDAvis with 1</span>
    <span class="n">sort_topics</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># IMPORTANT: otherwise the topic_ids between pyLDAvis and tomotopy are not matching!</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pyLDAvis</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">save_path</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">join_path</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s2">&quot;lda_basic.html&quot;</span><span class="p">)</span>
<span class="n">pyLDAvis</span><span class="o">.</span><span class="n">save_html</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>

<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&lt;a href=</span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2"> target=&#39;_blank&#39;&gt;show results&lt;/a&gt;&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><a href=../data/topic_models/lda_basic.html target='_blank'>show results</a></div></div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/lectures/intro_nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="topic.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Topic Modeling</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../deep_nlp/index.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Deep Learning for NLP</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Young Joon Lee<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>