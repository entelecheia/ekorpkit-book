{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899602f2-b9ee-458f-bb12-8da95bfb76d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# DALL·E 2\n",
    "\n",
    "## CLIP\n",
    "\n",
    "- Paper: https://arxiv.org/abs/2103.00020\n",
    "- Blog post: https://openai.com/blog/clip/\n",
    "- Code: https://github.com/openai/CLIP (does not cover the training part)\n",
    "- Models: Available (on Apr 22, 2022, the last and the best ViT-L/14@336px model was published)\n",
    "- Alternative code: https://github.com/mlfoundations/open_clip (with training)\n",
    "- Alternative models (Multilingual): https://github.com/FreddeFrallan/Multilingual-CLIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d145cd0-2c51-4d1a-add2-1afee681c21a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- CLIP was originally a separate auxiliary model to rank the results from DALL·E. \n",
    "- An abbreviation for Contrastive Language-Image Pre-Training.\n",
    "- Take a large dataset of image-text pairs scraped from the Internet (400M such pairs). \n",
    "- Then train a contrastive model on such a dataset. \n",
    "- Contrastive models produce high scores (similarity) for an image and a text from the same pair (so they are similar) and a low score for mismatched texts and images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c974d-81c7-40a6-9f42-f38bed950931",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP: technical details\n",
    "\n",
    "- The model consists of two encoders: one for a text and another one for an image. \n",
    "- Encoders produce embeddings (a multidimensional vector representation of an object, say 512 bytes for each). \n",
    "- Then a dot product is calculated with two embeddings, and it results in a similarity score. \n",
    "- Embeddings are normalized, so this procedure outputs cosine similarity. \n",
    "- It is close to 1 for vectors pointing in the same direction (and consequently a small angle between them), 0 for orthogonal vectors, and -1 for opposite ones.\n",
    "- The model is trained to maximize the similarity score for the image-text pairs from the same dataset and minimize it for mismatched pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fc5e6-edd6-44a8-b452-c05786223e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Contrastive pre-training \n",
    "\n",
    "![objects](../figs/aiart/dalle2/contrastive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f62cb7-532a-4907-8d85-53134ec0135e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP image encoders\n",
    "\n",
    "- There are nine image encoders, five convolutional, and four transformer ones. \n",
    "- Convolutional encoders are ResNet-50, ResNet-101 and EfficientNet-like models called RN50x4, RN50x16, RN50x64 (the higher numbers, the better the model). \n",
    "- Transformer encoders are Vision Transformers (or ViT): ViT-B/32, ViT-B/16, ViT-L/14, and ViT-L/14@336. \n",
    "- The last one was fine-tuned on images with a resolution of 336×336 pixels, and others were trained on 224×224 pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a6be5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP text encoders\n",
    "\n",
    "- The text encoder is an ordinary transformer encoder but with masked attention. \n",
    "- It consists of 12 layers with 8 attention heads each, with 63M parameters in total. \n",
    "- The attention span is only 76 tokens (compared to the GPT-3 with 2048 tokens and a standard BERT with 512 tokens). \n",
    "- The text part of the model is suitable for pretty short texts only, and you can’t put a large paragraph into the model. \n",
    "- As DALL·E 2 uses mostly the same CLIP, it should have the same limitation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb82a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP applications\n",
    "\n",
    "- You can use such a model for ranking text-image pairs as was done in DALL·E to score multiple results and choose the best one. \n",
    "- You can use the CLIP features to train your custom classifiers on top of it.\n",
    "- You can use CLIP for zero-shot classification (when you didn't specifically train the model to work with these classes) with any number of classes. The classes can be adjustable without retraining a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d3aa10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "###  Zero-shot classification with CLIP\n",
    "\n",
    "![](../figs/aiart/dalle2/clip-zeroshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd22977",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP prompt engineering: CLIPDraw\n",
    "\n",
    "- https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb\n",
    "- https://colab.research.google.com/github/pschaldenbrand/StyleCLIPDraw/blob/master/Style_ClipDraw.ipynb\n",
    "\n",
    "![](../figs/aiart/dalle2/clipdraw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0ced0",
   "metadata": {},
   "source": [
    "**CLIPDraw generation procedure**\n",
    "\n",
    "![](../figs/aiart/dalle2/clipdraw-procedure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eddd800",
   "metadata": {},
   "source": [
    "### CLIP prompt engineering: VQGAN-CLIP\n",
    "\n",
    "- https://github.com/nerdyrodent/VQGAN-CLIP\n",
    "- https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP(Updated).ipynb\n",
    "\n",
    "![](../figs/aiart/dalle2/vqgan-clip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e3be8c",
   "metadata": {},
   "source": [
    "**VQGAN-CLIP generation procedure**\n",
    "\n",
    "![](../figs/aiart/dalle2/vqgan-clip-procedure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d98fc",
   "metadata": {},
   "source": [
    "## GLIDE\n",
    "\n",
    "- Paper: https://arxiv.org/abs/2112.10741\n",
    "- Blog post: Strangely enough, OpenAI didn’t make a post on it\n",
    "- Code: https://github.com/openai/glide-text2im\n",
    "- Models: Available, but only a small model (300M instead of 3.5B parameters) trained on a filtered dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f374d",
   "metadata": {},
   "source": [
    "- GLIDE, which stands for `G`uided `L`anguage to `I`mage `D`iffusion for Generation and `E`diting, is a text-guided image generation model by OpenAI.\n",
    "- GLIDE generates images with a 256×256 pixel resolution.\n",
    "- GLIDE model with 3.5B parameters (but it seems the correct number is 5B parameters as there is a separate upsampling model with 1.5B parameters) is favored over 12B parameters DALL·E by human evaluators and also has beaten DALL·E by FID score.\n",
    "- GLIDE models can also be fine-tuned to perform image inpainting, enabling powerful text-driven image editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f78639",
   "metadata": {},
   "source": [
    "### GLIDE Examples\n",
    "\n",
    "![](../figs/aiart/dalle2/glide-examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5b044",
   "metadata": {},
   "source": [
    "### Text-conditional image inpainting\n",
    "\n",
    "\n",
    "![](../figs/aiart/dalle2/glide-inpainting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97a921",
   "metadata": {},
   "source": [
    "### Diffusion model\n",
    "\n",
    "- GLIDE resembles another kind of model called the diffusion model. \n",
    "- Diffusion models add random noise to input data through the chain of diffusion steps, and then they learn to reverse the diffusion process to construct images from the noise.\n",
    "\n",
    "![](../figs/aiart/dalle2/diffusion-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec8437",
   "metadata": {},
   "source": [
    "**Illustration of the process used to generate a new image with the diffusion model**\n",
    "\n",
    "![](../figs/aiart/dalle2/diffusion.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db94b8",
   "metadata": {},
   "source": [
    "**Diffusion models compared to the other classes of generative models**\n",
    "\n",
    "![](../figs/aiart/dalle2/diffusion-compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5c600",
   "metadata": {},
   "source": [
    "#### GLIDE technical details\n",
    "\n",
    "- First, the authors trained a 3.5B parameter diffusion model that uses a text encoder to condition on natural language descriptions. \n",
    "- Next, they compared two techniques for guiding diffusion models toward text prompts: CLIP guidance and classifier-free guidance.\n",
    "- Classifier guidance allows diffusion models to condition on a classifier’s labels, and gradients from the classifier are used to guide the sample towards the label.\n",
    "- The classifier-free guidance does not require a separate classifier model to be trained. is a form of guidance that interpolates between predictions from a diffusion model with and without labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27a84b",
   "metadata": {},
   "source": [
    "- Classifier-free guidance has two appealing properties\n",
    "- First, it allows a single model to leverage its own knowledge during guidance, rather than relying on the knowledge of a separate (and sometimes smaller) classification model. \n",
    "- Second, it simplifies guidance when conditioning on information that is difficult to predict with a classifier (such as text).\n",
    "- With CLIP guidance the classifier is replaced with a CLIP model. It uses the gradient of the dot product of the image and caption encodings with respect to the image.\n",
    "- The text-conditioned diffusion model is an augmented ADM model architecture that predicts an image for the next diffusion step based on a noised image xₜ and corresponding text caption c.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa022377",
   "metadata": {},
   "source": [
    "**GLIDE technical details: visual part**\n",
    "\n",
    "- The visual part is a modified U-Net architecture. \n",
    "- The U-Net model uses a stack of residual layers and down-sampling convolutions, followed by a stack of residual layers with up-sampling convolutions, with skip connections connecting the layers with the same spatial size.\n",
    "- There are different modifications to the original U-Net architecture regarding width, depth, and so on. \n",
    "- Global attention layers with several attention heads are added at the 8×8, 16×16, and 32×32 resolutions. \n",
    "- Also, a projection of the timestep embedding was added to each residual block..\n",
    "- For the classifier guidance, a classifier architecture is the down-sampling trunk of the U-Net model with an attention pool at the 8×8 layer to produce the final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28986026",
   "metadata": {},
   "source": [
    "**The original U-Net architecture**\n",
    "\n",
    "![](../figs/aiart/dalle2/u-net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ba26c",
   "metadata": {},
   "source": [
    "**GLIDE technical details: text part**\n",
    "\n",
    "- The text is encoded into a sequence of K (the maximum attention span is unclear) tokens that passed through a transformer model.\n",
    "- The output of this transformer is used in two ways: \n",
    "  - first, the final token embedding is used in place of a class embedding in the ADM model; \n",
    "  - second,the last layer of token embeddings (a sequence of K feature vectors) is separately projected to the dimensionality of each attention layer throughout the ADM model and then concatenated to the attention context at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21376be1",
   "metadata": {},
   "source": [
    "- The text transformer has 24 residual blocks of width 2048, resulting in roughly 1.2B parameters. \n",
    "- The visual part of the model trained for 64×64 resolution consists of 2.3B parameters. \n",
    "- In addition to the 3.5B parameters text-conditional diffusion model, the authors trained another 1.5B parameters text-conditional upsampling diffusion model to increase the resolution to 256×256 (this idea will be used in DALL·E as well). \n",
    "- The upsampling model is conditioned on text in the same way as the base model but uses a smaller text encoder with a width of 1024 instead of 2048. \n",
    "- For CLIP guidance, they also trained a noised 64×64 ViT-L CLIP model.\n",
    "- GLIDE was trained on the same dataset as DALL·E and the total training compute is roughly equal to that used to train DALL·E.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25caa6",
   "metadata": {},
   "source": [
    "**GLIDE is preferred by the human evaluators**\n",
    "\n",
    "![](../figs/aiart/dalle2/glide-eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a4554",
   "metadata": {},
   "source": [
    "#### GLIDE finetuning\n",
    "\n",
    "- The model was fine-tuned to support unconditional image generation. \n",
    "- This training procedure is exactly like pre-training, except 20% of text token sequences are replaced with the empty sequence. \n",
    "- This way, the model retained its ability to generate text-conditional outputs, but can also generate images unconditionally.\n",
    "- The model was also explicitly fine-tuned to perform inpainting. During fine-tuning, random regions of training examples are erased, and the remaining portions are fed into the model along with a mask channel as additional conditioning information.\n",
    "- GLIDE can be used iteratively to produce a complex scene using a zero-shot generation followed by a series of inpainting edits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f73553c",
   "metadata": {},
   "source": [
    "First, an image for the prompt “a cozy living room” is generated, then the shown inpainting masks are used and follow-up text prompts added a painting to the wall, a coffee table, and a vase of flowers on the coffee table, and finally to move the wall up to the couch.\n",
    "\n",
    "![](../figs/aiart/dalle2/glide-inpainting2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e332c9d8",
   "metadata": {},
   "source": [
    "## DALL·E 2/unCLIP\n",
    "\n",
    "- Paper: https://cdn.openai.com/papers/dall-e-2.pdf\n",
    "- Blog post: https://openai.com/dall-e-2/\n",
    "- Code: Not available\n",
    "- Models: Not available\n",
    "- Code (unofficial): https://github.com/lucidrains/DALLE2-pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80337b6b",
   "metadata": {},
   "source": [
    "DALL·E 1 vs. DALL·E 2\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle1-vs-dalle2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164229d",
   "metadata": {},
   "source": [
    "- OpenAI announced its DALL·E 2 system on April 6th, 2022. \n",
    "- The DALL·E 2 system significantly improves results over the original DALL·E. \n",
    "- It generates images with 4x greater resolution (compared to original DALL·E and GLIDE), now up to 1024×1024 pixels. \n",
    "- The model behind the DALL·E 2 system is called unCLIP.\n",
    "- The authors found that humans still slightly prefer GLIDE to unCLIP in terms of photorealism, but the gap is very small. \n",
    "- Even with similar photorealism, unCLIP is strongly preferred over GLIDE in terms of diversity, highlighting one of its benefits. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ba84c",
   "metadata": {},
   "source": [
    "DALL·E 2 can combine concepts, attributes, and styles:\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb26d9",
   "metadata": {},
   "source": [
    " Image editing based on text guidance\n",
    "\n",
    " ![Image editing based on text guidance](../figs/aiart/dalle2/dalle2-editing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d90cd91",
   "metadata": {},
   "source": [
    "Generating variations of an image\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-variations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c963a55",
   "metadata": {},
   "source": [
    "Some problems with DALL·E 2\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-problems.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0d359",
   "metadata": {},
   "source": [
    "unCLIP also struggles at producing coherent text\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e7f03",
   "metadata": {},
   "source": [
    "Producing details in complex scenes:\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-details.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb595a5",
   "metadata": {},
   "source": [
    "### DALL·E 2 technical details\n",
    "\n",
    "- It is a clever combination of CLIP and GLIDE, and the model itself (the full text-conditional image generation stack) is called unCLIP internally in the paper since it generates images by inverting the CLIP image encoder.\n",
    "- The CLIP model is trained separately. \n",
    "- Then the CLIP text encoder generates an embedding for the input text (caption). \n",
    "- Then a special prior model generates an image embedding based on the text embedding. \n",
    "- Then a diffusion decoder generates an image based on the image embedding. \n",
    "- The decoder essentially inverts image embeddings back into images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b1172",
   "metadata": {},
   "source": [
    "A high-level overview of DALL·E 2\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d0da9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
