{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "![](../figs/deep_nlp/tokenization/entelecheia_puzzle_pieces.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc7030",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "NLP systems have three main components that help machines understand natural language:\n",
    "\n",
    "- **Tokenization**: Splitting a string into a list of tokens.\n",
    "- **Embedding**: Mapping tokens to vectors.\n",
    "- **Model**: A neural network that takes token vectors as input and outputs predictions.\n",
    "\n",
    "Tokenization is the first step in the NLP pipeline.\n",
    "\n",
    "- Tokenization is the process of splitting a string into a list of tokens.\n",
    "- For example, the sentence \"I like to eat apples\" can be tokenized into the list of tokens `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`.\n",
    "- The tokens can be words, characters, or subwords.\n",
    "\n",
    "> In deep learning, tokenization is the process of converting a sequence of characters into a sequence of tokens, then converting each token into a numerical vector to be used as input to a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e6390",
   "metadata": {},
   "source": [
    "## What is Tokenization?\n",
    "\n",
    "- Tokenization is the process of representing a text in smaller units called tokens.\n",
    "- In a very simple case, we can simply map every word in the text to a numerical index.\n",
    "- For example, the sentence \"I like to eat apples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`.\n",
    "- Then, each token can be mapped to a unique index, such as:\n",
    "  > `{\"I\": 0, \"like\": 1, \"to\": 2, \"eat\": 3, \"apples\": 4}`.\n",
    "- There are more linguistic features to consider when tokenizing a text, such as punctuation, capitalization, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc320d",
   "metadata": {},
   "source": [
    "## Why do we need tokenization?\n",
    "\n",
    "- \"How can we make a machine read a sentence?\"\n",
    "- Machines don’t know any language, nor do they understand sounds or phonetics.\n",
    "- They need to be taught from scratch.\n",
    "- The first step is to break down the sentence into smaller units that the machine can process.\n",
    "- Tokenization determines how the input is represented to the model.\n",
    "- This decision has a huge impact on the performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9deb0f",
   "metadata": {},
   "source": [
    "## Tokenization Methods\n",
    "\n",
    "- Word-level tokenization: Split a sentence into words.\n",
    "- Character-level tokenization: Split a sentence into characters.\n",
    "- Subword-level tokenization: Split a sentence into subwords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4007e",
   "metadata": {},
   "source": [
    "## Word (White Space) Tokenization\n",
    "\n",
    "- The simplest tokenization method is to split a sentence into words.\n",
    "- This is also called white space tokenization.\n",
    "- The sentence \"I like to eat apples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \"like\", \"to\", \"eat\", \"apples\"]`.\n",
    "- This method is very fast and easy to implement.\n",
    "- However, it has some limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedde7b",
   "metadata": {},
   "source": [
    "### Problems with Word tokenizer\n",
    "\n",
    "- Out-of-vocabulary (OOV) words:\n",
    "  - The risk of missing words that are not in the vocabulary.\n",
    "  - The model will not recognize the variants of words that were not in the training set.\n",
    "  - For example, even though the words `pine` and `apple` exist in the training set, the model will not recognize the word `pineapple`.\n",
    "- Punctuation and abbreviations:\n",
    "  - The tokenizer will not recognize punctuation and abbreviations.\n",
    "  - For example, the word `don't` will be tokenized as `[\"do\", \"n't\"]`.\n",
    "- Slang and informal language:\n",
    "  - The tokenizer will not recognize slang and informal language.\n",
    "  - For example, the word `gonna` will be tokenized as `[\"gon\", \"na\"]`.\n",
    "  - `tl;dr` will be tokenized as `[\"tl\", \";\", \"dr\"]`.\n",
    "- What if language does not use spaces for separating words?\n",
    "  - Chinese, Japanese, and Korean do not use spaces to separate words.\n",
    "  - The tokenizer will not work for these languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f5243",
   "metadata": {},
   "source": [
    "## Character Tokenization\n",
    "\n",
    "- To solve the problems of word tokenization, we can split a sentence into characters.\n",
    "- The sentence \"I like to eat apples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \" \", \"l\", \"i\", \"k\", \"e\", \" \", \"t\", \"o\", \" \", \"e\", \"a\", \"t\", \" \", \"a\", \"p\", \"p\", \"l\", \"e\", \"s\"]`.\n",
    "- However, this method has its own problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c38ec",
   "metadata": {},
   "source": [
    "### Problems with Character tokenizer\n",
    "\n",
    "- The number of tokens is very large.\n",
    "  - This requires more computation and memory.\n",
    "- Limit the application of the model.\n",
    "  - Only certain types of models can be used.\n",
    "  - It is inefficient for the certain types of applications, such as NER.\n",
    "- It would be difficult to understand the relationship between the tokens.\n",
    "  - For example, the tokens `[\"a\", \"p\", \"p\", \"l\", \"e\"]` do not represent the word `apple`.\n",
    "  - The tokens `[\"a\", \"p\", \"p\", \"l\", \"e\"]` do not have any relationship with the tokens `[\"a\", \"p\", \"p\", \"l\", \"e\", \"s\"]`.\n",
    "- Incorrect spelling could be generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d9aa2",
   "metadata": {},
   "source": [
    "## Subword Tokenization\n",
    "\n",
    "- With character-level tokenization, we risk losing the semantic features of the words.\n",
    "- With word-level tokenization, we have out-of-vocabulary (OOV) words or very large vocabulary sizes.\n",
    "- To solve the problems of word tokenization and character tokenization, an algorithm should be able to:\n",
    "  - Retain the semantic features of the words.\n",
    "  - Tokenize any words without the need for a huge vocabulary.\n",
    "- Subword tokenization is a method that can solve these problems.\n",
    "- For example, the sentence \"I like to eat pineapples\" can be tokenized into the list of tokens:\n",
    "  > `[\"I\", \"like\", \"to\", \"eat\", \"pine\", \"##app\", \"##les\"]`.\n",
    "- The model only learns a few subwords that can be used to represent any word.\n",
    "- This solves the problem of OOV words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ac9c3",
   "metadata": {},
   "source": [
    "### How to decide which subwords to use?\n",
    "\n",
    "- There are several algorithms that can be used to decide which subwords to use.\n",
    "  - Byte Pair Encoding (BPE)\n",
    "  - Byte-level BPE\n",
    "  - WordPiece\n",
    "  - Unigram Language Model\n",
    "  - SentencePiece\n",
    "  - Subword Sampling\n",
    "  - BPE-dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f999992",
   "metadata": {},
   "source": [
    "## Normalization and pre-tokenization\n",
    "\n",
    "- Before tokenization, we need to normalize the text.\n",
    "- Normalization is the process of converting a text to a standard form.\n",
    "- For example, we can convert all characters to lowercase.\n",
    "\n",
    "![](../figs/deep_nlp/tokenization/tokenization_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f019d7",
   "metadata": {},
   "source": [
    "The Transformers tokenizer has an attribute called backend_tokenizer that provides access to the underlying tokenizer from the Tokenizers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f6fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37f643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ebd04",
   "metadata": {},
   "source": [
    "### Pre-tokenization\n",
    "\n",
    "- A tokenizer cannot be trained on raw text alone.\n",
    "- First, we need to split the text into smaller units, like words or characters.\n",
    "- Pre-tokenization is the process of splitting the text into smaller units before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72071935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16314362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('Ġ', (14, 15)),\n",
       " ('Ġyou', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e6253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Hello,', (0, 6)),\n",
       " ('▁how', (7, 10)),\n",
       " ('▁are', (11, 14)),\n",
       " ('▁you?', (16, 20))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fd915",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE)\n",
    "\n",
    "Sennrich et al. (2016) proposed a method called Byte Pair Encoding (BPE) to learn subword units.\n",
    "{cite}`sennrich-etal-2016-neural`\n",
    "\n",
    "- Byte Pair Encoding algorithm is originally used for compressing text.\n",
    "- It splits words into sequences of characters and iteratively combines the most frequent character pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c0a95",
   "metadata": {},
   "source": [
    "### How to learn subwords from data?\n",
    "\n",
    "- Count the frequency of each word shown in the corpus.\n",
    "- For each word, append a special stop token `</w>` at the end of the word.\n",
    "- Then, split the word into characters.\n",
    "- Initially, the tokens of the word are all of its characters plus the additional `</w>` token.\n",
    "- For example, the tokens for word `low` are [`l`, `o`, `w`, `</w>`] in order.\n",
    "- So after counting all the words in the dataset, we will get a vocabulary for the tokenized word with its corresponding counts\n",
    "\n",
    "```\n",
    "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b470e11",
   "metadata": {},
   "source": [
    "- In each iteration, count the frequency of each consecutive byte pair, find out the most frequent one, and merge the two byte pair tokens to one token.\n",
    "\n",
    "- For the above example, in the first iteration of the merge, because byte pair `e` and `s` occurred 6 + 3 = 9 times which is the most frequent, merge these into a new token `es`.\n",
    "- Note that token `s` is also gone in this particular example.\n",
    "\n",
    "```\n",
    "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20dc35",
   "metadata": {},
   "source": [
    "- In the second iteration of merge, token `es` and `t` occurred 6 + 3 = 9 times, which is the most frequent.\n",
    "- Merge these to into a new token `est`.\n",
    "- Note that token `es` and `t` are also gone.\n",
    "\n",
    "```\n",
    "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae4514",
   "metadata": {},
   "source": [
    "- In the third iteration of the merge, token `est` and `</w>` pair is the most frequent, etc.\n",
    "- Do this until we have the desired number of tokens or reach the maximum number of iterations.\n",
    "\n",
    "Stop token `</w>` is also important.\n",
    "\n",
    "- Without `</w>`, say if there is a token `st`, this token could be in the word `st ar`, or the wold `wide st`.\n",
    "- Those two words are very different in meaning, but the token `st` is the same.\n",
    "- With `</w>`, if there is a token `st</w>`, the model immediately know that it is the token for the wold `wide st</w>` but not `st ar</w>`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd9f95",
   "metadata": {},
   "source": [
    "To summarize, the algorithm is as follows:\n",
    "\n",
    "1. Extract the words from the given dataset along with their count.\n",
    "2. Define the vocabulary size.\n",
    "3. Split the words into a character sequence.\n",
    "4. Add all the unique characters in our character sequence to the vocabulary.\n",
    "5. Select and merge the symbol pair that has a high frequency.\n",
    "6. Repeat step 5 until the vocabulary size is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48017f83",
   "metadata": {},
   "source": [
    "### BPE in Practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89c1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "\n",
    "def get_vocab(texts):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    for text in texts:\n",
    "        words = text.strip().split()\n",
    "        for word in words:\n",
    "            vocab[\" \".join(list(word)) + \" </w>\"] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9005578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3050ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(\" \".join(pair))\n",
    "    p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(\"\".join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e66f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[\"\".join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edcfa16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Investing com Asian stock markets were broadly lower for a second day on Thursday as weak U S data o'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "cfg = eKonf.compose(\"path\")\n",
    "cfg.cache.uri = \"https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/us_equities_news_sampled.zip\"\n",
    "data = eKonf.load_data(\"us_equities_news_sampled.parquet\", cfg.cached_path)\n",
    "data.text[0][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a0e2d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Before BPE\n",
      "All tokens: dict_keys(['I', 'n', 'v', 'e', 's', 't', 'i', 'g', '</w>', 'c', 'o', 'm', 'A', 'a', 'k', 'r', 'w', 'b', 'd', 'l', 'y', 'f', 'T', 'h', 'u', 'U', 'S', 'p', 'D', 'H', 'K', 'x', '1', '5', 'X', '2', '0', 'J', 'N', '7', 'M', '9', '3', 'E', 'q', '6', '8', 'O', 'j', 'P', 'Y', 'C', '4', 'L', 'B', 'R', 'z', 'F', 'G', 'Q', 'W', 'V', 'Z'])\n",
      "Number of tokens: 63\n"
     ]
    }
   ],
   "source": [
    "vocab = get_vocab(data.text[:1000])\n",
    "\n",
    "print(\"Tokens Before BPE\")\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e34d72e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1\n",
      "Best pair: ('p', 'o')\n",
      "All tokens: dict_keys(['I', 'n', 'v', 'e', 'st', 'ing</w>', 'com', '</w>', 'A', 'si', 'an</w>', 'oc', 'k</w>', 'm', 'ar', 'k', 'ts</w>', 'w', 'er', 'e</w>', 'b', 'ro', 'ad', 'ly</w>', 'l', 'ow', 'er</w>', 'for</w>', 'a</w>', 's', 'ec', 'on', 'd</w>', 'd', 'ay</w>', 'on</w>', 'Th', 'ur', 'as</w>', 'a', 'U', 'S', 'at', 'ab', 'g', 'o', 's</w>', 'or', 'ers</w>', 'ed</w>', 'to</w>', 'con', 'c', 'ov', 'the</w>', 'lo', 'al</w>', 'th</w>', 'ou', 't', 'wh', 'il', 'in', 'por', 'p', 'r', 'of', 'i', 'al', 'o</w>', 'ig', 'h', 'D', 'tr', 'H', 'K', 'an', 'en', 'ex', 'u', '1</w>', '5', '5</w>', 'X', '2', '0', '0</w>', 'di', 'J', 'N', 'sh', '7', 'The</w>', 'f', 'th', 'year', 'h</w>', 'it</w>', 'ear', 'li', 'in</w>', 'as', 'of</w>', 'es', 'M', 'is</w>', 'and</w>', 'et</w>', 'ti', 've</w>', 'y</w>', 'un', 'om', 'ic', '1', '9', 'per', 'ter</w>', '3', 'em', 'E', 'x', 'ir', 'st</w>', 'qu', 'ac', 'y', 'en</w>', 'T', '6', '8', 're', 'el', 'su', 't</w>', 'O', 'n</w>', 'p</w>', 'es</w>', 'j', 'ol', 'that</w>', 'P', 'Y', 'wi', 'pro', 'ing', 'C', 'ri', '4', 'se</w>', 'ati', 'L', 'l</w>', '2</w>', 'ag', 'or</w>', 'ks</w>', 'B', 'R', 'comp', 'be', 'it', 'z', 'at</w>', 'F', 'G', 'ent</w>', 'po', 'Q', 'W', 'are</w>', 'V', 'Z', 'q'])\n",
      "Number of tokens: 175\n",
      "====================\n",
      "Iter: 2\n",
      "Best pair: ('at', 'e</w>')\n",
      "All tokens: dict_keys(['I', 'n', 'v', 'e', 'st', 'ing</w>', 'com', '</w>', 'A', 'si', 'an</w>', 'oc', 'k</w>', 'm', 'ar', 'k', 'ts</w>', 'w', 'er', 'e</w>', 'b', 'ro', 'ad', 'ly</w>', 'l', 'ow', 'er</w>', 'for</w>', 'a</w>', 's', 'ec', 'on', 'd</w>', 'd', 'ay</w>', 'on</w>', 'Th', 'ur', 'as</w>', 'a', 'U', 'S', 'at', 'ab', 'g', 'o', 's</w>', 'or', 'ers</w>', 'ed</w>', 'to</w>', 'con', 'c', 'ov', 'the</w>', 'lo', 'al</w>', 'th</w>', 'ou', 't', 'wh', 'il', 'in', 'por', 'ate</w>', 'p', 'r', 'of', 'i', 'al', 'o</w>', 'ig', 'h', 'D', 'tr', 'H', 'K', 'an', 'en', 'ex', 'u', '1</w>', '5', '5</w>', 'X', '2', '0', '0</w>', 'di', 'J', 'N', 'sh', '7', 'The</w>', 'f', 'th', 'year', 'h</w>', 'it</w>', 'ear', 'li', 'in</w>', 'as', 'of</w>', 'es', 'M', 'is</w>', 'and</w>', 'et</w>', 'ti', 've</w>', 'y</w>', 'un', 'om', 'ic', '1', '9', 'per', 'ter</w>', '3', 'em', 'E', 'x', 'ir', 'st</w>', 'qu', 'ac', 'y', 'en</w>', 'T', '6', '8', 're', 'el', 'su', 't</w>', 'O', 'n</w>', 'p</w>', 'es</w>', 'j', 'ol', 'that</w>', 'P', 'Y', 'wi', 'pro', 'ing', 'C', 'ri', '4', 'se</w>', 'ati', 'L', 'l</w>', '2</w>', 'ag', 'or</w>', 'ks</w>', 'B', 'R', 'comp', 'be', 'it', 'z', 'at</w>', 'F', 'G', 'ent</w>', 'po', 'Q', 'W', 'are</w>', 'V', 'Z', 'q'])\n",
      "Number of tokens: 176\n",
      "====================\n",
      "Iter: 999\n",
      "Best pair: ('announc', 'ed</w>')\n",
      "All tokens: dict_keys(['Invest', 'ing</w>', 'com</w>', 'A', 'si', 'an</w>', 'stock</w>', 'markets</w>', 'were</w>', 'bro', 'ad', 'ly</w>', 'lower</w>', 'for</w>', 'a</w>', 'second</w>', 'day</w>', 'on</w>', 'Thursday</w>', 'as</w>', 'we', 'ak</w>', 'U</w>', 'S</w>', 'data</w>', 'd', 'ur', 'able</w>', 'g', 'oo', 'ds</w>', 'or', 'ers</w>', 'ded</w>', 'to</w>', 'con', 'cer', 'ns</w>', 'over</w>', 'the</w>', 'global</w>', 'growth</w>', 'out', 'look</w>', 'while</w>', 'declin', 'cor', 'por', 'ate</w>', 'prof', 'its</w>', 'also</w>', 'igh', 'ed</w>', 'D', 'uring</w>', 'l', 'trade</w>', 'H', 'ong</w>', 'K', 's</w>', 'an', 'g</w>', 'S', 'en', 'In', 'ex</w>', 't', 'um', 'bl', '1</w>', '5', '5</w>', 'Au', 'str', 'al', 'ia</w>', 'X</w>', '20', '0</w>', 'di', 'pp', 'Jap', 'N', 'i', 'k', 'e', 'i</w>', '2', '25</w>', 'sh', '7</w>', 'The</w>', 'c', 'ame</w>', 'f', 'urther</w>', 'off</w>', 'one</w>', 'year</w>', 'clo', 'sing</w>', 'high</w>', 'h', 'it</w>', 'ear', 'lier</w>', 'in</w>', 'week</w>', 'investors</w>', 'as', 'ahead</w>', 'of</w>', 'ese</w>', 'fiscal</w>', 'end</w>', 'Mar', 'ch</w>', 'is</w>', 'fin', 'al</w>', 'month</w>', 'and</w>', 'market</w>', 'par', 'ti', 'ci', 'p', 'ts</w>', 'have</w>', 'expected</w>', 'many</w>', 'fun', 'oc', 'k</w>', 'from</w>', 'me', 'te', 'ic</w>', '1', '9</w>', 'r', 'ally</w>', 'J', 'u', 'ary</w>', 'peri', 'o', 'd</w>', 'after</w>', 'ed', 'ding</w>', 'more</w>', 'than</w>', '13</w>', 'pr', 'il</w>', 'Dec', 'ember</w>', 'Ex', 'port', 'which</w>', 'gained</w>', 'shar', 'first</w>', 'quarter</w>', 'back</w>', 'ak', 'y', 'en</w>', 'ut', 'om', 'To', 'ot', 'is', 's', 'ped</w>', '6', '8</w>', 'res', 'pec', 'tiv', 'ely</w>', 'consum', 'er</w>', 'elec', 'tr', 'on', 'ic', 'gi', 'ant</w>', 'y</w>', 're', 'ated</w>', 'On</w>', 'up', 'side</w>', 'Sh', 'ar', 'p</w>', 'aw</w>', 'shares</w>', 'j', 'ump</w>', '6</w>', 'ex', 'ten', 'previ', 'ous</w>', '15</w>', 'follow', 'repor', 'that</w>', 'T', 'ai', 'w', '</w>', 'P', 'rec', 'sion</w>', 'du', 'stry</w>', 'bu', 'ying</w>', '10</w>', 'man', 'ufac', 'tur', 'Y', '9', 'billion</w>', 'with</w>', 'two</w>', 'form</w>', 'e</w>', 'up</w>', 'li', 'qui', 'st', 'dis', 'pl', 'ay</w>', 'production</w>', 'E', 'se', 'where</w>', 'under</w>', 'pres', 'sure</w>', 'am', 'id</w>', 'ing', 'er', 'ard</w>', 'China</w>', 'wor', 'ri', 'es</w>', 'I', 'C', 'C</w>', 'ro', 'per', 'ty</w>', 'big', 'est</w>', 'n', 'in', 'sur', 'dro', '4</w>', 'ting</w>', '201', 'net</w>', 'income</w>', 'rose</w>', 'NY', '0', '3</w>', 'mis', 'expect', 'ations</w>', 'Con', 'ta', 'L', 'fell</w>', 'ation</w>', 'larg', 'carri', 'reported</w>', 'lo', 'ss</w>', '2</w>', '7', 'last</w>', 'wi', 'der</w>', 'average</w>', 'estimate</w>', 'ris', 'fu', 'el</w>', 'co', 'sts</w>', 'demand</w>', 'ort</w>', 'oper', 'at', 'or</w>', 'M', 'chan', 'old', 'ings</w>', 'ay', 'ann', 'due</w>', 'b', 'deal</w>', 'trad', 'Chin', 'ban', 'ks</w>', 'stri', 'Com', 'mer', 'cial</w>', 'Bank</w>', 'down</w>', 'their</w>', 'earnings</w>', 'lat', 'R', 'mat', 'ial</w>', 'produc', 'cont', 'ted</w>', 'ses</w>', 'op', 'per</w>', 'min', 'x', 'any</w>', 'Al', 'um</w>', 'Corporation</w>', 'O</w>', 'oil</w>', 'maj', 'ors</w>', 'et', 'O', 'But</w>', 'perform', 'onal</w>', 'equ', 'ities</w>', 'rema', 'close</w>', 'four</w>', 'index</w>', 'was</w>', 'by</w>', 'decline</w>', 'coun', 'struc', 'tion</w>', 'company</w>', 'under', 'ending</w>', '3', 'will</w>', 'be</w>', 'between</w>', 'U', '4', '00</w>', 'million</w>', '50</w>', 'below</w>', 'it', 'increased</w>', 'et</w>', 'a', 'ther</w>', 'ity</w>', 'Me', 'European</w>', 'm', 'il', 'sup', 'o</w>', 'z', 'lead', 'increase</w>', 'de', 'bt</w>', 'fi', 'all</w>', 'com', 'at</w>', 'X', 'F', 'ance</w>', '40</w>', 'G', 'SE</w>', 'ged</w>', 'pu', 'sh</w>', 'of', 'fic', 'employ', 'ment</w>', 'change</w>', 'rele', 'ase</w>', 'iti', 'ess</w>', 'cl', 'ms</w>', 'oli', 'ec', 't</w>', 'ver', 'strong</w>', 'br', 'gn', 'hel', 'ep', 'Inc</w>', 'NYSE</w>', 'N</w>', 'P</w>', 'air', 'performance</w>', 'tt', 'le</w>', 'mi', 'sen', 'se</w>', 'revenue</w>', 'but</w>', 'EPS</w>', 'ch', 'sensus</w>', 'targ', 'Ad', 'for', 'lu', 'tu', 'ter</w>', 'sales</w>', 'some</w>', 'pot', 'ght</w>', 'results</w>', 'th', 'us</w>', 'low</w>', '2016</w>', 'do', 'ad</w>', 'fut', 'ure</w>', 'This</w>', 'analy', 'tic', 'what</w>', 'you</w>', 'can</w>', 'expec', 'be', 'ond</w>', 'ap</w>', 'Q', 'high', 'gener', '18</w>', 'ago</w>', 'line</w>', 'ul', 'l</w>', 'over', 'vi', 'ew</w>', 'For</w>', 'po', 'sted</w>', 'Y</w>', '11</w>', 'show', 'v', 'st</w>', 'years</w>', 'gu', 'id', 'loo', 'king</w>', 'fl', 'However</w>', 'all', 'Wh', 'thr', 'ill', 'about</w>', 'duc', 'ov', 'B', 'es', 'boo', 'get</w>', 'ning</w>', 'mon', 'ey</w>', 'R</w>', 'D</w>', 'recent</w>', 'he', 'av', 'ily</w>', 'dri', 've</w>', 'through</w>', 'fer', 'enti', 'products</w>', 'It</w>', 'estim', 'acc', 'oun', 'early</w>', 'In</w>', 'has</w>', 'un', 'num', 'ber</w>', 'new</w>', 'ach</w>', 'ating</w>', 'reta', 'part', 'ef', 'are</w>', 'ow', 'expan', 'am</w>', 'su', 'verage</w>', 'business</w>', 'including</w>', 'ative</w>', 'pac', 'ag', 'reas', 'ons</w>', 'pe', 'ople</w>', 'increas', 'bec', 'wee', 'used</w>', 'dr', 'th</w>', 'they</w>', 'As</w>', 'such</w>', 'if', 'enc', 'our', 'age</w>', 'take</w>', 'us', 'pur', 'em', 'ent', 'mod', 'sal', 'acks</w>', 'world</w>', 'pre', 'ce</w>', 'develop', 'econom', 'ies</w>', 'ge</w>', 'going</w>', 'gre', 'adv', 'comp', 'ple</w>', 'inclu', 'de</w>', 'ju', 'ices</w>', 'to', 'ause</w>', 'qu', 'sc', 'because</w>', 'when</w>', 'pro', 'cu', 'als</w>', 'T</w>', 'other</w>', 'ben', 'row', 'om</w>', 'ateg', 'ory</w>', 'ack</w>', 'off', 'set</w>', 'grow', 'manag', 'ement</w>', 'own</w>', 'des', 'ir', 'ap', 'poin', 'third</w>', 'gg', 'sive</w>', 'mark', 'eting</w>', 'el', 'ile</w>', 'ran', 'ver</w>', 'past</w>', 'sp', 'perc', 'le', 'ast</w>', 'tec', 'hn', 'most</w>', 'fac', 'ig', 'els</w>', 'into</w>', 'ital</w>', 'ou', 'St', 'ac', 'bre', 'tax</w>', 'ps</w>', 'Re', 'Th', 'marg', 'provi', 'ott', 'port</w>', 'ency</w>', 'been</w>', 'out</w>', 'gr', 'ph', 'wh', 'way</w>', 'ati', 'ves</w>', 'announced</w>', 'current</w>', 'ets</w>', '2019</w>', 'pri', 'mar', 'bet', 'cap', 'iz', 'invest', 'V', 'inter', 'national</w>', 'ear</w>', 'ging</w>', 'still</w>', 'ins</w>', 'ven</w>', 'fr', 'there</w>', 'so</w>', 'much</w>', 'make</w>', 'Mor', 'n</w>', 'ans</w>', 'no</w>', 'may</w>', 'exp', 'share</w>', 'neg', 'im', 'ents</w>', 'busin', 'read', 'ates</w>', 'revenues</w>', 'do</w>', 'ose</w>', 'contin', 'ues</w>', 'inv', '20</w>', 'investment</w>', 'next</w>', 'no', 'tri', 'Inter', 'eng', 'term</w>', 'curr', 'low', 'lim', 'vol', 'ome</w>', 'ar</w>', 'car', 'ough</w>', 'aw', 'well</w>', 'not</w>', 'em</w>', 'ol', 'Group</w>', 'ind', 'eh', 'retur', 'value</w>', 'vid', 'improv', 'ently</w>', 'ned</w>', 'aliz', 'reg', 'before</w>', 'ere</w>', 'spec', 'same</w>', 'sed</w>', 'au', 'comm', 'end', 'dec', 'sions</w>', 'this</w>', 'earch</w>', 'report</w>', 'only</w>', 'ential</w>', 'secur', 'companies</w>', 'sub', 'sti', 'An', 'ited</w>', 'based</w>', 'te</w>', 'form', 'revi', 'sul', 'ob', 'ined</w>', 'ev', 'fore</w>', 'estimates</w>', 'best</w>', 'date</w>', 'publ', 'jec', 'with', 'Ch', 'hnology</w>', 'NASDAQ</w>', 'higher</w>', 'price</w>', 'hi', 'analysts</w>', 'ific', '60</w>', 'll</w>', 'come</w>', 'just</w>', '12</w>', '14</w>', 'buy</w>', 'better</w>', 'old</w>', 'surprise</w>', 'tim', 'c</w>', 'quar', 'ter', 'rev', 'tor</w>', 'ite</w>', 'three</w>', 'ight</w>', 'mov', 'time</w>', 'around</w>', 'tions</w>', 'Friday</w>', 'trading</w>', 'move</w>', 'While</w>', 'ata</w>', 'ha', 'anti', 'aly', 'Fed</w>', 'rates</w>', 'tw', 'ice</w>', 'days</w>', 'red</w>', '8', 'sign', 'ain</w>', 'h</w>', '000</w>', 'ross</w>', 'compared</w>', 'A</w>', 'these</w>', 'trac', 'Oc', 'posi', 'being</w>', 'ough', 'pic', 'alth', 'f</w>', 'ma', 'during</w>', 'resul', 'ational</w>', 'tal</w>', 'like</w>', 'ger</w>', 'ever</w>', 'indu', 'ical</w>', 'now</w>', 'major</w>', 'economy</w>', 'rate</w>', 'key</w>', 'Ju', 'gh', 'est', 'W', 'right</w>', 'news</w>', 'US</w>', 'cre', 'erv', 'Un', 'I</w>', 'ke</w>', 'rel', 'lev', 'prices</w>', 'ail', 'fec', 'since</w>', 'should</w>', 'ue</w>', 'eral</w>', 'months</w>', 'ew', 'ell', 'mo', 'bers</w>', 'see</w>', 'very</w>', 'government</w>', 'isc', 'leg', 'af', 'bo', 'ld</w>', 'ounc', 'timate</w>', 'dic', 'urr', 'E</w>', 'ent</w>', 'kers</w>', 'medi', 'man</w>', 'ach', 'val', 'ued</w>', 'long</w>', 'forec', 'who</w>', 'tal', 'ain', 'consi', 'der', 'against</w>', 'ound</w>', 're</w>', 'Americ', 'm</w>', 'soli', 'sy', 'Stoc', 'M</w>', 'indic', 'sector</w>', 'fir', 'few</w>', 'likely</w>', 'head</w>', 'issu', 'x</w>', 'son</w>', 'ill</w>', 'Gr', 'ving</w>', 'eg', 'our</w>', 'You</w>', 'ments</w>', 'if</w>', 'illion</w>', 'stocks</w>', 'We</w>', 'portfoli', 'Shares</w>', 'tors</w>', 'Bo', 'ab', 'ility</w>', 'deli', 'tive</w>', 'govern', 'shi', 'ous', 'both</w>', 'foc', 'ans', 'lin', 'services</w>', 'ee', '2018</w>', 'acqui', 'ener', 'anc', 'les</w>', 'ow</w>', 'K</w>', 'United</w>', 'States</w>', 'seg', 'operating</w>', '30</w>', 'basis</w>', 'points</w>', 'Zacks</w>', 'Rank</w>', 'Stocks</w>', 'currently</w>', 'ked</w>', 'lobal</w>', 'Strong</w>', 'Buy</w>', '16</w>', 'Z', 'ould</w>', 'strateg', '2017</w>', '500</w>', 'even</w>', 'top</w>', 'Reuters</w>', 'ward</w>', 'recor', 'level</w>', 'range</w>', 'above</w>', 'we</w>', 'ree</w>', 'star', 'ell</w>', 'ood</w>', 'ra', 'dollar</w>', 'sum', 'had</w>', 'could</w>', 'would</w>', 'oll', 'his</w>', 'own', 'ade</w>', 'ession</w>', 'inc', 'disc', 'dem', 'sid', 'tly</w>', 'Tuesday</w>', 'ore</w>', 'q', 'ir</w>', 'Tr', 'serv', 'ble</w>', 'Ener', 'gy</w>', 'sec', 'hnolo', 'De', 'Apple</w>', 'L</w>', 'Ph', 'ru', 'led</w>', 'G</w>', 'accor', 'and', 'dev', 'ably</w>', 'ratio</w>', 'cash</w>', 'how</w>', 'yiel', 'ems</w>', 'he</w>', 'ence</w>', 'iscal</w>', 'interest</w>', 'gain', '21</w>', 'economic</w>', 'ased</w>', 'ine</w>', 'Mon', 'fe', 'ures</w>', 'pol', 'ET', 'custom', 'lik', 'Price</w>', 'Consensus</w>', 'projec', 'Cor', 'ties</w>', 'By</w>', 'New</w>', 'beat</w>', 'bank</w>', 'said</w>', 'percent</w>', 'ste', 'ei', '200', 'Corp</w>', 'Wednesday</w>', 'Res', 'sident</w>', 'illi', 'sin', 'prov', 'olo', 'Ear', 'ace', 'cents</w>', 'positive</w>', 'quarters</w>', 'industry</w>', 'oup</w>', 'F</w>', 'os</w>', 'Amazon</w>', 'arg', 'ays</w>', 'SD', 'announc', 'rise</w>', 'Euro', 'loy', 'polic', 'uro', 'uc', 'Estimate</w>', 'lob', 'ich</w>', 'Bu', 'En', 'prise</w>', 'Ac', 'Earnings</w>', 'ESP</w>', 'Str', 'year', 'Trump</w>', 'anies</w>', 'res</w>', 'B</w>', 'owever</w>', 'aid</w>', 'Amer', 'bas', 'ash</w>', 'Pr', 'Tu', 'overn', 'mu', 'stoc', 'go', 'uters</w>', 'iel', 'month', 'ird</w>', 'Es', 'stom', 'compar', 'ace</w>', 'ised</w>', 'App', 'financial</w>', 'igh</w>', 'ters</w>', 'ollow', 'ollar</w>', 'rati', 'Fin', 'ancial</w>', 'econ', 'ares</w>', 'the', 'edn', 'bel', 'sus</w>', 'emp', 'ank</w>', 'ews</w>', 'nings</w>', 'ES', 'ext</w>', 'NA', 'Corpor', 'vest', 'Q</w>', 'foli', 'oug', 'ou</w>', 'urs', 'devel', 'Fri', 'abo', 'EP', 'Amaz', 'AQ</w>', 'Europe', 'ween</w>', 'reven'])\n",
      "Number of tokens: 1167\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "num_merges = 1000\n",
    "iter_to_print = [1, 2, num_merges - 1]\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    if i in iter_to_print:\n",
    "        print(\"Iter: {}\".format(i))\n",
    "        print(\"Best pair: {}\".format(best))\n",
    "        print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "        print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "        print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e33af",
   "metadata": {},
   "source": [
    "### Encoding and Decoding\n",
    "\n",
    "#### Decoding\n",
    "\n",
    "- Decoding is extremely simple.\n",
    "- Just concatenate all the tokens together and remove the stop token `</w>`.\n",
    "- For example, if the encoded sequence is [`the</w>`, `high`, `est</w>`, `moun`, `tain</w>`], the decoded sequence is `the highest mountain`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd4c6e",
   "metadata": {},
   "source": [
    "#### Encoding\n",
    "\n",
    "For the sentence `the highest mountain`,\n",
    "\n",
    "- List all the tokens in the vocabulary in the order of their length.\n",
    "- For each word, find the longest token that is a subword of the word.\n",
    "- Assume that the vocabulary is `[\"errrr</w>\", \"tain</w>\", \"moun\", \"est</w>\", \"high\", \"the</w>\", \"a</w>\"]`.\n",
    "- Iterate from the longest token `errrr</w>` to the shortest token `a</w>` trying to find the longest token that is a subword of the word.\n",
    "- After all the tokens are checked, all the substrings of the word will be replaced by the tokens.\n",
    "- If there is no token that is a subword of the word, then the word is replaced by the unknown token `</u>`.\n",
    "- In this example, the word `the` is replaced by `the</w>`, the word `highest` is replaced by `high est</w>`, and the word `mountain` is replaced by `moun tain</w>`.\n",
    "- Encoding is very computationally expensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d6764",
   "metadata": {},
   "source": [
    "### BPE Encoding and Decoding in Practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2cb63c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_length(token):\n",
    "    if token[-4:] == \"</w>\":\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46250ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(string, sorted_tokens, unknown_token=\"</u>\"):\n",
    "\n",
    "    if string == \"\":\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    string_tokens = []\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token.replace(\".\", \"[.]\"))\n",
    "\n",
    "        matched_positions = [\n",
    "            (m.start(0), m.end(0)) for m in re.finditer(token_reg, string)\n",
    "        ]\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        substring_end_positions = [\n",
    "            matched_position[0] for matched_position in matched_positions\n",
    "        ]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += tokenize_word(\n",
    "                string=substring,\n",
    "                sorted_tokens=sorted_tokens[i + 1 :],\n",
    "                unknown_token=unknown_token,\n",
    "            )\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(\n",
    "            string=remaining_substring,\n",
    "            sorted_tokens=sorted_tokens[i + 1 :],\n",
    "            unknown_token=unknown_token,\n",
    "        )\n",
    "        break\n",
    "    return string_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d86bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokenization(word_given, sorted_tokens):\n",
    "\n",
    "    print(\"Tokenizing word: {}...\".format(word_given))\n",
    "    if word_given in vocab_tokenization:\n",
    "        print(\"Tokenization of the known word:\")\n",
    "        print(vocab_tokenization[word_given])\n",
    "        print(\"Tokenization treating the known word as unknown:\")\n",
    "        print(\n",
    "            tokenize_word(\n",
    "                string=word_given, sorted_tokens=sorted_tokens, unknown_token=\"</u>\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"Tokenizating of the unknown word:\")\n",
    "        print(\n",
    "            tokenize_word(\n",
    "                string=word_given, sorted_tokens=sorted_tokens, unknown_token=\"</u>\"\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc085b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['performance</w>', 'Corporation</w>', 'investment</w>', 'government</w>', 'production</w>', 'investors</w>', 'companies</w>', 'Consensus</w>', 'estimates</w>', 'increased</w>']\n"
     ]
    }
   ],
   "source": [
    "word_given_known = 'investors</w>'\n",
    "word_given_unknown = 'dogecoin</w>'\n",
    "\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92399eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: investors</w>...\n",
      "Tokenization of the known word:\n",
      "['investors</w>']\n",
      "Tokenization treating the known word as unknown:\n",
      "['investors</w>']\n"
     ]
    }
   ],
   "source": [
    "print_tokenization(word_given_known, sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9150abcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: dogecoin</w>...\n",
      "Tokenizating of the unknown word:\n",
      "['do', 'g', 'ec', 'o', 'in</w>']\n"
     ]
    }
   ],
   "source": [
    "print_tokenization(word_given_unknown, sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa4716",
   "metadata": {},
   "source": [
    "## Byte-level Byte Pair Encoding (BBPE)\n",
    "\n",
    "- Byte-level BPE is a variant of BPE.\n",
    "- It splits words into sequences of bytes instead of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585691c",
   "metadata": {},
   "source": [
    "## Wordpiece\n",
    "\n",
    "- WordPiece is a subword tokenization algorithm that was proposed by Google in 2016.\n",
    "- Wordpiece works almost the same as BPE, but it uses a different way to merge the tokens.\n",
    "- Wordpiece merges tokens based on likelihood of the tokens instead of frequency.\n",
    "- The likelihood is calculated with the $p(w_{i}, w_{j})/p(w_{i})p(w_{j})$ formula.\n",
    "\n",
    "### Tokenization Algorithm\n",
    "\n",
    "- Tokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. \n",
    "- Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. \n",
    "- For instance, for the word `pineapple`, the longest subword in the vocabulary is `pine`, so the word is split into `pine` and `##apple`.\n",
    "- Then, the algorithm finds the longest subword in the vcabulary that is in `##apple`, which is `##apple`, so the word `pineapple` is tokenized into `pine` and `##apple`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e6131",
   "metadata": {},
   "source": [
    "### Implementing WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0af58e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e0fd5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 23578\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_vocab(texts):\n",
    "    vocab = defaultdict(int)\n",
    "    for text in texts:\n",
    "        words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\n",
    "            tokenizer.backend_tokenizer.normalizer.normalize_str(text)\n",
    "        )\n",
    "        words = [word for word, offset in words_with_offsets]\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "word_freqs = get_vocab(data.text[:1000])\n",
    "print(\"Number of words: {}\".format(len(word_freqs.keys())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe7cf7",
   "metadata": {},
   "source": [
    " The alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by ##:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "486e42fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##0', '##1', '##2', '##3', '##4', '##5', '##6', '##7', '##8', '##9', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##q', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', '##z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b913df4",
   "metadata": {},
   "source": [
    "Add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it’s the list [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20a72d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4a680",
   "metadata": {},
   "source": [
    "Split each word, with all the letters that are not the first prefixed by ##:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b95130df",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3494fddb",
   "metadata": {},
   "source": [
    "A function to compute the score of each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3b494b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73134672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', '##n'): 3.357907057413755e-06\n",
      "('##n', '##v'): 5.934364095607283e-07\n",
      "('##v', '##e'): 2.5054548037971313e-06\n",
      "('##e', '##s'): 6.818187606600126e-07\n",
      "('##s', '##t'): 8.862441560966282e-07\n",
      "('##t', '##i'): 9.96079157249097e-07\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b06f8",
   "metadata": {},
   "source": [
    "Find the pair with the highest score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8036f196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('##1', '##9') 5.56466547747346e-05\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08498fdf",
   "metadata": {},
   "source": [
    "So the first merge to learn is (`##1`, `##9`) -> `##19`. Add it to the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f633bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append(\"##19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe293c5a",
   "metadata": {},
   "source": [
    "To continue, we need to apply that merge in our splits dictionary. A function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84dbb5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4ebe0",
   "metadata": {},
   "source": [
    "And we can have a look at the result of the first merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d624b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '##0', '##19']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"##1\", \"##9\", splits)\n",
    "splits[\"2019\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d0b8a",
   "metadata": {},
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want. For example, we can loop until we have a vocabulary of size 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e84fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3000\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc976956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 tokens: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##0', '##1', '##2', '##3', '##4', '##5', '##6', '##7', '##8', '##9', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##q', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', '##z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '##19', '##019', '2019', '##18', '##018', '2018', '##17', '##017', '2017', '##0179', '##01798', '##201798', '##18201798', '##718201798', '##6718201798', '##86718201798', '186718201798', '918', '9182', '119', '##119', '2119', '9119']\n",
      "Last 100 tokens: ['quantita', 'quantitat', 'quantitati', 'quantitativ', 'quanta', 'ny', 'nys', 'nym', '83x', 'suppo', 'suppl', 'supply', 'suppr', 'suppor', 'suppos', 'support', 'suppli', '##lump', 'slump', 'lump', 'lumpy', 'lumpu', 'lumpi', 'lumpur', 'bump', 'lumpin', 'lumping', 'pump', '##sump', 'gump', '##bump', 'hump', '##umph', '##dbump', '##iumph', '##sumpt', '##sumpti', '##riumph', 'triumph', '##sumptio', '##sumptiv', '##sumption', '##mply', '##imply', 'simply', 'iqvi', 'iqvia', 'jpmo', 'jpmor', 'jpmorg', 'jpmorga', 'jpmorgan', 'oppositi', 'oppositio', 'opposition', '151m', 'eqm', 'eqt', '380m', '375m', 'aff', '170k', 'iqo', 'iqos', 'qui', 'quick', 'quickb', 'quickbo', 'quickboo', 'quickbook', 'quickbooks', 'quickl', 'quickly', 'quit', 'whi', 'whic', 'which', 'whil', 'whiff', 'whipp', 'whit', 'whip', 'whis', 'whisp', 'whisk', 'whish', 'whim', 'whims', 'whisha', 'whishaw', 'whist', 'whistl', 'whir', 'whirl', 'whirlp', 'whirlpo', 'whirlpoo', 'whirlpool', 'whipl', 'whipla']\n"
     ]
    }
   ],
   "source": [
    "print(\"First 100 tokens: {}\".format(vocab[:100]))\n",
    "print(\"Last 100 tokens: {}\".format(vocab[-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4118aea",
   "metadata": {},
   "source": [
    "Encoding is done by finding the biggest subword in the vocabulary that is in the word, and splitting on it. Iterating on the word until it is empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a0f6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e55dbedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['app']\n",
      "['app', '##l', '##e']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"app\"))\n",
    "print(encode_word(\"apple\"))\n",
    "print(encode_word(\"사과\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400b6c4",
   "metadata": {},
   "source": [
    "To tokenize a sentence, we can apply this function to each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bbc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenize(\"I like apples and oranges.\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806c658",
   "metadata": {},
   "source": [
    "## Unigram Language Model\n",
    "\n",
    "Kudo (2018) proposed the Unigram language model based subword segmentation algorithm which outputs multiple subword segmentation along with their probabilities. \n",
    "\n",
    "- The model assumes that each subword occurs independently.\n",
    "- The probability of a subword sequence $x=(x_{1}, x_{2}, \\cdots, x_{n})$ is calculated as $p(x)=\\prod_{i=1}^{n} p(x_{i})$.\n",
    "- The most probable segmentation $x^*$ for the sentence $X$ is given by $x^*=\\underset{x \\in S(X)}{\\operatorname{argmax}} P(x)$.\n",
    "- $S(X)$ is a set of all possible segmentations for the sentence $X$.\n",
    "- Subword occurrence probabilities $x_{i}$ are estimated using the [expectation maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) by maximizing the log-likelihood $L$ of the training data $D$.\n",
    "\n",
    "$$\n",
    "L=\\sum_{s = 1}^{|D|} \\log(P(X^{(s)})) = \\sum_{s = 1}^{|D|} \\log(\\sum_{x \\in S(X^{(S)})} P(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97666b",
   "metadata": {},
   "source": [
    "The procedure of obtaining the vocabulary V with a desired size.\n",
    "\n",
    "1. Initialize a reasonably big seed vocabulary.\n",
    "2. Define a desired vocabulary size.\n",
    "3. Optimize the subword occurrence probabilities using the EM algorithm by fixing the vocabulary.\n",
    "4. Compute the loss for each subword. \n",
    "   - The loss of a subword depicts the decrement in the aforementioned likelihood $L$ when that subword is removed from the vocabulary.\n",
    "5. Sort the subwords by loss and keep the top n% of subwords. \n",
    "   - Keep the subwords with a single character to avoid the out of vocabulary problem.\n",
    "6. Repeat step 3 to 5 until it reaches the desired vocabulary size defined in step 2.\n",
    "\n",
    "The most common way to prepare the seed vocabulary is to use the most frequent substrings and characters in the corpus. This unigram language model based subword segmentation consists of characters, subwords and words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f459eae",
   "metadata": {},
   "source": [
    "## SentencePiece\n",
    "\n",
    "- [SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the above models. \n",
    "- It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, ▁. \n",
    "- Used in conjunction with the Unigram algorithm, it doesn’t even require a pre-tokenization step, which is very useful for languages where the space character is not used to separate words, such as Chinese, Japanese, and Thai.\n",
    "- The other main feature of SentencePiece is reversible tokenization: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the _s with spaces — this results in the normalized text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b23c9",
   "metadata": {},
   "source": [
    "### Comparisons with other implementations\n",
    "\n",
    "| Feature                                     |      SentencePiece       | subword-nmt |    WordPiece    |\n",
    "| :------------------------------------------ | :----------------------: | :---------: | :-------------: |\n",
    "| Supported algorithm                         | BPE, unigram, char, word |     BPE     |      BPE\\*      |\n",
    "| OSS?                                        |           Yes            |     Yes     | Google internal |\n",
    "| Subword regularization                      |           Yes            |     No      |       No        |\n",
    "| Python Library (pip)                        |           Yes            |     No      |       N/A       |\n",
    "| C++ Library                                 |           Yes            |     No      |       N/A       |\n",
    "| Pre-segmentation required?                  |            No            |     Yes     |       Yes       |\n",
    "| Customizable normalization (e.g., NFKC)[Yes |            No            |     N/A     |\n",
    "| Direct id generation                        |           Yes            |     No      |       N/A       |\n",
    "\n",
    "Note that BPE algorithm used in WordPiece is slightly different from the original BPE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bb28e",
   "metadata": {},
   "source": [
    "## Subword Sampling\n",
    "\n",
    "- The models are trained with multiple subword segmentation based on a unigram language model and those are probabilistically sampled during training. \n",
    "- $L$-best segmentation is an approach that can be used for approximate sampling. \n",
    "- First, the $l$-best segmentations are obtained and after performing $l$-best search, one segmentation is sampled.\n",
    "- Subword regularization has two hyperparameters which are the size of sampling candidates ($l$) and smoothing constant ($\\alpha$). \n",
    "- Theoretically, setting $l \\to \\infty$ means considering all possible segmentations. \n",
    "- But it is infeasible since the number of characters exponentially increases with the length of the sentence.\n",
    "- Therefore, the Forward-Filtering and Backward-Sampling algorithm is used for sampling. \n",
    "- Further, if $\\alpha$ is small, the distribution is more uniform and if $\\alpha$ is large, it tends towards the Viterbi segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb67699",
   "metadata": {},
   "source": [
    "## BPE-Dropout\n",
    "\n",
    "- BPE-dropout is an effective subword regularization method based on BPE.\n",
    "- This keeps the BPE vocabulary and the merge table as original while changing the segmentation procedure. \n",
    "- Here, some merges are randomly removed with a probability of p at each merge step, thus giving multiple segmentations for the same word. \n",
    "- If the probability is zero, the subword segmentation is equal to the original BPE. \n",
    "- If the probability is one, the subword segmentation is equal to character segmentation. \n",
    "- If the probability is varied from 0 to 1, it gives multiple segmentations with various granularities. \n",
    "- Since this method exposes the models to various subword segmentation, it gives the ability to have a better understanding of words and subwords. \n",
    "- BPE-dropout is a simple procedure since training can be done without training any segmentations other than BPE and inference uses the standard BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [NLP Tokenization](https://medium.com/nerd-for-tech/nlp-tokenization-2fdec7536d17)\n",
    "- [Hugging Face Tokenizers](https://huggingface.co/course/chapter6/1?fw=pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df65b982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
