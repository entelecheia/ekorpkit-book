{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AESO0odcxhzs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Measuring Document Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T12:02:50.293005Z",
     "iopub.status.busy": "2023-02-04T12:02:50.292726Z",
     "iopub.status.idle": "2023-02-04T12:02:53.079486Z",
     "shell.execute_reply": "2023-02-04T12:02:53.078645Z",
     "shell.execute_reply.started": "2023-02-04T12:02:50.292985Z"
    },
    "id": "EJ0AoNl-xif_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0.1.40.post0.dev108\n",
      "project_dir: /workspace/projects/ekorpkit-book/exmaples\n",
      "time: 800 ms (started: 2023-03-27 10:44:47 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "if eKonf.is_colab():\n",
    "    eKonf.mount_google_drive()\n",
    "ws = eKonf.init_workspace(\n",
    "    workspace=\"/workspace\", \n",
    "    project=\"ekorpkit-book/exmaples\", \n",
    "    task=\"esg\", \n",
    "    log_level=\"WARNING\",\n",
    "    verbose=True\n",
    ")\n",
    "print(\"version:\", ws.version)\n",
    "print(\"project_dir:\", ws.project_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to measure similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T12:03:06.188123Z",
     "iopub.status.busy": "2023-02-04T12:03:06.187109Z",
     "iopub.status.idle": "2023-02-04T12:03:13.047506Z",
     "shell.execute_reply": "2023-02-04T12:03:13.046637Z",
     "shell.execute_reply.started": "2023-02-04T12:03:06.188091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.io.file:Saving dataframe to /workspace/projects/ekorpkit-book/exmaples/esg/data/similarity/source_data.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 17s (started: 2023-03-01 02:30:16 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_data_dir = ws.project_dir / \"esg/data/econ_news_kr/news_slice\"\n",
    "filename = \"esg_news_valid_20221229.parquet\"\n",
    "\n",
    "valid_data = eKonf.load_data(filename, news_data_dir)\n",
    "id_cols = [\"filename\", \"codes\", \"chunk_id\"]\n",
    "valid_data.chunk_id = valid_data.chunk_id.astype(str)\n",
    "valid_data[\"doc_id\"] = valid_data[id_cols].apply(lambda x: \"_\".join(x), axis=1)\n",
    "\n",
    "# make date column from filename by splitting filename by \".\", second element is date\n",
    "valid_data[\"date\"] = valid_data.filename.str.split(\".\").str[1]\n",
    "# only need first 14 characters\n",
    "valid_data[\"date\"] = valid_data.date.str[:14]\n",
    "# convert date column to datetime\n",
    "valid_data[\"date\"] = pd.to_datetime(valid_data.date, format=\"%Y%m%d%H%M%S\")\n",
    "\n",
    "source_data_file = ws.project_dir / \"esg/data/similarity/source_data.parquet\"\n",
    "eKonf.save_data(valid_data, source_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>codes</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02100101.20200101040200001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆ 2020 경제기상도 / 업종별 전망 (반도체) ◆ 지난해 미·중 무역분쟁과 공...</td>\n",
       "      <td>000660</td>\n",
       "      <td>02100101.20200101040200001.txt_000660_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02100101.20200101040200002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆ 2020 경제기상도 / 업종별 전망 (가전) ◆ TV, 냉장고, 세탁기 등 전...</td>\n",
       "      <td>066570</td>\n",
       "      <td>02100101.20200101040200002.txt_066570_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02100101.20200101040200002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆ 2020 경제기상도 / 업종별 전망 (가전) ◆ TV, 냉장고, 세탁기 등 전...</td>\n",
       "      <td>005930</td>\n",
       "      <td>02100101.20200101040200002.txt_005930_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02100101.20200101040201001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆ 2020 경제기상도 / 업종별 전망 (디스플레이) ◆ 액정표시장치(LCD) 시...</td>\n",
       "      <td>034220</td>\n",
       "      <td>02100101.20200101040201001.txt_034220_0</td>\n",
       "      <td>2020-01-01 04:02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02100101.20200101040201001.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>디스플레이 업계 등에서는 삼성과 LG가 글로벌 디스플레이 시장에서 중국 업체의 L...</td>\n",
       "      <td>003550</td>\n",
       "      <td>02100101.20200101040201001.txt_003550_1</td>\n",
       "      <td>2020-01-01 04:02:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename chunk_id  \\\n",
       "0  02100101.20200101040200001.txt        0   \n",
       "2  02100101.20200101040200002.txt        0   \n",
       "3  02100101.20200101040200002.txt        0   \n",
       "4  02100101.20200101040201001.txt        0   \n",
       "5  02100101.20200101040201001.txt        1   \n",
       "\n",
       "                                                text   codes  \\\n",
       "0   ◆ 2020 경제기상도 / 업종별 전망 (반도체) ◆ 지난해 미·중 무역분쟁과 공...  000660   \n",
       "2   ◆ 2020 경제기상도 / 업종별 전망 (가전) ◆ TV, 냉장고, 세탁기 등 전...  066570   \n",
       "3   ◆ 2020 경제기상도 / 업종별 전망 (가전) ◆ TV, 냉장고, 세탁기 등 전...  005930   \n",
       "4   ◆ 2020 경제기상도 / 업종별 전망 (디스플레이) ◆ 액정표시장치(LCD) 시...  034220   \n",
       "5   디스플레이 업계 등에서는 삼성과 LG가 글로벌 디스플레이 시장에서 중국 업체의 L...  003550   \n",
       "\n",
       "                                    doc_id                date  \n",
       "0  02100101.20200101040200001.txt_000660_0 2020-01-01 04:02:00  \n",
       "2  02100101.20200101040200002.txt_066570_0 2020-01-01 04:02:00  \n",
       "3  02100101.20200101040200002.txt_005930_0 2020-01-01 04:02:00  \n",
       "4  02100101.20200101040201001.txt_034220_0 2020-01-01 04:02:01  \n",
       "5  02100101.20200101040201001.txt_003550_1 2020-01-01 04:02:01  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.83 s (started: 2023-03-27 10:39:35 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# load source data\n",
    "source_data_file = ws.project_dir / \"esg/data/similarity/source_data.parquet\"\n",
    "data = eKonf.load_data(source_data_file)\n",
    "cols = [\"date\", \"doc_id\", \"text\"]\n",
    "# data = data[cols].sample(1000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyfi.hydra:instantiating ekorpkit.preprocessors.stopwords.Stopwords ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 864 ms (started: 2023-03-01 08:34:22 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cfg_norm = eKonf.compose(\"preprocessor/normalizer=formal_ko\", config_module=\"ekorpkit.conf\")\n",
    "cfg_mcb = eKonf.compose(\"preprocessor/tokenizer=mecab_econ\", config_module=\"ekorpkit.conf\")\n",
    "cfg_mcb.normalize = cfg_norm\n",
    "mecab = eKonf.instantiate(cfg_mcb, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyfi.pipe:Applying pipe: functools.partial(<function tokenize at 0x7f191c6b3ee0>)\n",
      "INFO:hyfi.hydra:instantiating ekorpkit.preprocessors.tokenizer.MecabTokenizer ...\n",
      "INFO:hyfi.hydra:instantiating ekorpkit.preprocessors.stopwords.Stopwords ...\n",
      "INFO:hyfi.pipe:Using batcher with minibatch size: 1000\n",
      "INFO:hyfi.utils.batch.batcher: backend: joblib  minibatch_size: 1000  procs: 50  input_split: False  merge_output: True  len(data): 558923 len(args): 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a156bf1f632a40fe86071b6e31e2d08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing column: text:   0%|          | 0/559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>codes</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02100101.20200101040200001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...</td>\n",
       "      <td>000660</td>\n",
       "      <td>02100101.20200101040200001.txt_000660_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02100101.20200101040200002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...</td>\n",
       "      <td>066570</td>\n",
       "      <td>02100101.20200101040200002.txt_066570_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02100101.20200101040200002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...</td>\n",
       "      <td>005930</td>\n",
       "      <td>02100101.20200101040200002.txt_005930_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02100101.20200101040201001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...</td>\n",
       "      <td>034220</td>\n",
       "      <td>02100101.20200101040201001.txt_034220_0</td>\n",
       "      <td>2020-01-01 04:02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02100101.20200101040201001.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>디스플레이/NNG /SP 업계/NNG /SP 등/NNB 에서/JKB 는/JX /SP...</td>\n",
       "      <td>003550</td>\n",
       "      <td>02100101.20200101040201001.txt_003550_1</td>\n",
       "      <td>2020-01-01 04:02:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename chunk_id  \\\n",
       "0  02100101.20200101040200001.txt        0   \n",
       "2  02100101.20200101040200002.txt        0   \n",
       "3  02100101.20200101040200002.txt        0   \n",
       "4  02100101.20200101040201001.txt        0   \n",
       "5  02100101.20200101040201001.txt        1   \n",
       "\n",
       "                                                text   codes  \\\n",
       "0  ◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...  000660   \n",
       "2  ◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...  066570   \n",
       "3  ◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...  005930   \n",
       "4  ◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...  034220   \n",
       "5  디스플레이/NNG /SP 업계/NNG /SP 등/NNB 에서/JKB 는/JX /SP...  003550   \n",
       "\n",
       "                                    doc_id                date  \n",
       "0  02100101.20200101040200001.txt_000660_0 2020-01-01 04:02:00  \n",
       "2  02100101.20200101040200002.txt_066570_0 2020-01-01 04:02:00  \n",
       "3  02100101.20200101040200002.txt_005930_0 2020-01-01 04:02:00  \n",
       "4  02100101.20200101040201001.txt_034220_0 2020-01-01 04:02:01  \n",
       "5  02100101.20200101040201001.txt_003550_1 2020-01-01 04:02:01  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 11s (started: 2023-03-01 08:34:55 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "\n",
    "cfg = eKonf.compose(\"pipeline/tokenize\")\n",
    "data = eKonf.pipe(data, cfg)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyfi.pipe:Applying pipe: functools.partial(<function extract_tokens at 0x7f191c6b3f70>)\n",
      "INFO:hyfi.hydra:instantiating ekorpkit.preprocessors.tokenizer.MecabTokenizer ...\n",
      "INFO:hyfi.hydra:instantiating ekorpkit.preprocessors.stopwords.Stopwords ...\n",
      "INFO:hyfi.pipe:Using batcher with minibatch size: 1000\n",
      "INFO:hyfi.utils.batch.batcher: backend: joblib  minibatch_size: 1000  procs: 50  input_split: False  merge_output: True  len(data): 558923 len(args): 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c237880ede428790c49af62be29c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting column: text:   0%|          | 0/559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyfi.io.file:Saving dataframe to /workspace/projects/ekorpkit-book/exmaples/esg/data/similarity/tokenized_data.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 24s (started: 2023-03-01 08:36:06 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Extract tokens\n",
    "# stopwords_file = ws.project_dir / \"esg/data/stopwords/stopwords.txt\"\n",
    "tkn_cfg = eKonf.compose(\"preprocessor/tokenizer=mecab_econ\")\n",
    "# tkn_cfg.extract.strip_pos = False\n",
    "\n",
    "cfg = eKonf.compose(\"pipeline/extract_tokens\")\n",
    "cfg.preprocessor.tokenizer = tkn_cfg\n",
    "cfg.nouns_only = False\n",
    "# cfg.stopwords_path = str(stopwords_file)\n",
    "# eKonf.print(cfg)\n",
    "data = eKonf.pipe(data, cfg)\n",
    "\n",
    "tokenized_data_file = ws.project_dir / \"esg/data/similarity/tokenized_data.parquet\"\n",
    "eKonf.save_data(data, tokenized_data_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict similarity\n",
    "\n",
    "Similarity will be measured among the news articles on the same day. The similarity is measured by the cosine similarity of the document vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>codes</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02100101.20200101040206002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>2020 경제 기상도  업종 별 전망 항공 국내 항공업 은 올해 도 침체기 를 이 ...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100101.20200101040206002.txt_020560_0</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02100101.20200102105208001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>지난해 말 HDC 현대산업개발 미래 에 셋 대우 컨소시엄 과 인수 합병 M A 계약...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100101.20200102105208001.txt_020560_0</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02100101.20200102105208001.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>그 는 아시아나항공 이 국내 최고 항공사 로 발돋움 할 수 있 는 기반 이 마련 됐...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100101.20200102105208001.txt_020560_2</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02100101.20200102163114001.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>한창수 아시아나항공 사장 은 2 일 열린 시무식 에서 지난해 체결 된 회사 의 인수...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100101.20200102163114001.txt_020560_2</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02100101.20200102163114001.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>무엇 보다 2 조 2000 억 원 에 달하 는 자본 이 아시아 나 항공 에 투입 돼...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100101.20200102163114001.txt_020560_4</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>02100851.20211221213406001.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>또 2016 년 4 월 아시아 나 항공 이 보유 중 인 금호터미널 지분 전량 을 금...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100851.20211221213406001.txt_020560_3</td>\n",
       "      <td>2021-12-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3713</th>\n",
       "      <td>02100851.20211226171045001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>운수 권 이 관건 조건부 승인 관측 세종 에 있 는 공정 거래 위원회 건물 사진 연...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100851.20211226171045001.txt_020560_0</td>\n",
       "      <td>2021-12-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>02100851.20211229085237001.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>양 연구원 은 대한항공 과 아시아나항공 의 기업 결합 에 따른 일부 노선 운수 권 ...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100851.20211229085237001.txt_020560_2</td>\n",
       "      <td>2021-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3715</th>\n",
       "      <td>02100851.20211229164610001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>미국 EU 등 7 개국 심사 결과 관건 해외 심사 트렌드 엄격 해져 12 월 26 ...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100851.20211229164610001.txt_020560_0</td>\n",
       "      <td>2021-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716</th>\n",
       "      <td>02100851.20211229183033001.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>이날 공정위 는 대한항공 의 아시아나항공 인수 합병 과 관련 해 경쟁 제한 우려 를...</td>\n",
       "      <td>020560</td>\n",
       "      <td>02100851.20211229183033001.txt_020560_2</td>\n",
       "      <td>2021-12-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3717 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename chunk_id  \\\n",
       "0     02100101.20200101040206002.txt        0   \n",
       "1     02100101.20200102105208001.txt        0   \n",
       "2     02100101.20200102105208001.txt        2   \n",
       "3     02100101.20200102163114001.txt        2   \n",
       "4     02100101.20200102163114001.txt        4   \n",
       "...                              ...      ...   \n",
       "3712  02100851.20211221213406001.txt        3   \n",
       "3713  02100851.20211226171045001.txt        0   \n",
       "3714  02100851.20211229085237001.txt        2   \n",
       "3715  02100851.20211229164610001.txt        0   \n",
       "3716  02100851.20211229183033001.txt        2   \n",
       "\n",
       "                                                   text   codes  \\\n",
       "0     2020 경제 기상도  업종 별 전망 항공 국내 항공업 은 올해 도 침체기 를 이 ...  020560   \n",
       "1     지난해 말 HDC 현대산업개발 미래 에 셋 대우 컨소시엄 과 인수 합병 M A 계약...  020560   \n",
       "2     그 는 아시아나항공 이 국내 최고 항공사 로 발돋움 할 수 있 는 기반 이 마련 됐...  020560   \n",
       "3     한창수 아시아나항공 사장 은 2 일 열린 시무식 에서 지난해 체결 된 회사 의 인수...  020560   \n",
       "4     무엇 보다 2 조 2000 억 원 에 달하 는 자본 이 아시아 나 항공 에 투입 돼...  020560   \n",
       "...                                                 ...     ...   \n",
       "3712  또 2016 년 4 월 아시아 나 항공 이 보유 중 인 금호터미널 지분 전량 을 금...  020560   \n",
       "3713  운수 권 이 관건 조건부 승인 관측 세종 에 있 는 공정 거래 위원회 건물 사진 연...  020560   \n",
       "3714  양 연구원 은 대한항공 과 아시아나항공 의 기업 결합 에 따른 일부 노선 운수 권 ...  020560   \n",
       "3715  미국 EU 등 7 개국 심사 결과 관건 해외 심사 트렌드 엄격 해져 12 월 26 ...  020560   \n",
       "3716  이날 공정위 는 대한항공 의 아시아나항공 인수 합병 과 관련 해 경쟁 제한 우려 를...  020560   \n",
       "\n",
       "                                       doc_id        date  \n",
       "0     02100101.20200101040206002.txt_020560_0  2020-01-01  \n",
       "1     02100101.20200102105208001.txt_020560_0  2020-01-02  \n",
       "2     02100101.20200102105208001.txt_020560_2  2020-01-02  \n",
       "3     02100101.20200102163114001.txt_020560_2  2020-01-02  \n",
       "4     02100101.20200102163114001.txt_020560_4  2020-01-02  \n",
       "...                                       ...         ...  \n",
       "3712  02100851.20211221213406001.txt_020560_3  2021-12-21  \n",
       "3713  02100851.20211226171045001.txt_020560_0  2021-12-26  \n",
       "3714  02100851.20211229085237001.txt_020560_2  2021-12-29  \n",
       "3715  02100851.20211229164610001.txt_020560_0  2021-12-29  \n",
       "3716  02100851.20211229183033001.txt_020560_2  2021-12-29  \n",
       "\n",
       "[3717 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.98 s (started: 2023-03-27 10:51:34 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "tokenized_data_file = ws.project_dir / \"esg/data/similarity/tokenized_data.parquet\"\n",
    "data = eKonf.load_data(tokenized_data_file)\n",
    "\n",
    "data = data[data.codes == \"020560\"]\n",
    "data = data.reset_index(drop=True)\n",
    "# Extract the date part\n",
    "data[\"date\"] = data[\"date\"].dt.date\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyfi.io.file:Saving dataframe to /workspace/projects/ekorpkit-book/exmaples/esg/data/similarity/similarity_results.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 4454647\n",
      "time: 1h 5min 33s (started: 2023-03-27 10:52:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "tokenized_data_file = ws.project_dir / \"esg/data/similarity/tokenized_data.parquet\"\n",
    "data = eKonf.load_data(tokenized_data_file)\n",
    "data = data.reset_index(drop=True)\n",
    "# Extract the date part\n",
    "data[\"date\"] = data[\"date\"].dt.date\n",
    "\n",
    "# Convert the data into a matrix representation using TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(\" \"))\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the unique dates, ignoring time information\n",
    "unique_dates = data[\"date\"].unique()\n",
    "for i, date in enumerate(unique_dates):\n",
    "    # Calculate the similarity between the current document and the previous seven days\n",
    "    current_doc = data[data[\"date\"] == date][\"doc_id\"].iloc[0]\n",
    "    current_text = data[data[\"date\"] == date][\"text\"].iloc[0]\n",
    "    previous_period_start = date - pd.Timedelta(7, \"d\")\n",
    "    previous_period_end = date\n",
    "    previous_period = data[\n",
    "        (data[\"date\"] >= previous_period_start) & (data[\"date\"] <= previous_period_end)\n",
    "    ][\"doc_id\"]\n",
    "    previous_text = data[\n",
    "        (data[\"date\"] >= previous_period_start) & (data[\"date\"] <= previous_period_end)\n",
    "    ][\"text\"]\n",
    "    matrix = vectorizer.fit_transform(previous_text.append(pd.Series(current_text)))\n",
    "    similarity = cosine_similarity(matrix)\n",
    "    current_doc_index = matrix.shape[0] - 1\n",
    "    for j, doc in enumerate(previous_period):\n",
    "        if current_doc == doc:\n",
    "            continue\n",
    "        sim = similarity[current_doc_index][j]\n",
    "        results.append(\n",
    "            [date, previous_period_start, previous_period_end, current_doc, doc, sim]\n",
    "        )\n",
    "\n",
    "# Convert the results list into a data frame\n",
    "results = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"date\", \"start_date\", \"end_date\", \"doc_id_1\", \"doc_id_2\", \"similarity\"],\n",
    ")\n",
    "print(f\"Number of results: {len(results)}\")\n",
    "# save the results\n",
    "# similarity_results_file = (\n",
    "#     ws.project_dir / \"esg/data/similarity/similarity_results-020560.parquet\"\n",
    "# )\n",
    "similarity_results_file = (\n",
    "    ws.project_dir / \"esg/data/similarity/similarity_results.parquet\"\n",
    ")\n",
    "eKonf.save_data(results, similarity_results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>doc_id_1</th>\n",
       "      <th>doc_id_2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21207</th>\n",
       "      <td>2021-03-27</td>\n",
       "      <td>2021-03-20</td>\n",
       "      <td>2021-03-27</td>\n",
       "      <td>02100201.20210327075129001.txt_020560_4</td>\n",
       "      <td>02100201.20210326180200001.txt_020560_5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>02100101.20210105174916001.txt_020560_4</td>\n",
       "      <td>02100101.20210105175726001.txt_020560_13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21255</th>\n",
       "      <td>2021-03-28</td>\n",
       "      <td>2021-03-21</td>\n",
       "      <td>2021-03-28</td>\n",
       "      <td>02100201.20210328175011001.txt_020560_1</td>\n",
       "      <td>02100801.20210328185945001.txt_020560_1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22122</th>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>02100101.20210706143057001.txt_020560_0</td>\n",
       "      <td>02100701.20210706135911001.txt_020560_1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18414</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>2021-12-26</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>02100701.20220102150018001.txt_020560_0</td>\n",
       "      <td>02100801.20220102145824001.txt_020560_0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  start_date    end_date  \\\n",
       "21207  2021-03-27  2021-03-20  2021-03-27   \n",
       "19987  2021-01-05  2020-12-29  2021-01-05   \n",
       "21255  2021-03-28  2021-03-21  2021-03-28   \n",
       "22122  2021-07-06  2021-06-29  2021-07-06   \n",
       "18414  2022-01-02  2021-12-26  2022-01-02   \n",
       "\n",
       "                                      doc_id_1  \\\n",
       "21207  02100201.20210327075129001.txt_020560_4   \n",
       "19987  02100101.20210105174916001.txt_020560_4   \n",
       "21255  02100201.20210328175011001.txt_020560_1   \n",
       "22122  02100101.20210706143057001.txt_020560_0   \n",
       "18414  02100701.20220102150018001.txt_020560_0   \n",
       "\n",
       "                                       doc_id_2  similarity  \n",
       "21207   02100201.20210326180200001.txt_020560_5         1.0  \n",
       "19987  02100101.20210105175726001.txt_020560_13         1.0  \n",
       "21255   02100801.20210328185945001.txt_020560_1         1.0  \n",
       "22122   02100701.20210706135911001.txt_020560_1         1.0  \n",
       "18414   02100801.20220102145824001.txt_020560_0         1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.6 ms (started: 2023-03-27 10:52:02 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# sort by similarity\n",
    "results = results.sort_values(by=['similarity'], ascending=False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "include_colab_link": true,
   "name": "preprocessor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
