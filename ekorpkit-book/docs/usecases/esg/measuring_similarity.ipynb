{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AESO0odcxhzs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Measuring Document Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T12:02:50.293005Z",
     "iopub.status.busy": "2023-02-04T12:02:50.292726Z",
     "iopub.status.idle": "2023-02-04T12:02:53.079486Z",
     "shell.execute_reply": "2023-02-04T12:02:53.078645Z",
     "shell.execute_reply.started": "2023-02-04T12:02:50.292985Z"
    },
    "id": "EJ0AoNl-xif_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.env:Set environment variable EKORPKIT_DATA_ROOT=/workspace/data\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable CACHED_PATH_CACHE_ROOT=/workspace/.cache/cached_path\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable WANDB_DIR=/workspace/projects/ekorpkit-book/exmaples/logs\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable WANDB_PROJECT=ekorpkit-book-exmaples\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable WANDB_NOTEBOOK_NAME=/workspace/projects/ekorpkit-book/exmaples/logs/esg-nb\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable WANDB_SILENT=False\n",
      "INFO:ekorpkit.hyfi.utils.env:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n",
      "INFO:ekorpkit.hyfi.hydra:initialized batcher with <ekorpkit.hyfi.utils.batch.batcher.Batcher object at 0x7f95fb3c5df0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0.1.40.post0.dev100\n",
      "project_dir: /workspace/projects/ekorpkit-book/exmaples\n",
      "time: 1.26 s (started: 2023-02-13 10:31:35 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "if eKonf.is_colab():\n",
    "    eKonf.mount_google_drive()\n",
    "ws = eKonf.set_workspace(\n",
    "    workspace=\"/workspace\", \n",
    "    project=\"ekorpkit-book/exmaples\", \n",
    "    task=\"esg\", \n",
    "    log_level=\"INFO\",\n",
    "    verbose=True\n",
    ")\n",
    "print(\"version:\", ws.version)\n",
    "print(\"project_dir:\", ws.project_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to measure similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T12:03:06.188123Z",
     "iopub.status.busy": "2023-02-04T12:03:06.187109Z",
     "iopub.status.idle": "2023-02-04T12:03:13.047506Z",
     "shell.execute_reply": "2023-02-04T12:03:13.046637Z",
     "shell.execute_reply.started": "2023-02-04T12:03:06.188091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.io.file:Saving dataframe to /workspace/projects/ekorpkit-book/exmaples/esg/data/similarity/source_data.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 17s (started: 2023-02-13 09:07:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_data_dir = ws.project_dir / \"esg/data/econ_news_kr/news_slice\"\n",
    "filename = \"esg_news_valid_20221229.parquet\"\n",
    "\n",
    "valid_data = eKonf.load_data(filename, news_data_dir)\n",
    "id_cols = [\"filename\", \"codes\", \"chunk_id\"]\n",
    "valid_data.chunk_id = valid_data.chunk_id.astype(str)\n",
    "valid_data[\"doc_id\"] = valid_data[id_cols].apply(lambda x: \"_\".join(x), axis=1)\n",
    "\n",
    "# make date column from filename by splitting filename by \".\", second element is date\n",
    "valid_data[\"date\"] = valid_data.filename.str.split(\".\").str[1]\n",
    "# only need first 14 characters\n",
    "valid_data[\"date\"] = valid_data.date.str[:14]\n",
    "# convert date column to datetime\n",
    "valid_data[\"date\"] = pd.to_datetime(valid_data.date, format=\"%Y%m%d%H%M%S\")\n",
    "\n",
    "source_data_file = ws.project_dir / \"esg/data/similarity/source_data.parquet\"\n",
    "eKonf.save_data(valid_data, source_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>codes</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02100101.20200101040200001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆ 2020 경제기상도 / 업종별 전망 (반도체) ◆ 지난해 미·중 무역분쟁과 공...</td>\n",
       "      <td>000660</td>\n",
       "      <td>02100101.20200101040200001.txt_000660_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02100101.20200101040200002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆ 2020 경제기상도 / 업종별 전망 (가전) ◆ TV, 냉장고, 세탁기 등 전...</td>\n",
       "      <td>066570</td>\n",
       "      <td>02100101.20200101040200002.txt_066570_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02100101.20200101040200002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆ 2020 경제기상도 / 업종별 전망 (가전) ◆ TV, 냉장고, 세탁기 등 전...</td>\n",
       "      <td>005930</td>\n",
       "      <td>02100101.20200101040200002.txt_005930_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02100101.20200101040201001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆ 2020 경제기상도 / 업종별 전망 (디스플레이) ◆ 액정표시장치(LCD) 시...</td>\n",
       "      <td>034220</td>\n",
       "      <td>02100101.20200101040201001.txt_034220_0</td>\n",
       "      <td>2020-01-01 04:02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02100101.20200101040201001.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>디스플레이 업계 등에서는 삼성과 LG가 글로벌 디스플레이 시장에서 중국 업체의 L...</td>\n",
       "      <td>003550</td>\n",
       "      <td>02100101.20200101040201001.txt_003550_1</td>\n",
       "      <td>2020-01-01 04:02:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename chunk_id  \\\n",
       "0  02100101.20200101040200001.txt        0   \n",
       "2  02100101.20200101040200002.txt        0   \n",
       "3  02100101.20200101040200002.txt        0   \n",
       "4  02100101.20200101040201001.txt        0   \n",
       "5  02100101.20200101040201001.txt        1   \n",
       "\n",
       "                                                text   codes  \\\n",
       "0   ◆ 2020 경제기상도 / 업종별 전망 (반도체) ◆ 지난해 미·중 무역분쟁과 공...  000660   \n",
       "2   ◆ 2020 경제기상도 / 업종별 전망 (가전) ◆ TV, 냉장고, 세탁기 등 전...  066570   \n",
       "3   ◆ 2020 경제기상도 / 업종별 전망 (가전) ◆ TV, 냉장고, 세탁기 등 전...  005930   \n",
       "4   ◆ 2020 경제기상도 / 업종별 전망 (디스플레이) ◆ 액정표시장치(LCD) 시...  034220   \n",
       "5   디스플레이 업계 등에서는 삼성과 LG가 글로벌 디스플레이 시장에서 중국 업체의 L...  003550   \n",
       "\n",
       "                                    doc_id                date  \n",
       "0  02100101.20200101040200001.txt_000660_0 2020-01-01 04:02:00  \n",
       "2  02100101.20200101040200002.txt_066570_0 2020-01-01 04:02:00  \n",
       "3  02100101.20200101040200002.txt_005930_0 2020-01-01 04:02:00  \n",
       "4  02100101.20200101040201001.txt_034220_0 2020-01-01 04:02:01  \n",
       "5  02100101.20200101040201001.txt_003550_1 2020-01-01 04:02:01  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.33 s (started: 2023-02-13 10:36:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# load source data\n",
    "source_data_file = ws.project_dir / \"esg/data/similarity/source_data.parquet\"\n",
    "data = eKonf.load_data(source_data_file)\n",
    "cols = [\"date\", \"doc_id\", \"text\"]\n",
    "# data = data[cols].sample(1000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.preprocessors.tokenizer:Initializing mecab with {'userdic_path': '/workspace/projects/ekorpkit/ekorpkit/resources/dictionaries/mecab/ekon_v1.dic', 'backend': 'mecab-python3', 'verbose': True}...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:instantiating ekorpkit.preprocessors.normalizer.Normalizer...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:instantiating ekorpkit.preprocessors.stopwords.Stopwords...\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.stopwords.Stopwords ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:MecabTokenizer initialized with:\n",
      "INFO:ekorpkit.preprocessors.tokenizer:\treturn_as_list: False\n",
      "INFO:ekorpkit.tokenizers.mecab:MeCab uses mecab-python3 as backend.\n",
      "INFO:ekorpkit.tokenizers.mecab:Mecab uses system dictionary: /opt/conda/lib/python3.8/site-packages/mecab_ko_dic/dicdir, user dictionary: /workspace/projects/ekorpkit/ekorpkit/resources/dictionaries/mecab/ekon_v1.dic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 953 ms (started: 2023-02-13 10:37:01 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cfg_norm = eKonf.compose(\"preprocessor/normalizer=formal_ko\")\n",
    "cfg_mcb = eKonf.compose(\"preprocessor/tokenizer=mecab_econ\")\n",
    "cfg_mcb.normalize = cfg_norm\n",
    "mecab = eKonf.instantiate(cfg_mcb, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.pipe:Applying pipe: functools.partial(<function tokenize at 0x7f91dcf11a60>)\n",
      "INFO:ekorpkit.pipelines.pipe:instantiating tokenizer\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.tokenizer.MecabTokenizer ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:Initializing mecab with {'userdic_path': None, 'backend': 'mecab-python3', 'verbose': True}...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:instantiating ekorpkit.preprocessors.stopwords.Stopwords...\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.stopwords.Stopwords ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:MecabTokenizer initialized with:\n",
      "INFO:ekorpkit.preprocessors.tokenizer:\treturn_as_list: False\n",
      "INFO:ekorpkit.tokenizers.mecab:MeCab uses mecab-python3 as backend.\n",
      "INFO:ekorpkit.tokenizers.mecab:Mecab uses system dictionary: /opt/conda/lib/python3.8/site-packages/mecab_ko_dic/dicdir, user dictionary: None\n",
      "INFO:ekorpkit.hyfi.pipe:Using batcher with minibatch size: 1000\n",
      "INFO:ekorpkit.hyfi.utils.batch.batcher: backend: joblib  minibatch_size: 1000  procs: 50  input_split: False  merge_output: True  len(data): 558923 len(args): 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8adff596344dc9bdd3371f807aaf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing column: text:   0%|          | 0/559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.pipelines.pipe: >> elapsed time to segment: 0:01:11.623086\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>codes</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02100101.20200101040200001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...</td>\n",
       "      <td>000660</td>\n",
       "      <td>02100101.20200101040200001.txt_000660_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02100101.20200101040200002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...</td>\n",
       "      <td>066570</td>\n",
       "      <td>02100101.20200101040200002.txt_066570_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02100101.20200101040200002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...</td>\n",
       "      <td>005930</td>\n",
       "      <td>02100101.20200101040200002.txt_005930_0</td>\n",
       "      <td>2020-01-01 04:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02100101.20200101040201001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...</td>\n",
       "      <td>034220</td>\n",
       "      <td>02100101.20200101040201001.txt_034220_0</td>\n",
       "      <td>2020-01-01 04:02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02100101.20200101040201001.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>디스플레이/NNG /SP 업계/NNG /SP 등/NNB 에서/JKB 는/JX /SP...</td>\n",
       "      <td>003550</td>\n",
       "      <td>02100101.20200101040201001.txt_003550_1</td>\n",
       "      <td>2020-01-01 04:02:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename chunk_id  \\\n",
       "0  02100101.20200101040200001.txt        0   \n",
       "2  02100101.20200101040200002.txt        0   \n",
       "3  02100101.20200101040200002.txt        0   \n",
       "4  02100101.20200101040201001.txt        0   \n",
       "5  02100101.20200101040201001.txt        1   \n",
       "\n",
       "                                                text   codes  \\\n",
       "0  ◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...  000660   \n",
       "2  ◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...  066570   \n",
       "3  ◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...  005930   \n",
       "4  ◆/SY /SP 2020/SN /SP 경제/NNG 기상도/NNG /SP //SC /...  034220   \n",
       "5  디스플레이/NNG /SP 업계/NNG /SP 등/NNB 에서/JKB 는/JX /SP...  003550   \n",
       "\n",
       "                                    doc_id                date  \n",
       "0  02100101.20200101040200001.txt_000660_0 2020-01-01 04:02:00  \n",
       "2  02100101.20200101040200002.txt_066570_0 2020-01-01 04:02:00  \n",
       "3  02100101.20200101040200002.txt_005930_0 2020-01-01 04:02:00  \n",
       "4  02100101.20200101040201001.txt_034220_0 2020-01-01 04:02:01  \n",
       "5  02100101.20200101040201001.txt_003550_1 2020-01-01 04:02:01  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 11s (started: 2023-02-13 10:37:02 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "\n",
    "cfg = eKonf.compose(\"pipeline/tokenize\")\n",
    "data = eKonf.pipe(data, cfg)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.pipe:Applying pipe: functools.partial(<function extract_tokens at 0x7f91dcf11af0>)\n",
      "INFO:ekorpkit.pipelines.pipe:instantiating tokenizer\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.tokenizer.MecabTokenizer ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:Initializing mecab with {'userdic_path': '/workspace/projects/ekorpkit/ekorpkit/resources/dictionaries/mecab/ekon_v1.dic', 'backend': 'mecab-python3', 'verbose': True}...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:instantiating ekorpkit.preprocessors.stopwords.Stopwords...\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.stopwords.Stopwords ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:MecabTokenizer initialized with:\n",
      "INFO:ekorpkit.preprocessors.tokenizer:\treturn_as_list: False\n",
      "INFO:ekorpkit.tokenizers.mecab:MeCab uses mecab-python3 as backend.\n",
      "INFO:ekorpkit.tokenizers.mecab:Mecab uses system dictionary: /opt/conda/lib/python3.8/site-packages/mecab_ko_dic/dicdir, user dictionary: /workspace/projects/ekorpkit/ekorpkit/resources/dictionaries/mecab/ekon_v1.dic\n",
      "INFO:ekorpkit.pipelines.pipe:extract_func: <bound method Tokenizer.extract_tokens of <ekorpkit.preprocessors.tokenizer.MecabTokenizer object at 0x7f91e844d730>>\n",
      "INFO:ekorpkit.hyfi.pipe:Using batcher with minibatch size: 1000\n",
      "INFO:ekorpkit.hyfi.utils.batch.batcher: backend: joblib  minibatch_size: 1000  procs: 50  input_split: False  merge_output: True  len(data): 558923 len(args): 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efb365621214b5c81fa89ca657bb36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting column: text:   0%|          | 0/559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.pipelines.pipe: >> elapsed time to extract tokens: 0:00:13.634850\n",
      "INFO:ekorpkit.hyfi.io.file:Saving dataframe to /workspace/projects/ekorpkit-book/exmaples/esg/data/similarity/tokenized_data.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 23s (started: 2023-02-13 10:38:15 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Extract tokens\n",
    "# stopwords_file = ws.project_dir / \"esg/data/stopwords/stopwords.txt\"\n",
    "tkn_cfg = eKonf.compose(\"preprocessor/tokenizer=mecab_econ\")\n",
    "# tkn_cfg.extract.strip_pos = False\n",
    "\n",
    "cfg = eKonf.compose(\"pipeline/extract_tokens\")\n",
    "cfg.preprocessor.tokenizer = tkn_cfg\n",
    "cfg.nouns_only = False\n",
    "# cfg.stopwords_path = str(stopwords_file)\n",
    "# eKonf.print(cfg)\n",
    "data = eKonf.pipe(data, cfg)\n",
    "\n",
    "tokenized_data_file = ws.project_dir / \"esg/data/similarity/tokenized_data.parquet\"\n",
    "eKonf.save_data(data, tokenized_data_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict similarity\n",
    "\n",
    "Similarity will be measured among the news articles on the same day. The similarity is measured by the cosine similarity of the document vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.io.file:Saving dataframe to /workspace/projects/ekorpkit-book/exmaples/esg/data/similarity/similarity_results.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 51min 39s (started: 2023-02-13 11:21:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "tokenized_data_file = ws.project_dir / \"esg/data/similarity/tokenized_data.parquet\"\n",
    "data = eKonf.load_data(tokenized_data_file)\n",
    "data = data.reset_index(drop=True)\n",
    "# Extract the date part\n",
    "data['date'] = data['date'].dt.date\n",
    "\n",
    "# Convert the data into a matrix representation using TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(\" \"))\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the unique dates, ignoring time information\n",
    "unique_dates = data['date'].unique()\n",
    "for i, date in enumerate(unique_dates):\n",
    "    # Calculate the similarity between the current document and the previous seven days\n",
    "    current_doc = data[data['date'] == date]['doc_id'].iloc[0]\n",
    "    current_text = data[data['date'] == date]['text'].iloc[0]\n",
    "    previous_period_start = date - pd.Timedelta(7, 'd')\n",
    "    previous_period_end = date\n",
    "    previous_period = data[(data['date'] >= previous_period_start) & (data['date'] < previous_period_end)]['doc_id']\n",
    "    previous_text = data[(data['date'] >= previous_period_start) & (data['date'] < previous_period_end)]['text']\n",
    "    matrix = vectorizer.fit_transform(previous_text.append(pd.Series(current_text)))\n",
    "    similarity = cosine_similarity(matrix)\n",
    "    current_doc_index = matrix.shape[0] - 1\n",
    "    for j, doc in enumerate(previous_period):\n",
    "        sim = similarity[current_doc_index][j]\n",
    "        results.append([date, previous_period_start, previous_period_end, current_doc, doc, sim])\n",
    "\n",
    "# Convert the results list into a data frame\n",
    "results = pd.DataFrame(results, columns=['date', 'start_date', 'end_date', 'doc_id_1', 'doc_id_2', 'similarity'])\n",
    "\n",
    "# save the results\n",
    "similarity_results_file = ws.project_dir / \"esg/data/similarity/similarity_results.parquet\"\n",
    "eKonf.save_data(results, similarity_results_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>doc_id_1</th>\n",
       "      <th>doc_id_2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1558596</th>\n",
       "      <td>2022-01-22</td>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>2022-01-22</td>\n",
       "      <td>02100101.20220122102031004.txt_308100_0</td>\n",
       "      <td>02100101.20220121161033003.txt_308100_0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469862</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>2021-12-26</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>02100101.20220102093606001.txt_373220_2</td>\n",
       "      <td>02100101.20211231172619002.txt_373220_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263972</th>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>02100101.20210203020154001.txt_122870_0</td>\n",
       "      <td>02100101.20210202164529001.txt_122870_0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263973</th>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>02100101.20210203020154001.txt_122870_0</td>\n",
       "      <td>02100101.20210202164529001.txt_037270_0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200858</th>\n",
       "      <td>2021-07-21</td>\n",
       "      <td>2021-07-14</td>\n",
       "      <td>2021-07-21</td>\n",
       "      <td>02100101.20210721000359002.txt_095700_0</td>\n",
       "      <td>02100101.20210720204138001.txt_095700_0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  start_date    end_date  \\\n",
       "1558596  2022-01-22  2022-01-15  2022-01-22   \n",
       "1469862  2022-01-02  2021-12-26  2022-01-02   \n",
       "2263972  2021-02-03  2021-01-27  2021-02-03   \n",
       "2263973  2021-02-03  2021-01-27  2021-02-03   \n",
       "3200858  2021-07-21  2021-07-14  2021-07-21   \n",
       "\n",
       "                                        doc_id_1  \\\n",
       "1558596  02100101.20220122102031004.txt_308100_0   \n",
       "1469862  02100101.20220102093606001.txt_373220_2   \n",
       "2263972  02100101.20210203020154001.txt_122870_0   \n",
       "2263973  02100101.20210203020154001.txt_122870_0   \n",
       "3200858  02100101.20210721000359002.txt_095700_0   \n",
       "\n",
       "                                        doc_id_2  similarity  \n",
       "1558596  02100101.20220121161033003.txt_308100_0         1.0  \n",
       "1469862  02100101.20211231172619002.txt_373220_2         1.0  \n",
       "2263972  02100101.20210202164529001.txt_122870_0         1.0  \n",
       "2263973  02100101.20210202164529001.txt_037270_0         1.0  \n",
       "3200858  02100101.20210720204138001.txt_095700_0         1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.99 s (started: 2023-02-13 12:13:08 +00:00)\n"
     ]
    }
   ],
   "source": [
    "i# sort by similarity\n",
    "results = results.sort_values(by=['similarity'], ascending=False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "include_colab_link": true,
   "name": "preprocessor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
