{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# SentencePiece Tokenizer\n",
    "\n",
    "![](../figs/deep_nlp/sentencepiece/entelecheia_alphabets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab83d6",
   "metadata": {},
   "source": [
    "## What is SentencePiece?\n",
    "\n",
    "- SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training.\n",
    "- SentencePiece implements **subword units** (e.g., **byte-pair-encoding (BPE)** [[Sennrich et al.](https://www.aclweb.org/anthology/P16-1162)]) and **unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)])\n",
    "with the extension of direct training from raw sentences.\n",
    "- SentencePiece is a pure end-to-end system that does not depend on language-specific pre/postprocessing.\n",
    "- SentencePiece is a general-purpose tokenizer that can be used for any language. \n",
    "\n",
    "To be surprised, SentencePiece is not a tokenizer itself, but a tool to train a tokenizer.\n",
    "\n",
    "- It is a method to select the best subword units from the corpus optimizing the tokenization process.\n",
    "- It implements the Subword Regularization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50659d5",
   "metadata": {},
   "source": [
    "## Technical highlights\n",
    "\n",
    "- **Purely data driven**: SentencePiece trains tokenization and detokenization\n",
    "  models from sentences. Pre-tokenization ([Moses tokenizer](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl)/[MeCab](http://taku910.github.io/mecab/)/[KyTea](http://www.phontron.com/kytea/)) is not always required.\n",
    "- **Language independent**: SentencePiece treats the sentences just as sequences of Unicode characters. There is no language-dependent logic.\n",
    "- **Multiple subword algorithms**: **BPE**  [[Sennrich et al.](https://www.aclweb.org/anthology/P16-1162)] and **unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)] are supported.\n",
    "- **Subword regularization**: SentencePiece implements subword sampling for [subword regularization](https://arxiv.org/abs/1804.10959) and [BPE-dropout](https://arxiv.org/abs/1910.13267) which help to improve the robustness and accuracy of NMT models.\n",
    "- **Fast and lightweight**: Segmentation speed is around 50k sentences/sec, and memory footprint is around 6MB.\n",
    "- **Self-contained**: The same tokenization/detokenization is obtained as long as the same model file is used.\n",
    "- **Direct vocabulary id generation**: SentencePiece manages vocabulary to id mapping and can directly generate vocabulary id sequences from raw sentences.\n",
    "- **NFKC-based normalization**: SentencePiece performs NFKC-based text normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbfbf06",
   "metadata": {},
   "source": [
    "### Comparisons with other implementations\n",
    "\n",
    "| Feature                                     |      SentencePiece       | subword-nmt |    WordPiece    |\n",
    "| :------------------------------------------ | :----------------------: | :---------: | :-------------: |\n",
    "| Supported algorithm                         | BPE, unigram, char, word |     BPE     |      BPE\\*      |\n",
    "| OSS?                                        |           Yes            |     Yes     | Google internal |\n",
    "| Subword regularization                      |           Yes            |     No      |       No        |\n",
    "| Python Library (pip)                        |           Yes            |     No      |       N/A       |\n",
    "| C++ Library                                 |           Yes            |     No      |       N/A       |\n",
    "| Pre-segmentation required?                  |            No            |     Yes     |       Yes       |\n",
    "| Customizable normalization (e.g., NFKC)[Yes |            No            |     N/A     |\n",
    "| Direct id generation                        |           Yes            |     No      |       N/A       |\n",
    "\n",
    "Note that BPE algorithm used in WordPiece is slightly different from the original BPE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e862e",
   "metadata": {},
   "source": [
    "## High level differences between SentencePiece and other tokenizers\n",
    "\n",
    "### The number of unique tokens is predetermined\n",
    "\n",
    "- Neural Machine Translation models typically operate with a fixed vocabulary. \n",
    "- Unlike most unsupervised word segmentation algorithms, which assume an infinite vocabulary, SentencePiece trains the segmentation model such that the final vocabulary size is fixed, e.g., 8k, 16k, or 32k.\n",
    "\n",
    "Note that SentencePiece specifies the final vocabulary size for training, which is different from\n",
    "[subword-nmt](https://github.com/rsennrich/subword-nmt) that uses the number of merge operations.\n",
    "The number of merge operations is a BPE-specific parameter and not applicable to other segmentation algorithms, including unigram, word and character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe3edb",
   "metadata": {},
   "source": [
    "### Trains from raw sentences\n",
    "\n",
    "- Previous sub-word implementations assume that the input sentences are pre-tokenized. \n",
    "- This constraint was required for efficient training, but makes the preprocessing complicated as we have to run language dependent tokenizers in advance.\n",
    "- The implementation of SentencePiece is fast enough to train the model from raw sentences. \n",
    "- This is useful for training the tokenizer and detokenizer for Chinese and Japanese where no explicit spaces exist between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d52ef",
   "metadata": {},
   "source": [
    "### Whitespace is treated as a basic symbol\n",
    "\n",
    "- The first step of Natural Language processing is text tokenization. \n",
    "- For example, a standard English tokenizer would segment the text \"Hello world.\" into the following three tokens.\n",
    "\n",
    "  > [Hello] [World] [.]\n",
    "\n",
    "- One observation is that the original input and tokenized sequence are **NOT reversibly convertible**. \n",
    "- For instance, the information that is no space between “World” and “.” is dropped from the tokenized sequence, since e.g., `Tokenize(“World.”) == Tokenize(“World .”)`\n",
    "\n",
    "- SentencePiece treats the input text just as a sequence of Unicode characters. Whitespace is also handled as a normal symbol. \n",
    "- To handle the whitespace as a basic token explicitly, SentencePiece first escapes the whitespace with a meta symbol \"▁\" (U+2581) as follows.\n",
    "\n",
    "  > Hello▁World.\n",
    "\n",
    "- Then, this text is segmented into small pieces, for example:\n",
    "\n",
    "  > [Hello] [▁Wor] [ld] [.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a53dc",
   "metadata": {},
   "source": [
    "- Since the whitespace is preserved in the segmented text, we can detokenize the text without any ambiguities.\n",
    "\n",
    "```\n",
    "  detokenized = ''.join(pieces).replace('▁', ' ')\n",
    "```\n",
    "\n",
    "- This feature makes it possible to perform detokenization without relying on language-specific resources.\n",
    "\n",
    "Note that we cannot apply the same lossless conversions when splitting the sentence with standard word segmenters, since they treat the whitespace as a special symbol. Tokenized sequences do not preserve the necessary information to restore the original sentence.\n",
    "\n",
    "* (en) Hello world.   → [Hello] [World] [.]   \\(A space between Hello and World\\)\n",
    "* (ja) こんにちは世界。  → [こんにちは] [世界] [。] \\(No space between こんにちは and 世界\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f220d0",
   "metadata": {},
   "source": [
    "### Subword regularization and BPE-dropout\n",
    "\n",
    "- Subword regularization [[Kudo.](https://arxiv.org/abs/1804.10959)] and BPE-dropout [Provilkov et al](https://arxiv.org/abs/1910.13267) are simple regularization methods that virtually augment training data with on-the-fly subword sampling, which helps to improve the accuracy as well as robustness of NMT models.\n",
    "\n",
    "- To enable subword regularization, you would like to integrate SentencePiece library (C++/Python) into the NMT system to sample one segmentation for each parameter update, which is different from the standard off-line data preparations. \n",
    "- You can find that 'New York' is segmented differently on each ``SampleEncode (C++)`` or ``encode with enable_sampling=True (Python)`` calls. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4338591",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece\n",
    "%pip install --pre ekorpkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a21749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=../data/sentencepiece/botchan.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/sentencepiece/botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: ../data/sentencepiece/botchan.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71868 num_tokens=20446 num_tokens/piece=5.21183\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3922 obj=8.66277 num_tokens=20447 num_tokens/piece=5.21341\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2941 obj=8.95617 num_tokens=22741 num_tokens/piece=7.7324\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2941 obj=8.88103 num_tokens=22745 num_tokens/piece=7.73376\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2205 obj=9.26224 num_tokens=25461 num_tokens/piece=11.5469\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2205 obj=9.17719 num_tokens=25457 num_tokens/piece=11.5451\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2200 obj=9.17892 num_tokens=25475 num_tokens/piece=11.5795\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2200 obj=9.17823 num_tokens=25475 num_tokens/piece=11.5795\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    \"--input=../data/sentencepiece/botchan.txt --model_prefix=m --vocab_size=2000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a2ecd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁N', 'e', 'w', '▁Y', 'or', 'k']\n",
      "['▁New', '▁Y', 'or', 'k']\n",
      "['▁New', '▁', 'Y', 'or', 'k']\n",
      "['▁New', '▁Y', 'or', 'k']\n",
      "['▁', 'N', 'e', 'w', '▁Y', 'or', 'k']\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.load(\"m.model\")\n",
    "\n",
    "for n in range(5):\n",
    "    print(\n",
    "        s.encode(\n",
    "            \"New York\", out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ab689",
   "metadata": {},
   "source": [
    "## Subword Regularization\n",
    "\n",
    "- Given a sequence of unigrams $X = (x_1, x_2, \\cdots, x_n)$, the probability of the sequence $X$ is given by the product of the unigram conditional probabilities by the Bayes chain rule:\n",
    "\n",
    "  $$\n",
    "  P(X) = p(x_1) p(x_2 | x_1) \\cdots p(x_n | x_1, \\cdots, x_{n-1}) = \\prod_{i=1}^n p(x_i | x_1, \\cdots, x_{i-1})\n",
    "  $$\n",
    "\n",
    "- In the problem of Neural Machine Translation, the probability of $P(Y|X)$ is given by the product of the conditional probabilities of the target sequence $Y$ given the source sequence $X$:\n",
    "\n",
    "  $$\n",
    "  P(Y|X;\\theta) = \\prod_{i=1}^n P(y_i | \\mathbf{x}, y_{<i}; \\theta)\n",
    "  $$\n",
    "\n",
    "  where the lower case variables represent the actual tokens, and the upper case variables represent the sequence of tokens. $\\theta$ is the model parameter.\n",
    "\n",
    "- This formula is not actually correct, since $X$ and $Y$ can be formed by an exponentailly large number of possible subword sequences.\n",
    "- For example, the workd `hello` can be segmented in a number of ways, e.g., `h e l l o`, `he ll o`, `hel lo`, `hell o`, `hello`.\n",
    "- Therefore, we should replace $X$ and $Y$ on the left with a specific sequence of subwords, $\\mathbf{x}$ and $\\mathbf{y}$, respectively.\n",
    "- The cost function for NMT is given by sum of the expected log-likelihood of the target sequence $\\mathbf{y}$ given the source sequence $\\mathbf{x}$:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}(\\theta) = - \\sum_{s=1}^{|D|} \\mathbb{E}_{\\substack{\\mathbf{x} \\sim P(\\mathbf{x}|X^{(s)}) \\\\ \\mathbf{y} \\sim P(\\mathbf{y}|Y^{(s)})}} \\log P(\\mathbf{y}|\\mathbf{x};\\theta)\n",
    "  $$\n",
    "  \n",
    "- This formula looks intimidating, but it is actually quite simple.\n",
    "- In practice, we can approximate the expected log-likelihood of a single training example $(\\mathbf{x}, \\mathbf{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4ac1b",
   "metadata": {},
   "source": [
    "## How to train SentencePiece\n",
    "\n",
    "- Assume that we have a large collection of bigrams, greater than what we ultimately want to use in our model.\n",
    "- To train a SentencePiece model, we want to maximize the probability of obtaining a particular tokenization $X = (x_1, x_2, \\cdots, x_n)$ of the corpus, given the unigram probabilities $p(x_i), p(x_2), \\cdots, p(x_n)$.\n",
    "- The actual tokenization $X$ is not observed, we only observe the un-tokenized sequence $X$.\n",
    "- This is a classic problem of maximum likelihood estimation, and we can solve it by the EM algorithm.\n",
    "- The problem is that thte $x_i$ are all of different lengths, and we cannot apply the EM algorithm directly.\n",
    "- Instead, we should use a Bayesian approach to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SentencePiece training objective is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = - \\sum_{s=1}^{|D|} \\log P(X^{(s)}) = - \\sum_{s=1}^{|D|} \\log (\\sum_{\\mathbf{x} \\in S(\\mathbf{x})} P(\\mathbf{x}) )\n",
    "$$\n",
    "\n",
    "where the $\\mathbf{x}$ is a unigram sequence, and $S(\\mathbf{x})$ is the set of all possible sequences that can be generated from $\\mathbf{x}$.\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. Initialize the unigram probabilities $p(x_i)$. The frequency of each unigram is used as the initial value.\n",
    "2. M-step: Compute the most likely sequence $\\mathbf{x}$ given the current unigram probabilities $p(x_i)$. \n",
    "3. E-step: Given the current most likely sequence $\\mathbf{x}$, update the unigram probabilities $p(x_i)$. In Bayesian setting, the unigram probabilities are defined as:\n",
    "\n",
    "   $$\n",
    "   p(x_i | \\mathbf{x}) = \\frac{c_i}{\\sum_{j=1}^{|V|} c_j} \\implies \\frac{e^{\\psi(c_i)}}{\\sum_{j=1}^{|V|} e^{\\psi(c_j)}} = \\frac{e^{\\psi(c_i)}}{e^{\\psi(\\sum_{j=1}^{|V|} c_j)}} = \\frac{e^{\\psi(c_i)}}{e^{\\psi(c_1) + \\cdots + \\psi(c_n)}}\n",
    "   $$\n",
    "\n",
    "   where $c_i$ is the frequency of the unigram $x_i$ in the corpus, $|V|$ is the size of the vocabulary, and $\\psi(c_i)$ is the digamma function.\n",
    "4. Repeat the M-step and E-step until the unigram probabilities converge. The log-likelihood is monotonically increasing, so we can stop the training when the log-likelihood does not increase for a certain number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c310d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.base:setting environment variable CACHED_PATH_CACHE_ROOT to /workspace/.cache/cached_path\n",
      "INFO:ekorpkit.base:setting environment variable KMP_DUPLICATE_LIB_OK to TRUE\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "sp = eKonf.instantiate(eKonf.compose(\"model/tokenizer=sentencepiece\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287b2ac",
   "metadata": {},
   "source": [
    "- Initialize the unigram probabilities $p(x_i)$. The frequency of each unigram is used as the initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf8cce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def initialize_vocab(self, texts, initial_vocab_size=None):\n",
      "        if initial_vocab_size is None:\n",
      "            initial_vocab_size = self.initial_vocab_size\n",
      "        text = self.pre_tokenize(\" \".join(texts))\n",
      "        word_freqs = collections.Counter(text.split(self.whitespace_token))\n",
      "        sorted_subwords, characters = self.initialize_subwords(word_freqs)\n",
      "        tokens = (\n",
      "            list(characters.items())\n",
      "            + sorted_subwords[: self.initial_vocab_size - len(characters)]\n",
      "        )\n",
      "        tokens = {token: freq for token, freq in tokens}\n",
      "        tokens = collections.Counter(tokens)\n",
      "        return text, tokens, characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.initialize_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75582211",
   "metadata": {},
   "source": [
    "- M-step: Compute the most likely sequence $\\mathbf{x}$ given the current unigram probabilities $p(x_i)$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd8238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def M_step(self, text, trie):\n",
      "        loss, p = self.forward_step(text, trie)\n",
      "        tokenization = self.backward_step(text, p)\n",
      "        return tokenization, loss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.M_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46c7f4",
   "metadata": {},
   "source": [
    "- E-step: Given the current most likely sequence $\\mathbf{x}$, update the unigram probabilities $p(x_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f7d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def E_step(self, tokenization, trie):\n",
      "        # get the new token counts based on updated tokenization\n",
      "        counts = collections.Counter(tokenization)\n",
      "        norm = sum(list(counts.values()))\n",
      "\n",
      "        # Bayesianify them: https://cs.stanford.edu/~pliang/papers/tutorial-acl2007-talk.pdf\n",
      "        # https://github.com/google/sentencepiece/blob/master/src/unigram_model_trainer.cc\n",
      "        # we are returning the log probabilties here (alpha=0 prior)\n",
      "        logsum = digamma(norm)\n",
      "        for k, v in counts.items():\n",
      "            counts[k] = digamma(v) - logsum\n",
      "\n",
      "        for k, v in counts.items():\n",
      "            trie.set_value(k, v)\n",
      "        return trie\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.E_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b525684",
   "metadata": {},
   "source": [
    "- Repeat the M-step and E-step until the unigram probabilities converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aea9adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def EM_round(self, text, tokens, delta=0.01, max_iter=10):\n",
      "        tokenization, old_loss = self.M_step(text, self.trie)\n",
      "        for step in range(max_iter):\n",
      "            print(f\"EM iter {step}: \", end=\"\")\n",
      "            loss, tokenization, trie = self.EM_step(text, tokenization, self.trie)\n",
      "            print(f\"Loss={loss:.2f}\")\n",
      "            if abs(old_loss - loss) < delta:\n",
      "                break\n",
      "            old_loss = loss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.EM_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03d760",
   "metadata": {},
   "source": [
    "### Finding the optimal segmentation\n",
    "\n",
    "- If all of the subwords were of the same length, we could apply the Viterbi algorithm to find the optimal segmentation.\n",
    "- The Viterbi algorithm is a dynamic programming algorithm that finds the most likely sequence of states, given a sequence of observations and a model of the transition probabilities between states and the emission probabilities of the observations given the states.\n",
    "  - You have some hidden states $z_1, z_2, \\cdots, z_n$, and you want to transition from $z_1 \\rightarrow z_2 \\rightarrow \\cdots \\rightarrow z_n$, and you know the transition matrix $A_{ij}$, giving the probability of transitioning from $z_i^{(1)}$ to $z_j^{(2)}$, where $i$ and $j$ are the hidden states, and the superscript indicates the sequence order.\n",
    "- The problem is that $A$ is not between adjacent states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ab185",
   "metadata": {},
   "source": [
    "- Consider tokenizing the word `hello` given the subwords {`he`, `h`, `ll`, `e`, `o`, `hell`}.\n",
    "- We can generate the following figure:\n",
    "\n",
    "  ![](../figs/deep_nlp/sentencepiece/transition.png)\n",
    "\n",
    "  - Each arrow represents a transition, and the weight of the arrow is the probability of the transition.\n",
    "  - The goal is to pick arrows that we arrive at `<eos>` (end of sequence) with the highest probability.\n",
    "- This problem has optimal substructure, so we can apply dynamic programming to solve it.\n",
    "- For example, assume that we are at the state (4).\n",
    "  - There are three arrows that can lead to the state (4), a red, a blue, and a green arrow.\n",
    "  - The highest probability at the state (4) is just the best path from the previous state, plus the probability of the transition.\n",
    "\n",
    "    $$\n",
    "    p_i = \\max_{j \\le i} (p_j p_{j \\rightarrow i})\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce3d585",
   "metadata": {},
   "source": [
    "- The Trie structure is used to find the optimal segmentation.\n",
    "  - The Trie structure is a tree structure that is used to store a set of strings.\n",
    "  - The following figure shows the Trie structure for the subwords {`h`, `he`, `hell`, `hello`}:\n",
    "\n",
    "    ![](../figs/deep_nlp/sentencepiece/trie.png)\n",
    "\n",
    "  - The root node is the start of the sequence, `<sos>`.\n",
    "  - Any time we encounter an `end` node, it means that everything in the path from `<sos>` to `end` is a valid subword.\n",
    "  - The root node `<sos>` will begin with exactly one branch for every unique first character in the vocabulary.\n",
    "  - As we grow the available subwords, we create more branches.\n",
    "  - The Trie is going to be the fundamental data structure that the tokenizer uses to store and retrieve the subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f13e3b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _initialize_trie(self, tokens):\n",
      "        trie = Trie()\n",
      "        norm = sum(list(tokens.values()))\n",
      "        logsum = digamma(norm)\n",
      "\n",
      "        maxlen = 0\n",
      "        for tok, val in tokens.items():\n",
      "            trie.add(tok, digamma(val) - logsum)\n",
      "            maxlen = max(maxlen, len(tok))\n",
      "\n",
      "        return trie, maxlen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp._initialize_trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011d611",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "- One of the algorithm for finding the optimal sequence from the Trie is a forwards-backwards algorithm.\n",
    "  - It is a special subset of the sum-product algorithm for training directed graphical models.\n",
    "  - More sophisticated algorithms include the Forward-DP Backward-A* algorithm, and the Forward-Filtering and Backward-Sampling algorithm (FFBS).\n",
    "- When we compute the forward step, we also store the length of the longest subword that ends at the current position.\n",
    "- This allows us to backtrack from the end of the sequence to the beginning, and find the optimal segmentation, since the length of the arrow is the length of the subword.\n",
    "- The EM step puts together the E step and the M step, where the E step is updating the Trie, and the M step is finding the optimal segmentation using the forwards-backwards algorithm explained above.\n",
    "- Then, fitting the model is just a matter of repeating the EM step until the log-likelihood converges.\n",
    "- One more thing to consider is to get the desired vocabulary size by pruning the vocabulary.\n",
    "  - First, prepare more subword tokens than the desired vocabulary size.\n",
    "  - After each EM step, remove the least probable, say 10%, of the subwords.\n",
    "  - Repeat this process until the desired vocabulary size is reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c232e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward_step(self, text, trie):\n",
      "        N = len(text)\n",
      "\n",
      "        # d[i] contains the maximum log_prob of any tokenization\n",
      "        # of text[:i], initialized to 0 (i.e. log(0)=-infty)\n",
      "        d = [-np.inf] * (N + 1)\n",
      "\n",
      "        # p[i] (stands for parent) contains the number of characters of\n",
      "        # the final token in the most likely sequence that ends at index i\n",
      "        p = [None] * (N + 1)\n",
      "        d[0] = 0\n",
      "\n",
      "        for i in range(1, N + 1):\n",
      "\n",
      "            # find all possible final words. Have to look back\n",
      "            # a distance set by the length of the longest token\n",
      "            for j in range(max(i - self.maxlen, 0), i):\n",
      "\n",
      "                final_token = text[j:i]\n",
      "                final_value = trie.get_value(final_token)\n",
      "\n",
      "                # if the current ending word has a higher log-probability,\n",
      "                # save that value and store the word (i.e. # chars to backtrack)\n",
      "                if final_value and d[j] + final_value > d[i]:\n",
      "                    d[i] = d[j] + final_value\n",
      "                    p[i] = len(final_token)\n",
      "            if p[i] is None:\n",
      "                raise ValueError(f\"Encountered unknown token '{text[i-1]}'.\")\n",
      "\n",
      "        loss = d[-1]\n",
      "        return loss, p\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.forward_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d14ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def backward_step(self, text, p):\n",
      "        idx = len(p)\n",
      "        tokenization = []\n",
      "        while idx > 1:\n",
      "            # move back the number of steps p tells you to\n",
      "            next_idx = idx - p[idx - 1]\n",
      "\n",
      "            # extract the final token\n",
      "            tok = text[next_idx - 1 : idx - 1]\n",
      "            tokenization.append(tok)\n",
      "\n",
      "            idx = next_idx\n",
      "        tokenization = list(reversed(tokenization))\n",
      "        return tokenization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.backward_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30673c24",
   "metadata": {},
   "source": [
    "### Subword sampling\n",
    "\n",
    "- To find alternative segmentations, we can save the n-best paths in the forward-backward step.\n",
    "- Now, we can sample from the n-best paths to find alternative segmentations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cf20e",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c32d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "cfg = eKonf.compose(\"path\")\n",
    "cfg.cache.uri = \"https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/us_equities_news_sampled.zip\"\n",
    "data = eKonf.load_data(\"us_equities_news_sampled.parquet\", cfg.cached_path)\n",
    "texts = data.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e303efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 subwords: [('▁t', 74287), ('in', 58224), ('th', 57063), ('▁a', 56990), ('▁th', 45693), ('▁s', 43934), ('he', 43345), ('er', 41161), ('re', 40441), ('▁i', 40216)]\n",
      "--- Round 1. Vocab size: 10000 ---\n",
      "EM iter 0: Loss=-5332775.79\n",
      "EM iter 1: Loss=-5312437.48\n",
      "EM iter 2: Loss=-5307440.04\n",
      "EM iter 3: Loss=-5305712.62\n",
      "EM iter 4: Loss=-5305299.00\n",
      "--- Round 2. Vocab size: 8000 ---\n",
      "EM iter 0: Loss=-5462687.88\n",
      "EM iter 1: Loss=-5458428.29\n",
      "EM iter 2: Loss=-5457284.60\n",
      "EM iter 3: Loss=-5456802.94\n",
      "EM iter 4: Loss=-5456598.02\n",
      "--- Round 3. Vocab size: 6400 ---\n",
      "EM iter 0: Loss=-5617511.85\n",
      "EM iter 1: Loss=-5612685.28\n",
      "EM iter 2: Loss=-5611271.82\n",
      "EM iter 3: Loss=-5610959.49\n",
      "EM iter 4: Loss=-5610750.89\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.models.tokenizer.sentencepiece import SentencePieceTokenizer\n",
    "\n",
    "initial_vocab_size=10000\n",
    "vocab_size=6000\n",
    "\n",
    "sp = SentencePieceTokenizer(initial_vocab_size=initial_vocab_size, vocab_size=vocab_size)\n",
    "sp.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a291a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['invest', 'm', 'en', 't', '▁oppo', 'r', 'tun', 'iti', 'es', '▁in', '▁', 't', 'he', '▁comp', 'any']\n",
      "1 ['i', 'nvestm', 'ent', '▁', 'opp', 'o', 'rtun', 'ities', '▁in', '▁', 't', 'he', '▁', 'company']\n",
      "2 ['in', 'vestm', 'ent', '▁oppo', 'rtun', 'it', 'i', 'e', 's', '▁', 'in', '▁th', 'e', '▁c', 'omp', 'any']\n",
      "3 ['i', 'n', 'vestment', '▁o', 'ppo', 'r', 'tun', 'i', 'ties', '▁in', '▁', 't', 'h', 'e', '▁', 'company']\n",
      "4 ['investm', 'ent', '▁', 'o', 'pport', 'uni', 'tie', 's', '▁', 'in', '▁', 'the', '▁', 'com', 'pany']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tokenized_text = sp.tokenize(\"investment opportunities in the company\", nbest_size=5)\n",
    "    print(i, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc3bb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def fit(self, texts, vocab_size=None, delta=0.01, max_iter=5, max_rounds=5):\n",
      "        \"\"\"To turn off pruning, just set max_rounds=1\"\"\"\n",
      "        # text = re.sub(\" \", \"_\", text)\n",
      "        text, tokens, characters = self.initialize_vocab(texts)\n",
      "        if vocab_size is None:\n",
      "            vocab_size = self.vocab_size\n",
      "\n",
      "        if vocab_size > len(tokens):\n",
      "            raise ValueError(\n",
      "                f\"Vocab size is larger than the availble number of tokens {len(tokens)}.\"\n",
      "            )\n",
      "        self.trie, self.maxlen = self._initialize_trie(tokens)\n",
      "        for i in range(1, max_rounds + 1):\n",
      "            print(f\"--- Round {i}. Vocab size: {len(tokens)} ---\")\n",
      "            self.EM_round(text, tokens, delta, max_iter)\n",
      "            if not self.prune_tokens(tokens, characters, vocab_size):\n",
      "                break\n",
      "        self.vocab_size = len(tokens)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [SentencePiece](https://github.com/google/sentencepiece)\n",
    "- [SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c6cd4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
