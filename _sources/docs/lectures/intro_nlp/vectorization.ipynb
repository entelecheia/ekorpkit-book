{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Vector Semantics and Representation\n",
    "\n",
    "![](../figs/intro_nlp/vector/entelecheia_alphabet_letters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## Vector Semantics and Word Embeddings\n",
    "\n",
    "- Lexical semantics is the study of the meaning of words\n",
    "- Distributional hypothesis: words that occur in similar contexts have similar meanings\n",
    "- Sparse vectors: one-hot encoding or bag-of-words\n",
    "- Dense vectors: word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f57ee",
   "metadata": {},
   "source": [
    "### What do words mean, and how do we represent that?\n",
    "\n",
    "> `cassoulet`\n",
    "\n",
    "Do we want to represent that ...\n",
    "\n",
    "- \"cassoulet\" is a French dish?\n",
    "- \"cassoulet\" contains meat and beans?\n",
    "- \"cassoulet\" is a stew?\n",
    "\n",
    "> `bar`\n",
    "\n",
    "Do we want to represent that ...\n",
    "\n",
    "- \"bar\" is a place where you can drink alcohol?\n",
    "- \"bar\" is a long rod?\n",
    "- \"bar\" is to prevent something from moving?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9b973",
   "metadata": {},
   "source": [
    "About words, we can say that ...\n",
    "\n",
    "- Concepts or word senses have a complex many-to-many relationship with words\n",
    "- Words have relations with each other\n",
    "  - Synonyms: \"bar\" and \"pub\"\n",
    "  - Antonyms: \"bar\" and \"open\"\n",
    "  - Similarity: \"bar\" and \"club\"\n",
    "  - Relatedness: \"bar\" and \"restaurant\"\n",
    "  - Superordinate: \"bar\" and \"place\"\n",
    "  - Subordinate: \"bar\" and \"pub\"\n",
    "  - Connotation: \"bar\" and \"prison\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf9cd3",
   "metadata": {},
   "source": [
    "### Different approaches to lexical semantics\n",
    "\n",
    "NLP draws on two different approaches to lexical semantics:\n",
    "\n",
    "- **Lexical semantics**: \n",
    "  - The study of the meaning of words\n",
    "  - The lexicographic tradition aims to capture the information represented in lexical entries in dictionaries\n",
    "- **Distributional semantics**: \n",
    "  - The study of the meaning of words based on their distributional properties in large corpora\n",
    "  - The distributional hypothesis: words that occur in similar contexts have similar meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55701de7",
   "metadata": {},
   "source": [
    "#### Lexical semantics\n",
    "\n",
    "- Uses resources such as `lexicons`, `thesauri`, `ontologies` etc. that capture explicit knowledge about word meanings.\n",
    "- Assumes that words have `discrete word senses` that can be represented in a `lexicon`.\n",
    "  - bank 1 = a financial institution\n",
    "  - bank 2 = a river bank\n",
    "- May capture explicit knowledge about word meanings, but is limited in its ability to capture the meaning of words that are not in the lexicon.\n",
    "  -  `dog` is a `canine` (lexicon)\n",
    "  -  `cars` have `wheels` (lexicon)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d7870",
   "metadata": {},
   "source": [
    "#### Distributional semantics\n",
    "\n",
    "- Uses `large corpora of raw text` to learn the meaning of words from the contexts in which they occur.\n",
    "- Maps words to `vector representations` that capture the `distributional properties` of the words in the corpus.\n",
    "- Uses neural networks to learn the dense vector representations of words, `word embeddings`, from large corpora of raw text.\n",
    "- If each word is mapped to a single vector, this ignores the fact that words can have multiple meanings or parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c71ae",
   "metadata": {},
   "source": [
    "### How do we represent words to capture word similarities?\n",
    "\n",
    "- As `atomic symbols`\n",
    "  - in a traditional n-gram language model\n",
    "  - explicit features in a machine learning model\n",
    "  - this is equivalent to very high-dimensional one-hot vectors:\n",
    "    - aardvark = [1,0,...,0], bear = [0,1,...,0], ..., zebra = [0,0,...,1]\n",
    "    - height and tall are as different as aardvark and zebra\n",
    "- As very high-dimensional `sparse vectors`\n",
    "  - to capture the distributional properties of words\n",
    "- As low-dimensional `dense vectors`\n",
    "  - word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943297e",
   "metadata": {},
   "source": [
    "### What should word representations capture?\n",
    "\n",
    "- Vector representations of words were originally used to capture `lexical semantics` so that words with similar meanings would be represented by vectors that are close together in vector space.\n",
    "- These representations may also capture some `morphological` and `syntactic` information about words. (part of speech, inflections, stems, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04d66b",
   "metadata": {},
   "source": [
    "#### The Distributional Hypothesis\n",
    "\n",
    "Zellig Harris (1954):\n",
    "- Words that occur in similar contexts have similar meanings.\n",
    "- `oculist` and `eye doctor` occur in almost the same contexts\n",
    "- If A and B have almost the same environment, then A and B are synonymous.\n",
    "\n",
    "John Firth (1957):\n",
    "- You shall know a word by the company it keeps.\n",
    "\n",
    "> The `contexts` in which words occur tell us a lot about the meaning of words.\n",
    "> \n",
    "> Words that occur in similar contexts have similar meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad362e4",
   "metadata": {},
   "source": [
    "#### Why do we care about word contexts?\n",
    "\n",
    "What is `tezgüino`?\n",
    "\n",
    "- A bottle of `tezgüino` is on the table.\n",
    "- Everybody likes `tezgüino`.\n",
    "- `Tezgüino` makes you drunk.\n",
    "- We make `tezgüino` out of corn.\n",
    "\n",
    "We don't know what `tezgüino` is, but we can guess that it is a drink because we understand these sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f6e92",
   "metadata": {},
   "source": [
    "If we have the following sentences:\n",
    "\n",
    "- A bottle of `wine` is on the table.\n",
    "- There is a `beer` bottle on the table\n",
    "- `Beer` makes you drunk.\n",
    "- We make `bourbon` out of corn.\n",
    "- Everybody likes `chocolate`\n",
    "- Everybody likes `babies`\n",
    "\n",
    "Could we guess that `tezgüino` is a drink like `wine` or `beer`?\n",
    "\n",
    "However, there are also red herrings:\n",
    "- Everybody likes `babies`\n",
    "- Everybody likes `chocolate`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca22e33",
   "metadata": {},
   "source": [
    "### Two ways NLP uses context for semantics\n",
    "\n",
    "`Distributional similarity`: (vector-space semantics)\n",
    "- Assume that words that occur in similar contexts have similar meanings.\n",
    "- Use the `set of all contexts` in which a word occurs to measure the `similarity` between words.\n",
    "\n",
    "`Word sense disambiguation`:\n",
    "- Assume that if a word has multiple meanings, then it will occur in different contexts for each meaning.\n",
    "- Use the context of a particular occurrence of a word to identify the `sense` of the word in that context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abcaaa",
   "metadata": {},
   "source": [
    "## Distributional Similarity\n",
    "\n",
    "### Basic idea\n",
    "\n",
    "- Measure the semantic `similarities of words` by measuring the `similarity of their contexts` in which they occur\n",
    "\n",
    "### How?\n",
    "\n",
    "- Represent words as `sparse vectors` such that:\n",
    "  - each `vector element` (dimension) represents a different `context`\n",
    "  - the `value` of each element is the `frequency` of the context in which the word occurs, capturing how `strongly` the word is associated with that context\n",
    "- Compute the `semantic similarity of words` by measuring the `similarity of their context vectors`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d1978",
   "metadata": {},
   "source": [
    "Distributional similarities represent each word $w$ as a vector $v_w$ of context counts:\n",
    "\n",
    "$$w = (w_1 , \\ldots , w_N ) \\in R^N$$\n",
    "\n",
    "in a vector space $R^N$ where $N$ is the number of contexts.\n",
    "\n",
    "- each dimension $i$ represents a different context $c_i$\n",
    "- each element $v_{w,i}$ captures how strongly $w$ is associated with context $c_i$\n",
    "- $v_{w,i}$ is the co-occurrence count of $w$ and $c_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6149211",
   "metadata": {},
   "source": [
    "### The Information Retrieval perspective: The Term-Document Matrix\n",
    "\n",
    "In information retrieval, we search a collection of $N$ documents for $M$ terms:\n",
    "\n",
    "- We can represent each `word` in the vocabulary $V$ as an $N$-dimensional vector $v_w$ where $v_{w,i}$ is the `frequency` of the word $w$ in document $i$.\n",
    "- Conversely, we can represent each `document` as an $M$-dimensional vector $v_d$ where $v_{d,j}$ is the `frequency` of the term $t_j$ in document $d$.\n",
    "\n",
    "Finding the `most relevant` documents for a query $q$ is equivalent to finding the `most similar` documents to the query vector $v_q$.\n",
    "- Queries are also documents, so we can use the same vector representation for queries and documents.\n",
    "- Use the similarity of the query vector $v_q$ to the document vectors $v_d$ to rank the documents.\n",
    "- Documents are similar to queries if they have similar terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4f370",
   "metadata": {},
   "source": [
    "## Term-Document Matrix\n",
    "\n",
    "![](../figs/intro_nlp/vector/2.png)\n",
    "\n",
    "A term-document matrix is a 2D matrix:\n",
    "- each row represents a `term` in the vocabulary\n",
    "- each column represents a `document`\n",
    "- each element is the `frequency` of the term in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c953ff",
   "metadata": {},
   "source": [
    "![](../figs/intro_nlp/vector/3.png)\n",
    "\n",
    "- Each `column vector` = a `document`\n",
    "  - Each entry = the `frequency` of the term in the document\n",
    "- Each `row vector` = a `term`\n",
    "  - Each entry = the `frequency` of the term in the document\n",
    "\n",
    "> Two documents are similar if their vectors are similar.\n",
    "\n",
    "> Two words are similar if their vectors are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851ffa1",
   "metadata": {},
   "source": [
    "For information retrieval, the term-document matrix is useful because it allows us to represent documents as vectors and compute the similarity between documents in terms of the words they contain, or of terms in terms of the documents they occur in.\n",
    "\n",
    "We can adapt this idea to implement `a model of the distributional hypothesis` if we treat each context as a column in the matrix and each word as a row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df58ee8",
   "metadata": {},
   "source": [
    "### What is a `context`?\n",
    "\n",
    "There are many ways to define a context:\n",
    "\n",
    "**Contexts defined by nearby words:**\n",
    "- How often does the word $w_i$ occur within a window of $k$ words of the word $w_j$?\n",
    "- Or, how often do the words $w_i$ and $w_j$ occur in the same document or sentence?\n",
    "- This yields fairly broad thematic similarities between words.\n",
    "\n",
    "**Contexts defined by `grammtical relations`:**\n",
    "- How often does the word $w_i$ occur as the `subject` of the word $w_j$?\n",
    "- This requires a `grammatical parser` to identify the grammatical relations between words.\n",
    "- This yields more `fine-grained` similarities between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a8253",
   "metadata": {},
   "source": [
    "### Using nearby words as contexts\n",
    "\n",
    "1. Define a fixed vocabulary of $N$ context words $c_1 , \\ldots , c_N$\n",
    "\n",
    "- Contexts words should occur frequently enough in the corpus that you can get reliable counts.\n",
    "- However, you should ignore very frequent words (stopwords) like `the` and `a` because they are not very informative.\n",
    "\n",
    "2. Define what `nearby` means:\n",
    "\n",
    "- For example, we can define a `window` of $k$ words on either side of the word $w_j$.\n",
    "\n",
    "2. Count the number of times each context word $c_i$ occurs within a window of $k$ words of the word $w_j$.\n",
    "3. Define how to transform the co-occurrence counts into a vector representation of the word $w_j$.\n",
    "\n",
    "- For example, we can use the (positive) `PMI` of the word $w_j$ and the context word $c_i$.\n",
    "\n",
    "4. Compute the similarity between words by measuring the similarity of their context vectors.\n",
    "\n",
    "- For example, we can use the cosine similarity of the context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70deb7",
   "metadata": {},
   "source": [
    "### Word-Word Matrix\n",
    "\n",
    "![](../figs/intro_nlp/vector/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca864a53",
   "metadata": {},
   "source": [
    "Resulting word-word matrix:\n",
    "\n",
    "- $f(w, c)$ = how often does word w appear in context $c$:\n",
    "- `information` appeared six times in the context of `data`\n",
    "\n",
    "![](../figs/intro_nlp/vector/5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620e8ad",
   "metadata": {},
   "source": [
    "### Defining co-occurrences:\n",
    "\n",
    "- `Within a fixed window`: $c_i$ occurs within $\\pm n$ words of $w$\n",
    "- `Within the same sentence`: requires sentence boundaries\n",
    "- `By grammatical relations`: $c_i$ occurs as a subject/object/modifier/… of verb $w$ (requires parsing - and separate features for each relation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff002ea",
   "metadata": {},
   "source": [
    "### Representing co-occurrences:\n",
    "\n",
    "- $f_i$ as` binary features` (1,0): $w$ does/does not occur with $c_i$\n",
    "- $f_i$ as frequencies: $w$ occurs $n$ times with $c_i$\n",
    "- $f_i$ as probabilities: e.g. $f_i$ is the probability that $c_i$ is the subject of $w$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2305c7",
   "metadata": {},
   "source": [
    "### Getting co-occurrence counts\n",
    "\n",
    "Co-occurrence as a `binary` feature:\n",
    "- Does word $w$ ever appear in the context $c$? (1 = yes/0 = no)\n",
    "\n",
    "![](../figs/intro_nlp/vector/6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680df91",
   "metadata": {},
   "source": [
    "Co-occurrence as a frequency count:\n",
    "- How often does word $w$ appear in the context $c$? (0,1,2,… times)\n",
    "\n",
    "![](../figs/intro_nlp/vector/7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c704738",
   "metadata": {},
   "source": [
    "### Counts vs PMI\n",
    "\n",
    "Sometimes, low co-occurrences counts are very informative, and high co-occurrence counts are not:\n",
    "\n",
    "- Any word is going to have relatively high co-occurrence counts with very common contexts (e.g. `the` with `a`), but this won’t tell us much about what that word means.\n",
    "- We need to identify when co-occurrence counts are higher than we would expect by chance.\n",
    "\n",
    "We can use pointwise mutual information (PMI) values instead of raw frequency counts:\n",
    "\n",
    "$$ PMI(w,c) = \\log \\frac{p(w,c)}{p(w)p(c)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b1fcf",
   "metadata": {},
   "source": [
    "![](../figs/intro_nlp/vector/8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d338c",
   "metadata": {},
   "source": [
    "### Computing PMI of $w$ and $c$:\n",
    "\n",
    "#### Using a fixed window of $\\pm k$ words\n",
    "\n",
    "- $N$: How many tokens does the corpus contain?\n",
    "- $f(w) \\le N$: How often does $w$ occur?\n",
    "- $f(w, c) \\le f(w)$: How often does $w$ occur with $c$ in its window?\n",
    "- $f(c) = \\sum_w f(w, c)$: How many tokens have $c$ in their window?\n",
    "\n",
    "$$p(w) = \\frac{f{w}}{N}, p(c) = \\frac{f(c)}{N}, p(w,c) = \\frac{f(w,c)}{N}$$\n",
    "\n",
    "$$PMI(w,c) = \\log \\frac{p(w,c)}{p(w)p(c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef35b5",
   "metadata": {},
   "source": [
    "Positive Pointwise Mutual Information\n",
    "\n",
    "PMI is negative when words co-occur less than expected by chance.\n",
    "\n",
    "- This is unreliable without huge corpora:\n",
    "- With $P(w ) \\approx P(w2 ) \\approx 10^{−6}$ , we can’t estimate whether $P(w_1 , w_2 )$ is significantly different from $10^{−12}$\n",
    "\n",
    "We often just use positive PMI values, and replace all negative PMI values with 0:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050f869",
   "metadata": {},
   "source": [
    "Positive Pointwise Mutual Information (PPMI):\n",
    "\n",
    "$$\n",
    "\\text{PPMI}(w, c) = PMI, \\text{ if } \\text{PMI}(w, c) \\gt 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PPMI}(w, c) = 0, \\text{ if } \\text{PMI}(w, c) \\le 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f78fb4",
   "metadata": {},
   "source": [
    "PMI and smoothing\n",
    "\n",
    "PMI is biased towards infrequent events:\n",
    "\n",
    "- If $P(w, c) = P(w) = P(c)$, then $\\text{PMI}(w, c) = \\log (\\frac{1}{P(w)})$\n",
    "- So $\\text{PMI}(w, c)$ is larger for rare words $w$ with low $P(w)$.\n",
    "\n",
    "Simple remedy: `Add-k smoothing` of $P(w, c), P(w), P(c)$ pushes all PMI values towards zero.\n",
    "\n",
    "- Add-k smoothing affects low-probability events more, and will therefore reduce the bias of PMI towards infrequent events. (Pantel & Turney 2010)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db24877",
   "metadata": {},
   "source": [
    "## Dot product as similarity\n",
    "\n",
    "If the vectors consist of simple binary features (0,1), we can use the `dot product` as `similarity metric`:\n",
    "\n",
    "$$\n",
    "sim_{dot-prod}(\\vec{x}\\cdot\\vec{y}) = \\sum_{i=1}^{N} x_i \\times y_i\n",
    "$$\n",
    "\n",
    "The dot product is a bad metric if the vector elements are arbitrary features: it prefers `long` vectors\n",
    "\n",
    "- If one $x_i$ is very large (and $y_i$ nonzero), $sim(x, y)$ gets very large\n",
    "- If the number of nonzero $x_i$ and $y_i$ is very large, $sim(x, y)$ gets very large.\n",
    "- Both can happen with frequent words.\n",
    "\n",
    "$$\n",
    "\\text{length of }\\vec{x}: |\\vec{x}|=\\sqrt{\\sum_{i=1}^{N}x_i^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c84803",
   "metadata": {},
   "source": [
    "## Vector similarity: Cosine\n",
    "\n",
    "One way to define the similarity of two vectors is to use the cosine of their angle.\n",
    "\n",
    "The cosine of two vectors is their dot product, divided by the product of their lengths:\n",
    "\n",
    "$$\n",
    "sim_{cos}(\\vec{x},\\vec{y})=\\frac{\\sum_{i=1}^{N} x_i \\times y_i}{\\sqrt{\\sum_{i=1}^{N}x_i^2}\\sqrt{\\sum_{i=1}^{N}y_i^2}} = \\frac{\\vec{x}\\cdot\\vec{y}}{|\\vec{x}||\\vec{y}|}\n",
    "$$\n",
    "\n",
    "> $sim(\\mathbf{w}, \\mathbf{u}) = 1$: $\\mathbf{w}$ and $\\mathbf{u}$ point in the same direction\n",
    "\n",
    "> $sim(\\mathbf{w}, \\mathbf{u}) = 0$: $\\mathbf{w}$ and $\\mathbf{u}$ are orthogonal\n",
    "\n",
    "> $sim(\\mathbf{w}, \\mathbf{u}) = -1$: $\\mathbf{w}$ and $\\mathbf{u}$ point in the opposite direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe129c03",
   "metadata": {},
   "source": [
    "### Visualizing cosines\n",
    "\n",
    "![](../figs/intro_nlp/vector/cosine-viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d08c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
