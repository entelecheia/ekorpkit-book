{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "![](../figs/intro_nlp/embeddings/embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a8744",
   "metadata": {},
   "source": [
    "## What are word embeddings?\n",
    "\n",
    "Word embeddings are a way of representing words as vectors. The vectors are learned from text data and are able to capture some of the semantic and systactic information of the words. \n",
    "\n",
    "For example, the word`cat` is similar to `doc` from the following sentences:\n",
    "\n",
    "\"The cat is lying on the floor and the dog was eating”,\n",
    " \n",
    "\"The doc was lying on the floor and the cat was eating”\n",
    "\n",
    "In a mathematical sense, a word embedding is a parameterized function of the word:\n",
    "\n",
    "$$ f_{\\theta}(w) = \\theta $$\n",
    "\n",
    "where $\\theta$ is a vector of real numbers. The vector $\\theta$ is the embedding of the word $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd7391",
   "metadata": {},
   "source": [
    "In a broad sense, `embedding` refers to a lower-dimensional dense vector representation of a higher-dimensional object.\n",
    "  - in NLP, this higher-dimensional object will be a document.\n",
    "  - in computer vision, this higher-dimensional object will be an image.\n",
    "\n",
    "Examples of embeddings and non-embeddings:\n",
    "\n",
    "  - **Non-embeddings**:\n",
    "    - one-hot encoding, bag-of-words, TF-IDF, etc.\n",
    "    - counts over LIWC dictionary categories.\n",
    "    - sklearn CountVectorizer count vectors\n",
    "  - **Embeddings**:\n",
    "    - word2vec, GloVe, BERT, ELMo, etc.\n",
    "    - PCA reductions of the word count vectors\n",
    "    - LDA topic shares\n",
    "    - compressed encodings from an autoencoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee767b61",
   "metadata": {},
   "source": [
    "## Categorical Embeddings\n",
    "\n",
    "![](../figs/intro_nlp/embeddings/1.png)\n",
    "\n",
    "Categorical embeddings are a way of representing categorical variables as vectors.\n",
    "\n",
    "For a binary classification problem with outcome $Y$:\n",
    "    - If you have a high-dimensional categorical variable $X$, (e.g. 1000 categories), you can represent $X$ as a vector of length 1000.\n",
    "    - It is computationally expensive for a ML model to learn from a high-dimensional categorical variable.\n",
    "\n",
    "Instead, you can represent $X$ as a lower-dimensional vector of length $k$ (e.g. 10). This is called a categorical embedding. \n",
    "\n",
    "Embedding approaches:\n",
    "\n",
    "1. PCA applied to the dummy variables $X$ to get a lower-dimensional vector representation of $\\tilde{X}$.\n",
    "2. Regress $Y$ on $X$, predict $\\hat{Y}(X_i)$, use that as a feature in a new model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec2a71",
   "metadata": {},
   "source": [
    "### An embedding layer is matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\underbrace{h_1}_{n_E \\times 1} = \\underbrace{\\omega_E}_{n_E \\times n_W} \\cdot \\underbrace{x}_{n_x \\times 1} \n",
    "$$\n",
    "\n",
    "- $x$ = a categorical variable (e.g., representing a word)\n",
    "  - One-hot vector with a single item equaling one.\n",
    "  - Input to the embedding layer.\n",
    "- $h_1$ = the first hidden layer of the neural net\n",
    "  - The output of the embedding layer.\n",
    "- The embedding matrix $\\omega_E$ encodes predictive information about the categories.\n",
    "- It has a spatial interpretation when projected into 2D space.\n",
    "  - Each row of $\\omega_E$ is a vector in $n_E$-dimensional space.\n",
    "  - The rows of $\\omega_E$ are the coordinates of the points in the vector space.\n",
    "  - The points are the categories.\n",
    "  - The distance between the points is the similarity between the categories.\n",
    "  - The angle between the points is the relationship between the categories.\n",
    "\n",
    "### Embedding Layers versus Dense Layers\n",
    "\n",
    "An embedding layer is statistically equivalent to a fully-connected dense layer with one-hot vectors as input and linear activation.\n",
    "\n",
    "- Embedding layers are much faster for many categories (>~50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbcb02",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "> Word embeddings are neural network layers that map words to dense vectors.\n",
    "\n",
    "\n",
    "Documents are lists of word indexes ${w_1 ,w_2 ,...,w_{n_i} }$.\n",
    "\n",
    "- Let $w_i$ be a one-hot vector (dimensionality $n_w$ = vocab size) where the associated word’s index equals one.\n",
    "- Normalize all documents to the same length L; shorter documents can be padded with a null token.\n",
    "- This requirement can be relaxed with recurrent neural networks.\n",
    "\n",
    "The embedding layer replaces the list of sparse one-hot vectors with a list of n E -dimensional ($n_E$ << $n_w$ ) dense vectors\n",
    "\n",
    "$$ \\mathbf{X} = [x_1 \\ldots x_L ] $$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\underbrace{x_j}_{n_E \\times 1} = \\underbrace{\\mathbf{E}}_{n_E \\times n_W} \\cdot \\underbrace{w_j}_{n_w \\times 1}\n",
    "$$\n",
    "\n",
    "$\\mathbf{E}$ a matrix of word vectors. The column associated with the word at $j$ is selected by the dot-product with one-hot vector $w_j$.\n",
    "\n",
    "$\\mathbf{X}$ is flattened into an $L * n_E$ vector for input to the next layer.\n",
    "\n",
    "\n",
    "![](../figs/intro_nlp/embeddings/4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d52051",
   "metadata": {},
   "source": [
    "### Why do we need neural networks for word embeddings?\n",
    "\n",
    "There are a lot of shallow algorithms that work well for clustering.\n",
    "- k-means\n",
    "- hierarchical clustering\n",
    "- spectral clustering\n",
    "- PCA\n",
    "\n",
    "The reasons we use neural networks for word embeddings are:\n",
    "- They are able to learn the relationships between words.\n",
    "- They can be used as input to a downstream task.\n",
    "- They create a mapping of discrete words to continuous vectors.\n",
    "- They solve the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16570c80",
   "metadata": {},
   "source": [
    "## Neural Language Models\n",
    "\n",
    "Word embeddings were proposed by {cite}`bengio2003neural` as a way to represent words as vectors.\n",
    "\n",
    "Bengio’s method could train a neural network such that each training sentence could inform the model about a number of semantically available neighboring words, which was known as `distributed representation of words`. The nueural network preserved relationships between words in terms of their contexts (semantic and syntactic).\n",
    "\n",
    "![](../figs/intro_nlp/embeddings/bengio.png)\n",
    "\n",
    "\n",
    "This introduced a neural network architecture approach that laid the foundation for many current approaches. \n",
    "\n",
    "This neural network has three components:\n",
    "- **Embedding layer**: maps words to vectors, the parameters are shared across the network.\n",
    "- **Hidden layer**: a fully connected layer with a non-linear activation function.\n",
    "- **Output layer**: produces a probability distribution over the vocabulary using a softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d5820",
   "metadata": {},
   "source": [
    "### Step 1: Indexing the words. \n",
    "\n",
    "For each word in the sentence, we assign an index.\n",
    "\n",
    "```python\n",
    "word_list = \" \".join(raw_sentence).split()\n",
    "word_list = list(set(word_list))\n",
    "word2id = {w: i for i, w in enumerate(word_list)}\n",
    "id2word = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word2id)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676257f",
   "metadata": {},
   "source": [
    "### Step 2: Building the model.\n",
    "\n",
    "```python\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(n_class, m) #embedding layer or look up table\n",
    "\n",
    "        self.hidden1 = nn.Linear(n_step * m, n_hidden, bias=False)\n",
    "        self.ones = nn.Parameter(torch.ones(n_hidden))\n",
    "\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_class, bias=False)\n",
    "        self.hidden3 = nn.Linear(n_step * m, n_class, bias=False) #final layer\n",
    "\n",
    "        self.bias = nn.Parameter(torch.ones(n_class))\n",
    "\n",
    "    def forward(self, X):\n",
    "        word_embeds = self.embeddings(X) # embeddings\n",
    "        X = word_embeds.view(-1, n_step * m) # first layer\n",
    "        tanh = torch.tanh(self.ones + self.hidden1(X)) # tanh layer\n",
    "        output = self.bias + self.hidden3(X) + self.hidden2(tanh) # summing up all the layers with bias\n",
    "        return word_embeds, output\n",
    "```\n",
    "\n",
    "- An embedding layer is a lookup table that maps each word to a vector.\n",
    "- Once the input index of the word is embedded, it is passed through the first hidden layer with bias added to it.\n",
    "- The output of the first hidden layer is passed through a tanh activation function.\n",
    "- The output from the embedding layer is also passed through the final layer where the output of the tanh layer is added to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbc857",
   "metadata": {},
   "source": [
    "### Step 3: Loss and optimization function.\n",
    "\n",
    "Now that we have the model, we need to define the loss function and the optimization function.\n",
    "\n",
    "We are using the cross-entropy loss function and the Adam optimizer.\n",
    "\n",
    "The cross-entropy loss function is made up of two parts:\n",
    "- The softmax function: this is used to normalize the output of the model so that the sum of the probabilities of all the words in the vocabulary is equal to one.\n",
    "- The negative log-likelihood: this is used to calculate the loss.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "model = NNLM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba837b38",
   "metadata": {},
   "source": [
    "### Step 4: Training the model.\n",
    "\n",
    "Finally, we train the model.\n",
    "\n",
    "\n",
    "```python\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    embeddings, output = model(input_batch)\n",
    "\n",
    "    # output : [batch_size, n_class], target_batch : [batch_size]\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predict\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "\n",
    "# Test\n",
    "print([sen.split()[:2] for sen in raw_sentence], '->', [id2word[n.item()] for n in predict.squeeze()])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60aa00",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Word embeddings are a way to represent words as low-dimensional dense vectors.\n",
    "- These embeddings have associated learnable vectors, which optimize themselves through back propagation. \n",
    "- Essentially, the embedding layer is the first layer of a neural network.\n",
    "- They try to preserve the semantic and syntactic relationships between words.\n",
    "\n",
    "![](../figs/intro_nlp/embeddings/w2v.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e7bc0",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec is a neural network architecture that was proposed by {cite}`mikolov2013distributed` in 2013. It is a shallow, two-layer neural network that is trained to reconstruct linguistic contexts of words.\n",
    "\n",
    "The problem of the previous neural network is that it is computationally expensive to train. The hidden layer computes probability distribution for all the words in the vocabulary. This is because the output layer is a fully connected layer.\n",
    "\n",
    "Word2Vec solves this problem by removing hidden layers and sharing the projection layer for all the words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2433c",
   "metadata": {},
   "source": [
    "### Main idea\n",
    "\n",
    "- Use a `binary classifier` to predict which words appear in the context of (i.e. near) a target word.\n",
    "- The `parameters of that classifier` provide a dense vector representation of the target word (embedding).\n",
    "- Words that appear in similar contexts (that have high distributional similarity) will have very similar vector representations.\n",
    "- These models can be trained on large amounts of raw text (and pre-trained embeddings can be downloaded).\n",
    "\n",
    "### Two models\n",
    "\n",
    "- **Continuous Bag of Words (CBOW)**: predicts the target word from the context words.\n",
    "- **Skip-gram**: predicts the context words from the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24d947",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    "- [A Visual Guide to FastText Word Embeddings](https://amitness.com/2020/06/fasttext-embeddings/)\n",
    "- [FastText](https://fasttext.cc/)\n",
    "- [Get FastText representation from pretrained embeddings with subword information](http://christopher5106.github.io/deep/learning/2020/04/02/fasttext_pretrained_embeddings_subword_word_representations.html)\n",
    "- [The Ultimate Guide to Word Embeddings](https://neptune.ai/blog/word-embeddings-guide)\n",
    "- [Neural Network Language Model.ipynb](https://colab.research.google.com/drive/12TQ4CmY6jUnFlQZFnKenmKL3UdTkcatx?usp=sharing#scrollTo=bxwcGfO8eI6G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab5401",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
