{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc871e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Zero Shot, Prompt, and Search Strategies\n",
    "\n",
    "![bloom](../figs/aiart/entelecheia_Zero_Shot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3e881",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Zero Shot and Few Shot Learners\n",
    "\n",
    "![prompt](../figs/aiart/entelecheia_a_robot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46fccc-6d3d-497d-bf93-197e83b64db9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Large Language Models can show good enough performance for some tasks based on just a few examples.\n",
    "These examples are called `prompts` to a language model.\n",
    "\n",
    "For clarity, we will define a prompting task as one that requires no fine-tuning to the base language model. \n",
    "This is done by inputting some prompts into the language model and asking it to return a response. \n",
    "The model does not see any training data for this task and is expected to generalize from these few examples. \n",
    "\n",
    "Formatting the examples as input is referred to as `prompt engineering` and is a process that comes with some trial and error.\n",
    "The goal of prompt engineering is to take your prompts and format them in a way, so they are easy to input into the model.\n",
    "\n",
    "Language generation based on prompts is a brilliant concept, and it can be done in two ways, mainly - Zero Shot predictions and Few Shot predictions. \n",
    "Zero-Shot prediction is where the model is not trained on any data for that specific task, and Few Shot predictions are where the model is trained on a very few amount of data for that specific task. \n",
    "In both cases, we need some sort of prompt or seed text to get started with so that the model can generate new text from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3564e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Prompting on LLMs\n",
    "\n",
    " All you need to do is input several examples of your prompts into the `generate` function on either `gpt2` or `gpt3`. You don't even have to specify what kind of task it is, just give it some example inputs and let it figure out how to generalize from there! The only parameters required are:\n",
    "\n",
    " - `text`, which contains the text you want to be generated for (this should be your prompt set)  \n",
    " - `length`, which specifies how long you want each generated sequence returned by the model   \n",
    " - `num_return_sequences`, which specifies how many sequences you want to be returned by the model\n",
    "\n",
    "In Zero-shot predictions, you mainly pass prompts which give a task description to the LLM to generate text. For example, for zero-shot summarization, you can present a body of text to the LLM along with an instruction for it to follow, like 'In summary', or 'tldr:', or even 'To explain to a 5-year-old'.\n",
    "\n",
    "In Few-Shot summarization, you can preset a few examples of text & their summary to an LLM. You can then present a text to the model and could expect the summary generated by the model. In other words, you give it a few examples vs. none.\n",
    "Language generation based on prompts is a brilliant concept and it is so much simpler than fine-tuning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e04ef0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Zero-shot \n",
    "\n",
    "The model predicts the answer when provided only a description of the task. \n",
    "No gradient updates are performed on the model. \n",
    "\n",
    "- prompt => Translate English to French: (This is the task description)\n",
    "- Cheese => (this is you prompting the LLM to complete the sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba726bed-e6e5-4698-b8e2-2c8e12026a31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### One-shot\n",
    "\n",
    "In addition to task description, you provide the model with one example of what you are expecting it to produce. \n",
    "\n",
    "- prompt => Translate English to French: (Task description for the model)\n",
    "- Sea Otter => loutre de mer (One example for the model to learn from)\n",
    "- Cheese => (providing a prompt to LLM to follow the lead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76c6c3-fb25-4a33-a078-edfad32319c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Few-shot\n",
    "\n",
    "On addition to task description, the model is provided with a few examples of the task. \n",
    "\n",
    "- prompt => Translate English to French: (Task description for the model)\n",
    "- Sea Otter => loutre de mer (a few examples for the model to learn from)\n",
    "- Plush girafe => girafe poivree\n",
    "- Cheese => (providing a prompt to LLM to follow the lead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b604eb",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# If you run this notebook in Colab, set Hardware accelerator to GPU.\n",
    "# Then, install transformers\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86e4aefc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilgpt2 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Using pad_token, but it is not set yet.\n",
      "Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'One of the hottest areas of investing in recent years has been ESG',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.3732561469078064, 0.3641113340854645, 0.26263248920440674]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"distilgpt2\")\n",
    "classifier(\n",
    "    \"One of the hottest areas of investing in recent years has been ESG\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a90025",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Zero Shot Reasoners and Chain-of-Thought\n",
    "\n",
    "The [paper](https://arxiv.org/abs/2205.11916) from University of Tokyo and Google Brain team suggests that LLMs have fundamental zero-shot capabilities in high-level broad cognitive tasks and that these capabilities can be extracted by simple Chain-of-Thought (or CoT) prompting.\n",
    "\n",
    "Another [paper](https://arxiv.org/abs/2201.11903) by Google Brain team has further investigated the CoT prompting. They noted that by generating a chain-of-thought (or a series of intermediate reasoning steps) LLMs significantly improve their ability to perform complex reasoning. Their experiments on three large language models have shown that chain-of-thought prompting improves performance on a range of arithmetic, common sense, and symbolic reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12915a7-c602-4aa9-b906-2e9d00919740",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "One exmaple:\n",
    "\n",
    "- Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis  balls. How many tennis balls does he have now? \n",
    "\n",
    "- A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Additionally: \n",
    "\n",
    "- Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have\n",
    "\n",
    "- A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.\n",
    "\n",
    "> Chain of thought reasoning allows models to decompose complex problems into intermediate steps that are solved individually. Moreover, the language-based nature of chain of thought makes it applicable to any task that a person could solve via language. We find through empirical experiments that chain of thought prompting can improve performance on various reasoning tasks, and that successful chain of thought reasoning is an emergent property of model scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e5692",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Decoding / search strategies\n",
    "\n",
    "[How to generate text](https://huggingface.co/blog/how-to-generate) - using different decoding methods for language generation with Transformers\n",
    "by Patrick von Platen (Huggingface)\n",
    "\n",
    "- In recent years, there has been an increasing interest in open-ended language generation thanks to the rise of LLMs trained on millions of documents, such as GPT2, XLNet, OpenAi-GPT, CTRL, TransfoXL, XLM, Bart, T5, GPT3, and BLOOM. \n",
    "\n",
    "- Such models have achieved promising results on several generation tasks, including open-ended dialogue, summarization, and story generation.\n",
    "\n",
    "- For these models, better decoding methods have played an important role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d761687-8144-42a9-95b2-79a79287b006",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Auto-regressive language generation is based on the assumption that the text being generated can be decomposed into a sequence of subparts.\n",
    "Each part is dependent on the previous parts, thus we can use an auto-regressive decoder to generate text one token at a time based on its predecessors.\n",
    "\n",
    "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
    "\n",
    "> $W_0$ being the initial *context* word sequence. The length $T$ of the word sequence is usually determined *on-the-fly* and corresponds to the timestep $t=T$ the EOS token is generated from $P(w_{t} | w_{1: t-1}, W_{0})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fdc2a9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\",\n",
    "                                          pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0232d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Greedy Search\n",
    "\n",
    "![greedy](../figs/aiart/entelecheia_greedy_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a31c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Greedy search simply selects the word with the highest probability as its next word: $w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$.\n",
    "\n",
    "![greedy](../figs/deep_nlp/zero/deepnlp_2_greedy_search.png)\n",
    "\n",
    "Starting from the word \"The\" the algorithm greedily chooses the next word of highest probability \"nice\" and so on, so\n",
    "that the final generated word sequence is (\"The\", \"nice\", \"woman\") having an overall probability of $0.5 \\times 0.4 = 0.2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7744784",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Generate word sequences using GPT2 on the context (\"I\",\"enjoy\",\"studying\",\"deep\",\"learning\",\"for\",\"natural\", \"language\", \"processing\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0dabdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, but I'm not sure how to apply it to real-world applications.\n",
      "\n",
      "I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy studying deep learning for natural language processing',\n",
    "                             return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=100)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a03e74",
   "metadata": {},
   "source": [
    "- The generated words following the context are reasonable, but the model quickly starts repeating itself! This is a very common problem in language generation in general and seems to be even more so in greedy and beam search.\n",
    "\n",
    "- The major drawback of greedy search though is that it misses high probability words hidden behind a low probability word as can be seen in our sketch above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f06559",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Beam search\n",
    "\n",
    "![beam](../figs/aiart/entelecheia_beam_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a98286",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely `num_beams` of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. \n",
    "\n",
    "- Beam search with `num_beams=2`:\n",
    "\n",
    "![beam](../figs/deep_nlp/zero/deepnlp_2_greedy_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80c158-0eea-4f5b-99bf-21df2905efb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- At time step 1, besides the most likely hypothesis  (\"The\",\"nice\"), beam search also keeps track of the second\n",
    "most likely one (\"The\",\"dog\"). \n",
    "\n",
    "- At time step 2, beam search finds that the word sequence (\"The\",\"dog\",\"has\"),  has with $0.36$, a higher probability than (\"The\",\"nice\",\"woman\"), which has $0.2$. \n",
    "\n",
    "- It has found the most likely word sequence in our toy example\\!\n",
    "\n",
    "- Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163955d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Set `num_beams > 1` and `early_stopping=True` so that generation is finished when all beam hypotheses reached the EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4be3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "What is Deep Learning?\n",
      "\n",
      "Deep learning is a type of artificial intelligence (AI) that can be applied to real-world applications. Deep learning is a type of artificial intelligence (AI) that can be applied to real-world applications.\n",
      "\n",
      "Deep learning is a type of artificial intelligence (AI) that can be applied to real-world applications\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7f218",
   "metadata": {},
   "source": [
    "- While the result is arguably more fluent, the output still includes repetitions of the same word sequences.\n",
    "- A simple remedy is to introduce *n-grams* penalties as introduced by Paulus et al. (2017) and Klein et al. (2017). \n",
    "- The most common n-grams penalty makes sure that no *n-gram* appears twice by manually setting the probability of next words that could create an already seen *n-gram* to 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693530b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Set `no_repeat_ngram_size=2` so that no 2-gram appears twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3896798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "In this post, I'll show you how you can use Deep Learning to build a neural network that can learn to read and write a sentence. In this article, you'll learn how to use the Deep Neural Network (DNN) to learn a language. You'll also learn about how the DNN works and what you need to do to get started\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54cfb2",
   "metadata": {},
   "source": [
    "- Looks much better! We can see that the repetition does not appear anymore. \n",
    "- Nevertheless, *n-gram* penalties have to be used with care. \n",
    "- An article generated about the city *New York* should not use a *2-gram* penalty or otherwise, the name of the city would only appear once in the whole text!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bfcaa9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- We can compare the top beams after generation and choose the generated beam that fits our purpose best.\n",
    "\n",
    "- Set the parameter `num_return_sequences` to the number of highest scoring beams that should be returned. \n",
    "- Make sure that `num_return_sequences <= num_beams`\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2db782d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "In this post, I'll show you how you can use Deep Learning to build a neural network that can learn to read and write a sentence. In this article, you'll learn how to use the Deep Neural Network (DNN) to learn a language. You'll also learn about how the DNN works and what you need to do to get started\n",
      "1: I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "In this post, I'll show you how you can use Deep Learning to build a neural network that can learn to read and write a sentence. In this article, you'll learn how to use the Deep Neural Network (DNN) to learn a language. You'll also learn about how the DNN works and what you need to know about it to\n",
      "2: I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "In this post, I'll show you how you can use Deep Learning to build a neural network that can learn to read and write a sentence. In this article, you'll learn how to use the Deep Neural Network (DNN) to learn a language. You'll also learn about how the DNN works and what you need to know to get started\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=3,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "    print(\"{}: {}\".format(\n",
    "        i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805dd0e",
   "metadata": {},
   "source": [
    "- The five beam hypotheses are only marginally different to each other - which should not be too surprising when using only 5 beams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eefe98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In open-ended generation, a couple of reasons have recently been brought forward why beam search might not be the best possible option:\n",
    "\n",
    "- Beam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization. \n",
    "- But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation.\n",
    "\n",
    "- We have seen that beam search heavily suffers from repetitive generation. \n",
    "- This is especially hard to control with *n-gram*- or other penalties in story generation since finding a good trade-off between forced \"no-repetition\" and repeating cycles of identical *n-grams* requires a lot of finetuning.\n",
    "\n",
    "- High quality human language does not follow a distribution of high probability next words. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b4eeb-9143-4677-8440-3b8a7f8a711b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- In other words, as humans, we want generated text to surprise us and not to be boring/predictable. [(Ari Holtzman et al., 2019)](https://arxiv.org/abs/1904.09751)\n",
    "\n",
    "![beam_vs_human](../figs/deep_nlp/zero/deepnlp_2_beam_vs_human.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b450fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Sampling\n",
    "\n",
    "Sampling means randomly picking the next word $w_t$ according to its conditional probability distribution:\n",
    "\n",
    "$$ w_t \\sim P(w|w_{1:t-1}) $$\n",
    "\n",
    "The following graphic visualizes language generation when sampling.\n",
    "\n",
    "![sampling](../figs/deep_nlp/zero/deepnlp_2_sampling_search.png)\n",
    "\n",
    "Language generation using sampling is not *deterministic* anymore. \n",
    "The word (\"car\") is sampled from the conditioned probability distribution $P(w | \\text{\"The\"})$, followed by sampling (\"drives\") from $P(w | \\text{\"The\"}, \\text{\"car\"})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd0ac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Set `do_sample=True` and deactivate *Top-K* sampling via `top_k=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ef99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, especially to see how it does useful things over evolutionary time once you shift the ideology of theoretical language. I also really like developing algorithmically tailored introductory programming books to teach my students, but I simply don't have the spare time to do all that I did before. HALAX fight3r also released their original issue warning the public that Gaming with Game Software may be detrimental to your solidified web skills and appearances, likely to undermine your beliefs and help\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8370c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- The text seems alright - but when taking a closer look, it is not very coherent. \n",
    "- Some words don't sound like they were written by a human. \n",
    "- That is the big problem when sampling word sequences: The models often generate incoherent gibberish.\n",
    "\n",
    "- A trick is to make the distribution $P(w|w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so-called `temperature` of the [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max).\n",
    "\n",
    "![temperature](../figs/deep_nlp/zero/deepnlp_2_sampling_search_with_temp.png)\n",
    "\n",
    "- The conditional next word distribution of step t=1 becomes much sharper leaving almost no chance for word (\"car\") to be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f82721",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Cool down the distribution in the library by setting `temperature=0.7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca49dff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, but this is a simplistic example of how the language can be applied to a complex problem.\n",
      "\n",
      "Using more complex architectures\n",
      "\n",
      "The most common approach is to look at a whole set of ML designs that are quite complex and require a lot of work to deploy. For example, the first ML project I'm learning is the human language learning project. Let's take a look at the human-language learning project.\n",
      "\n",
      "The current human-language\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=0,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fa1d1",
   "metadata": {},
   "source": [
    "- There are less weird n-grams and the output is a bit more coherent now. \n",
    "- While applying temperature can make a distribution less random, in its limit, when setting `temperature` $\\to 0$, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8db85c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Top-K Sampling\n",
    "\n",
    "\n",
    "![top_k](../figs/deep_nlp/zero/deepnlp_2_top_k_sampling.png)\n",
    "\n",
    "In *Top-K* sampling, the *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e94719",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Having set $K = 6$, in both sampling steps we limit our sampling pool to 6 words. \n",
    "- While the 6 most likely words, defined as $V_{\\text{top-K}}$ encompass only two-thirds of the whole\n",
    "probability mass in the first step, it includes almost all of the probability mass in the second step. \n",
    "- Nevertheless, we see that it successfully eliminates the rather weird candidates (\"not\", \"the\", \"small\", \"told\") in the second sampling step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70658b2-fb8a-4ab1-8a97-81e3d4f58364",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's see how *Top-K* can be used in the library by setting `top_k=50`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e639a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, as well as in terms of learning from experience. As well as having my students who are learning an intermediate language and learn how to use it in a meaningful way or with a minimal effort, I can also focus on learning programming languages like C++ (though I wouldn't go so far as to call that \"advanced\"), Python (for those that aren't familiar with Python) or Java (I think I actually want to see what I can learn\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bea5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- The text is arguably the most *human-sounding* text so far. \n",
    "- One concern with *Top-K* sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w|w_{1:t-1})$. \n",
    "- This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above).\n",
    "\n",
    "\n",
    "- In step $t=1$, Top-K eliminates the possibility to sample (\"people\",\"big\",\"house\",\"cat\"), which seem like reasonable candidates. \n",
    "- On the other hand, in step $t=2$ the method includes the arguably ill-fitted words (\"down\",\"a\") in the sample pool of words. \n",
    "- Thus, limiting the sample pool to a fixed size $K$ could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution. \n",
    "- This intuition led Ari Holtzman et al. (2019) to create ***Top-p***- or ***nucleus***-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd5b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Top-p (nucleus) sampling\n",
    "\n",
    "![top_p](../figs/deep_nlp/zero/deepnlp_2_top_p_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3dae2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Instead of sampling only from the most likely *K* words, in *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. \n",
    "- The probability mass is then redistributed among this set of words. \n",
    "- This way, the size of the set of words (*a.k.a* the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. \n",
    "\n",
    "\n",
    "- Having set $p=0.92$, *Top-p* sampling picks the *minimum* number of words to exceed together $p=92$ of the probability mass, defined as $V_{\\text{top-p}}$. \n",
    "- In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%. \n",
    "- It can be seen that it keeps a wide range of words where the next word is arguably less predictable, *e.g.* $P(w | \\text{\"The''})$, and only a few words when the next word seems more predictable, *e.g.* $P(w | \\text{\"The\"}, \\text{\"car\"})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549d760-3940-4349-822b-88874f096e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Activate *Top-p* sampling by setting `0 < top_p < 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b41fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing. Learn more at lecture.lightlink.com.\n",
      "\n",
      "Ciao — Social conscious \"synchronization\" between conscious and nonconscious participants at the Cognition Experience. Learn more at lecture.lightlink.com.\n",
      "\n",
      "Everett's Processio\n",
      "\n",
      "A video discussion\n",
      "\n",
      "The Evolution of Mindfulness in Today's Economy\n",
      "\n",
      "The Recent Advances in the Study of Mindfulness and Success in Society. The results from St Louis University\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_p=0.92,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e5ff5",
   "metadata": {},
   "source": [
    "Great, that sounds like it could have been written by a human. Well, maybe not quite yet.\n",
    "\n",
    "While in theory, *Top-p* seems more elegant than *Top-K*, both methods work well in practice. \n",
    "*Top-p* can also be used in combination with *Top-K*, which can avoid very low ranked words while allowing for some\n",
    "dynamic selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9bf73-78a9-44ee-9859-d45ba1977d55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Finally, to get multiple independently sampled outputs, we can *again* set the parameter `num_return_sequences > 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aafe1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy studying deep learning for natural language processing, as I've learned it quickly enough for me to understand it. The idea of modeling learning comes from both of these worlds. The first is that of natural language processing and the second is a mathematical theory that allows you to draw meaningful conclusions. To understand what is in the world of Natural Language Processing, go to the book by Mike Biederman of the University of Toronto or check out his website at www.layers.com. You are also\n",
      "1: I enjoy studying deep learning for natural language processing. My favourite part, even though it is just about my main job, is learning to play a game. That is exactly what I am doing on my website. All this is part of my job as a developer and I love playing games.\n",
      "\n",
      "My job with this company was to create a website where my students could explore the world as they would enjoy learning about computer science, physics, or any other science.\n",
      "\n",
      "In doing so, I\n",
      "2: I enjoy studying deep learning for natural language processing. In my experience it doesn't help my training to teach my students how to do it, but it's an awesome source for training and has led to many exciting discoveries about Deep Learning and neural nets in general.\n",
      "\n",
      "Learning to code\n",
      "\n",
      "Another way to learn Deep Learning is by coding. For example, I often code to solve problem A and then use it for the solving of problem B. This is where the language really comes in. I\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(\n",
    "        i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b7002",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Summary of decoding / search strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9fb0b",
   "metadata": {},
   "source": [
    "As *ad-hoc* decoding methods, *top-p* and *top-K* sampling seem to produce more fluent text than traditional *greedy* - and *beam* search on open-ended language generation. Recently, there has been more evidence though that the apparent flaws of *greedy* and *beam* search - mainly generating repetitive word sequences - are caused by the model (especially the way the model is trained), rather than the decoding method, *cf.* [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). Also, as demonstrated in [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492), it looks as *top-K* and *top-p* sampling also suffer from generating repetitive word sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010d4e7-3d9c-4980-80d3-95bea9cbba60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Greedy Search \n",
    "  - simply chooses the next word at each timestep t+1 that has the highest predicted probability of following the word at t. \n",
    "  - One of the main issues here is that greedy search will miss words with a high probability at t+1 if it is preceded by a word with a low probability at t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb28a24-ea98-4a1d-a70a-f445648e3d08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Beam Search \n",
    "  - keeps track of the n-th (num_beams) most likely word sequences and outputs the most likely sequence. \n",
    "  - Sounds great, but this method breaks down when the output length can be highly variable — as in the case of open-ended text generation. \n",
    "  - Both greedy and beam search also produce outputs whose distribution does not align very well with the way humans might perform the same task (i.e. both are liable to produce fairly repetitive, boring text).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b20ed6-1e2e-4eae-abaa-47da7af1d4d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Sampling With Top-k + Top-p\n",
    "  - a combination of three methods. \n",
    "  - By sampling, we mean that the next word is chosen randomly based on its conditional probability distribution (von Platen, 2020). \n",
    "  - In Top-k, we choose the k most likely words, and then redistribute the probability mass amongst them before the next draw. \n",
    "  - Top-p adds an additional constraint to top-k, in that we’re choosing from the smallest set of words whose cumulative probability exceed p.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a12f491",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Prompt Engineering: The Career of Future\n",
    "\n",
    "![prompt](../figs/deep_nlp/zero/deepnlp_2_prompt.png)\n",
    "(source: https://twitter.com/karpathy/status/1273788774422441984/photo/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a132fd0-c774-49f6-b671-43f8187bc4d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "> With the No-Code revolution around the corner, and the coming of new-age technologies like GPT-3 we may see a stark difference between the career of today and the careers of tomorrow…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4439c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "As a rule of thumb while designing the training prompt you should aim towards getting a zero-shot response from the model, if that isn’t possible move forward with few examples rather than providing it with an entire corpus. The standard flow for training prompt design should look like: Zero-Shot → Few Shots → Corpus-based Priming.\n",
    "\n",
    "- Step 1: Define the problem you are trying to solve and bucket it into one of the possible natural language tasks classification, Q & A, text generation, creative writing, etc.\n",
    "- Step 2: Ask yourself if there is a way to get a solution with zero-shot (i.e. without priming the GPT-3 model with any external training examples)\n",
    "- Step 3: If you think that you need external examples to prime the model for your use case, go back to step-2 and think really hard.\n",
    "- Step 4: Now think of how you might encounter the problem in a textual fashion given the “text-in, text-out” interface of GPT-3. Think about all the possible scenarios to represent your problem in textual form.\n",
    "- Step 5: If you end up using the external examples, use as few as possible and try to include variety in your examples without essentially overfitting the model or skewing the predictions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f869af7787e6a1c49e09e367fc6e1b81d93d1c6583b43249c80edc047bd13cb2"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
