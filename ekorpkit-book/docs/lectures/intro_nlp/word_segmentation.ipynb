{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Word Segmentation and Association\n",
    "\n",
    "![](../figs/intro_nlp/words/entelecheia_associaltion_vs_segmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f9e1e",
   "metadata": {},
   "source": [
    "## Word Segmentation\n",
    "\n",
    "- **Word segmentation** is the task of splitting a string of characters into words.\n",
    "- Word segmentation is important for a machine to understand the meaning of a sentence.\n",
    "- In English, we can split a string of characters into words by spaces.\n",
    "- However, in languages like Chinese and Janpanese, there is no space between words.  \n",
    "- Even in English, there are some cases where no space is used between words.\n",
    "- Humans can easily segment a string of characters into words, even though there is no space between words.\n",
    "- For example, we can easily segment the string of characters `Ilikechocolate` into words `I like chocolate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7cf33",
   "metadata": {},
   "source": [
    "## Why should we segment words?\n",
    "\n",
    "There are many applications that require word segmentation, even in English.\n",
    "\n",
    "- Normalizing English compound nouns that are variably written for search engines.\n",
    "  - For example, `ice cream` and `ice-cream` should be segmented into `icecream`.\n",
    "- Word segmentation for compounds: Both orginal words and split words should be in the dictionary.\n",
    "- Typing errors may be corrected by word segmentation.\n",
    "- Conversion errors: During conversion, some spaces may be lost.\n",
    "- OCR errors: OCRed text may contain errors.\n",
    "- Keyword extraction from URL addresses, domain names, table column description or programming variables that are written without spaces.\n",
    "- For password analysis, the extraction of terms from passwords can be required.\n",
    "- Automatic CamelCasing of programming variables.\n",
    "- Speech recognition: Speech recognition systems may not properly recognize spaces between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de54549",
   "metadata": {},
   "source": [
    "## Generating segment variants\n",
    "\n",
    "We can generate all possible segment variants of a string of characters. Each distinct segment variant is called a **composition**.\n",
    "\n",
    "- En a string of length $n$, there are $n-1$ possible positions to split the string.\n",
    "- Each of the $n-1$ positions can be used as word boundary.\n",
    "- Therefore, there are $2^{n-1}$ possible compositions.\n",
    "\n",
    "The compositions have to be evaluated to find the best segmentation.\n",
    "\n",
    "- The best segmentation is the one that has the highest probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf09dec",
   "metadata": {},
   "source": [
    "### Naive Recursive Algorithm\n",
    "\n",
    "- The naive recursive algorithm is to generate all possible compositions and evaluate them.\n",
    "- The time complexity of the naive recursive algorithm is $O(2^n)$.\n",
    "- The naive recursive algorithm is not efficient for long strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d279eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def segment_naive(string):\n",
    "    if not string:\n",
    "        return []\n",
    "    else:\n",
    "        return [[string]] + [\n",
    "            [string[:i]] + rest\n",
    "            for i in range(1, len(string))\n",
    "            for rest in segment_naive(string[i:])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdac8abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['isit'],\n",
      " ['i', 'sit'],\n",
      " ['i', 's', 'it'],\n",
      " ['i', 's', 'i', 't'],\n",
      " ['i', 'si', 't'],\n",
      " ['is', 'it'],\n",
      " ['is', 'i', 't'],\n",
      " ['isi', 't']]\n"
     ]
    }
   ],
   "source": [
    "pprint(segment_naive(\"isit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401975f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['가방에'], ['가', '방에'], ['가', '방', '에'], ['가방', '에']]\n"
     ]
    }
   ],
   "source": [
    "pprint(segment_naive(\"가방에\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a93ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 8192\n"
     ]
    }
   ],
   "source": [
    "text = \"thisislongtext\"\n",
    "print(len(text), len(segment_naive(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a93a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1024\n"
     ]
    }
   ],
   "source": [
    "text = \"아버지가방에들어가신다\" # Father goes into the bag or Father enters the room\n",
    "print(len(text), len(segment_naive(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7523592",
   "metadata": {},
   "source": [
    "### Dynamic Programming\n",
    "\n",
    "- Dynamic programming is a technique to solve a problem by breaking it into subproblems and storing the results of subproblems to avoid computing the same results again.\n",
    "- The time complexity of dynamic programming is $O(n)$.\n",
    "- For long strings, dynamic programming is much more efficient than the naive recursive algorithm.\n",
    "\n",
    "```python\n",
    "def segment(string, dictionary):\n",
    "    if not string:\n",
    "        return []\n",
    "    for end in range(1, len(string) + 1):\n",
    "        first, rest = string[:end], string[end:]\n",
    "        if first in dictionary:\n",
    "            return [first] + segment(rest, dictionary)\n",
    "    return [string]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17af89",
   "metadata": {},
   "source": [
    "### Triangular Matrix\n",
    "\n",
    "- The dynamic programming algorithm can be implemented using a triangular matrix.\n",
    "- The tryangular matrix algorithm uses nested loops and a circular buffer to store the results of subproblems.\n",
    "- A triangular matrix of parts with increasing length is generated and organized in a circular buffer.\n",
    "- This allows a constant amount of memory to be used for the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8845f",
   "metadata": {},
   "source": [
    "### Unknown Words\n",
    "\n",
    "- We can not rely on the dictionary to segment all words.\n",
    "- There are uncommon words, new words, misspelled words, foreign words, proper nouns, slang words, etc.\n",
    "- Even in these cases, we want to segment the words into meaningful parts.\n",
    "- Therefore, we have to estimate the probability of any possible segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1763b6",
   "metadata": {},
   "source": [
    "## Evaluation of Compositions\n",
    "\n",
    "- Generally, we can evaluate a composition by calculating the probability of the composition.\n",
    "- Word probabilities can be estimated from a corpus:\n",
    "\n",
    "    $$\n",
    "    P(w_i) = \\frac{c(w_i)}{N}\n",
    "    $$\n",
    "\n",
    "    where $c(w_i)$ is the count of word $w_i$ and $N$ is the total number of words in the corpus.\n",
    "\n",
    "- However, for unkonwn words, we have to use other criteria to evaluate the composition.\n",
    "- At word boundary, the uncertainty of the segmentation increases.\n",
    "- By measuring the uncertainty, we can evaluate the composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "### Uncertainty of word boundaries\n",
    "\n",
    "- The uncertainty of word boundaries can be measured by the entropy of the word boundary.\n",
    "- Harris, 1970 said that if the uncertainty of successive tokens increases, the location is a word boundary.\n",
    "- Feng et al., 2004 proposed a statistical criterion called accessor variety (AV) to measure how likely a sub-sequence is a word, and then to find the best segmentation pattern that maximizes a target function of accessor variety and the length of the sub-sequence as variants. \n",
    "- Jin and TanakaIshii, 2006 proposed branch entropy as another criterion for unsupervised segmentation.\n",
    "- Both criteria share a similar assumption as in the fundamental work by Harris, 1970, that the uncertainty of successive tokens increases at word boundaries.\n",
    "- The latter is the countinous version of the former.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321cbbe",
   "metadata": {},
   "source": [
    "![](../figs/intro_nlp/words/branching_entropy_uncertainty.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2f2dc",
   "metadata": {},
   "source": [
    "\n",
    "### Accessor Variety\n",
    "\n",
    "- The accessor variety (AV) defines that the uncertainty of a sub-sequence is the number of different words that can be formed by adding a sub-sequence to the sub-sequence.\n",
    "- For the right-side accessor variety, it is the number of different words that can be formed by adding a sub-sequence to the right side of the sub-sequence.\n",
    "- For the following sub-sequence, the right-side accessor variety of `hope` is 2, because `hope` can be followed by `less` or `fully`.\n",
    "\n",
    "    ```\n",
    "    \"hopeful\": 100\n",
    "    \"hopeless\": 80\n",
    "    ```\n",
    "- The left-side accessor variety is the number of different words that can be formed by adding a sub-sequence to the left side of the sub-sequence.\n",
    "- For example, the left-side accessor variety of `less` is 3, because `hopeless`, `useless`, and `pointless` can be formed by adding `less` to the left side of `less`.\n",
    "\n",
    "    ```\n",
    "    \"hopeless\": 80\n",
    "    \"unless\": 160\n",
    "    \"pointless\": 70\n",
    "    ```\n",
    "- Depending on the language, the left-side accessor variety or the right-side accessor variety may be more suitable for segmentation.\n",
    "- Threshold values can be used to determine the word boundaries.\n",
    "- The threshold values can be determined by the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8e83c",
   "metadata": {},
   "source": [
    "### Branch Entropy\n",
    "\n",
    "- The branch entropy is defined as the entropy of the distribution of the number of words that can be formed by adding a single character to the end of a sub-sequence.\n",
    "\n",
    "    $$\n",
    "    \\text{BE}(w|c) = -\\sum_{i=1}^n p_i \\log p_i\n",
    "    $$\n",
    "\n",
    "    where $p_i$ is the probability of the number of words that can be formed by adding a single character to the end of a sub-sequence $w$ and $c$ is the character.\n",
    "\n",
    "- As in the case of accessor variety, the branch entropy can be calculated for the left-side and the right-side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5b448",
   "metadata": {},
   "source": [
    "### Cohesion Probability\n",
    "\n",
    "- The accesor variety and the branch entropy determine the boundary of a word by measuring the uncertainty of the word boundary, that is, the exterior boundary of a word.\n",
    "- Unlike the accesor variety and the branch entropy, the cohesion probability determines the boundary of a word by measuring the association among characters inside a word, that is, the interior boundary of a word.\n",
    "- The cohesion probability is defined as the probability of a sequence of n characters given the first n-1 characters.\n",
    "\n",
    "    $$\n",
    "    cohesion(c_1, c_2, \\cdots, c_n) = \\sqrt[n-1]{\\prod_{i=1}^{n-1} P(c_1, c_2, \\cdots, c_{i+1}|c_1, c_2, \\cdots, c_i)}\n",
    "    $$\n",
    "\n",
    "    where $P(c_1, c_2 | c_1) = \\frac{count(c_1, c_2)}{count(c_1)}$.\n",
    "\n",
    "    Therefore, the formula can be simplified as:\n",
    "\n",
    "    $$\n",
    "    cohesion(c_1, c_2, \\cdots, c_n) = \\sqrt[n-1]{\\prod_{i=1}^{n-1} \\frac{count(c_1, c_2, \\cdots, c_{i+1})}{count(c_1, c_2, \\cdots, c_i)}} \\\\ = \\sqrt[n-1]{\\frac{count(c_1, c_2, \\cdots, c_n)}{count(c_1)}}\n",
    "    $$\n",
    "\n",
    "- The above formula assumes that characters are assoiciated in the forward direction.\n",
    "- For the backward direction, the formula is:\n",
    "\n",
    "    $$\n",
    "    cohesion(c_1, c_2, \\cdots, c_n) = \\sqrt[n-1]{\\prod_{i=1}^{n-1} \\frac{count(c_{i}, c_{i+1}, \\cdots, c_n)}{count(c_{i+1}, c_{i+2}, \\cdots, c_{n})}} \\\\ = \\sqrt[n-1]{\\frac{count(c_1, c_2, \\cdots, c_n)}{count(c_n)}}\n",
    "    $$\n",
    "\n",
    "    where $count(c_{i+1}, c_{i+2}, \\cdots, c_n)$ is the count of the sequence of characters from $c_{i+1}$ to $c_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5995c",
   "metadata": {},
   "source": [
    "### Maximum Matching Algorithm\n",
    "\n",
    "- If we have all known words in a dictionary, we can use the maximum matching algorithm to segment a sentence.\n",
    "- The maximum matching algorithm is a greedy algorithm that finds the longest matching word from the dictionary.\n",
    "- The algorithm is as follows:\n",
    "\n",
    "    1. Find the longest matching word from the dictionary.\n",
    "    2. If the word is found, add the word to the result and remove the word from the input.\n",
    "    3. If the word is not found, add the first character to the result and remove the first character from the input.\n",
    "    4. Repeat 1-3 until the input is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778caf04",
   "metadata": {},
   "source": [
    "## Word Segmentation in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca51dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus : fomc\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "eKonf.setLogger(\"WARNING\")\n",
    "\n",
    "cfg = eKonf.compose(\"corpus\")\n",
    "cfg.name = \"fomc\"\n",
    "cfg.path.cache.uri = (\n",
    "    \"https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/fomc.zip\"\n",
    ")\n",
    "cfg.data_dir = cfg.path.cached_path\n",
    "cfg.auto.merge = True\n",
    "fomc_corpus = eKonf.instantiate(cfg)\n",
    "print(fomc_corpus)\n",
    "texts = fomc_corpus.data[fomc_corpus.data.content_type == \"fomc_statement\"].text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "98e283cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chairman alan greenspan announced today that the federal open market committee decided to increase slightly the degree of pressure on reserve positions. the action is expected to be associated with a small increase in short-term money market interest rates. the decision was taken to move toward a less accommodative stance in monetary policy in order to sustain and enhance the economic expansion. chairman greenspan decided to announce this action immediately so as to avoid any misunderstanding of the committee's purposes, given the fact that this is the first firming of reserve market conditions by the committee since early 1989.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "\n",
    "def pre_tokenize(text, lowercase=True, whitespace_token=\" \"):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", whitespace_token, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "print(pre_tokenize(texts[0]))\n",
    "text = pre_tokenize(\" \".join(texts))\n",
    "word_freqs = collections.Counter(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ab2683c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 subwords: [('in', 19653), ('th', 17728), ('on', 17156), ('an', 15227), ('er', 13990), ('at', 13073), ('re', 12861), ('ti', 12781), ('he', 12611), ('the', 11984)]\n"
     ]
    }
   ],
   "source": [
    "def initialize_subwords(word_freqs, verbose=True):\n",
    "    character_freqs = collections.defaultdict(int)\n",
    "    subwords_freqs = collections.defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        for i in range(len(word)):\n",
    "            character_freqs[word[i]] += freq\n",
    "            # Loop through the subwords of length at least 2\n",
    "            for j in range(i + 2, len(word) + 1):\n",
    "                subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "    # Sort subwords by frequency\n",
    "    sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    if verbose:\n",
    "        print(\"Top 10 subwords: {}\".format(sorted_subwords[:10]))\n",
    "    return sorted_subwords, character_freqs\n",
    "\n",
    "\n",
    "sorted_subwords, characters = initialize_subwords(word_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6bd5aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(characters.items()) + sorted_subwords\n",
    "tokens = {token: freq for token, freq in tokens}\n",
    "tokens = collections.Counter(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "08d795d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trie:\n",
    "    def __init__(self, end_symbol=\"<END>\"):\n",
    "        self.root = {}\n",
    "        self.end_symbol = end_symbol\n",
    "\n",
    "    def add(self, word, value):\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            if ch not in node:\n",
    "                node[ch] = {}\n",
    "            node = node[ch]\n",
    "        node[self.end_symbol] = value\n",
    "\n",
    "    def get_value(self, word):\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            if ch not in node:\n",
    "                return 0\n",
    "            node = node[ch]\n",
    "        if self.end_symbol not in node:\n",
    "            return 0\n",
    "        return node[self.end_symbol]\n",
    "\n",
    "    def set_value(self, word, value):\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            if ch not in node:\n",
    "                raise ValueError(\"word not in trie\")\n",
    "            node = node[ch]\n",
    "        if self.end_symbol not in node:\n",
    "            raise ValueError(\"word not in trie\")\n",
    "        node[self.end_symbol] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c604a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma\n",
    "\n",
    "def initialize_trie(tokens):\n",
    "    trie = Trie()\n",
    "    norm = sum(list(tokens.values()))\n",
    "    logsum = digamma(norm)\n",
    "\n",
    "    maxlen = 0\n",
    "    for tok, val in tokens.items():\n",
    "        trie.add(tok, digamma(val) - logsum)\n",
    "        maxlen = max(maxlen, len(tok))\n",
    "\n",
    "    return trie, maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27aba355",
   "metadata": {},
   "outputs": [],
   "source": [
    "trie, maxlen = initialize_trie(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "68ba39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "L = collections.defaultdict(int)\n",
    "R = collections.defaultdict(int)\n",
    "_aL = collections.defaultdict(int)\n",
    "_aR = collections.defaultdict(int)\n",
    "\n",
    "def normalize_word(word):\n",
    "    # replace all non-alphanumeric characters at the end of the word with a space\n",
    "    word = re.sub(r\"[^a-zA-Z0-9]+$\", \" \", word)\n",
    "    # replace all non-alphanumeric characters at the beginning of the word with a space\n",
    "    word = re.sub(r\"^[^a-zA-Z0-9]+\", \" \", word)\n",
    "    return word.strip()\n",
    "\n",
    "def pre_tokenize(text, lowercase=True, whitespace_token=\" \"):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", whitespace_token, text)\n",
    "    return [normalize_word(word) for word in text.split() if len(normalize_word(word)) > 0]\n",
    "\n",
    "# words = pre_tokenize(text).split()\n",
    "all_words = pre_tokenize(\" \".join(texts[:10]))\n",
    "\n",
    "max_left_length = maxlen\n",
    "max_right_length = maxlen\n",
    "\n",
    "for word in all_words:\n",
    "    \n",
    "    if (not word) or (len(word) <= 1):\n",
    "        continue\n",
    "    word_len = len(word)\n",
    "    for i in range(1, min(max_left_length + 1, word_len) + 1):\n",
    "        L[word[:i]] += 1\n",
    "    for i in range(1, min(max_right_length + 1, word_len)):\n",
    "        R[word[-i:]] += 1\n",
    "\n",
    "    for left_word, word, right_word in zip(\n",
    "        [all_words[-1]] + all_words[:-1], all_words, all_words[1:] + [all_words[0]]\n",
    "    ):\n",
    "        # print(left_word, word, right_word)\n",
    "\n",
    "        l_word = word[-i:]\n",
    "        r_word = word[:i]\n",
    "        word_len = len(word)\n",
    "        _aL[\"%s %s\" % (l_word, \"▁\")] += 1\n",
    "        _aR[\"%s %s\" % (\"▁\", r_word)] += 1\n",
    "        for i in range(1, min(max_right_length + 1, word_len)):\n",
    "            _aL[\"%s %s\" % (l_word, right_word[0])] += 1\n",
    "        for i in range(1, min(max_left_length + 1, word_len)):\n",
    "            _aR[\"%s %s\" % (left_word[-1], r_word)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7e2ec57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753 677 1125 897\n"
     ]
    }
   ],
   "source": [
    "print(len(_aL), len(_aR), len(L), len(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "94cc3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def entropy(dic):\n",
    "    if not dic:\n",
    "        return 0.0\n",
    "    sum_ = sum(dic.values())\n",
    "    entropy = 0\n",
    "    if sum_ == 0:\n",
    "        return 0.0\n",
    "    for freq in dic.values():\n",
    "        prob = float(freq) / sum_\n",
    "        if prob > 0:\n",
    "            entropy -= prob * math.log(prob)\n",
    "    return -1 * entropy\n",
    "\n",
    "\n",
    "def all_branching_entropy(\n",
    "    get_score=entropy, verbose=True\n",
    "):\n",
    "    def parse_left(extension):\n",
    "        return extension[:-1]\n",
    "\n",
    "    def parse_right(extension):\n",
    "        return extension[1:]\n",
    "\n",
    "    def sort_by_length(counter):\n",
    "        sorted_by_length = collections.defaultdict(lambda: [])\n",
    "        for w in counter.keys():\n",
    "            sorted_by_length[len(w)].append(w)\n",
    "        return sorted_by_length\n",
    "\n",
    "    def get_entropy_table(\n",
    "        parse, sorted_by_length, sorted_by_length_a, max_length, counter, counter_a\n",
    "    ):\n",
    "        num_sum = sum((len(words) for length, words in sorted_by_length.items()))\n",
    "        be = {}\n",
    "        for word_len in range(2, max_length):\n",
    "            words = sorted_by_length.get(word_len, [])\n",
    "            extensions = collections.defaultdict(lambda: [])\n",
    "            for word in words:\n",
    "                extensions[parse(word)].append(word)\n",
    "            words_ = sorted_by_length_a.get(word_len + 1, [])\n",
    "            for word in words_:\n",
    "                extensions[parse(word.replace(\" \", \"\"))].append(word)\n",
    "            for root_word, extension_words in extensions.items():\n",
    "                extension_frequency = {\n",
    "                    ext: counter_a.get(ext, 0) if \" \" in ext else counter.get(ext)\n",
    "                    for ext in extension_words\n",
    "                }\n",
    "                be[root_word] = get_score(extension_frequency)\n",
    "        return be\n",
    "\n",
    "    def merge(be_l, be_r):\n",
    "        be = {word: (v, be_r.get(word, 0)) for word, v in be_l.items()}\n",
    "        for word, v in be_r.items():\n",
    "            if word in be_l:\n",
    "                continue\n",
    "            be[word] = (0, v)\n",
    "        return be\n",
    "\n",
    "    be_l = get_entropy_table(\n",
    "        parse_right,\n",
    "        sort_by_length(R),\n",
    "        sort_by_length(_aR),\n",
    "        max_right_length + 1,\n",
    "        R,\n",
    "        _aR,\n",
    "    )\n",
    "    be_r = get_entropy_table(\n",
    "        parse_left,\n",
    "        sort_by_length(L),\n",
    "        sort_by_length(_aL),\n",
    "        max_left_length + 1,\n",
    "        L,\n",
    "        _aL,\n",
    "    )\n",
    "    be = merge(be_l, be_r)\n",
    "    if verbose > 0:\n",
    "        print_head = (\n",
    "            \"branching entropies\" if get_score == entropy else \"accessor variety\"\n",
    "        )\n",
    "        print(\"\\rall %s was computed # words = %d\" % (print_head, len(be)))\n",
    "    return be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4f36f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_words():\n",
    "    words = {word for word in L.keys() if len(word) <= max_left_length}\n",
    "    words.update({word for word in R.keys() if len(word) <= max_right_length})\n",
    "    return words\n",
    "\n",
    "def frequency(word):\n",
    "    return (L.get(word, 0), R.get(word, 0))\n",
    "    \n",
    "def cohesion_score(word):\n",
    "    word_len = len(word)\n",
    "    if (not word) or (word_len <= 1):\n",
    "        return (0, 0)\n",
    "    l_freq, r_freq = map(float, frequency(word))\n",
    "    l_cohesion = 0 if l_freq == 0 else np.power( (l_freq / L[word[0]]), (1 / (word_len - 1)) )\n",
    "    r_cohesion = 0 if r_freq == 0 else np.power( (r_freq / R[word[-1]]), (1 / (word_len - 1)) )\n",
    "    return (l_cohesion, r_cohesion)\n",
    "\n",
    "def all_cohesion_scores(verbose=True):\n",
    "    cps = {}\n",
    "    words = get_words()\n",
    "    for i, word in enumerate(words):\n",
    "        # if (verbose > 0) and (i % verbose == 0):\n",
    "        #     print('\\r cohesion probabilities ... (%d in %d)' % (i+1, len(words)))\n",
    "        cp = cohesion_score(word)\n",
    "        if (cp[0] == 0) and (cp[1] == 0):\n",
    "            continue\n",
    "        cps[word] = cp\n",
    "    if (verbose > 0):\n",
    "        print('\\rall cohesion probabilities was computed. # words = %d' % len(cps))\n",
    "    return cps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3f595227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cohesion probabilities was computed. # words = 1929\n",
      "all branching entropies was computed # words = 1615\n",
      "all accessor variety was computed # words = 1615\n"
     ]
    }
   ],
   "source": [
    "cps = all_cohesion_scores()\n",
    "bes = all_branching_entropy()\n",
    "avs = all_branching_entropy(get_score=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a641beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores = collections.namedtuple('Scores', 'cohesion_forward cohesion_backward left_branching_entropy right_branching_entropy left_accessor_variety right_accessor_variety leftside_frequency rightside_frequency')\n",
    "\n",
    "\n",
    "def word_scores(cps={}, bes={}, avs={}):\n",
    "    scores = {}\n",
    "    for word in get_words():\n",
    "        cp = cps.get(word, (0, 0))\n",
    "        be = bes.get(word, (0, 0))\n",
    "        av = avs.get(word, (0, 0))\n",
    "        scores[word] = Scores(cp[0], cp[1], be[0], be[1], av[0], av[1], L.get(word, 0), R.get(word, 0))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1da0c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(\n",
    "    scores=None,\n",
    "    min_frequency=3,\n",
    "    min_cohesion_forward=0.05,\n",
    "    min_cohesion_backward=0.0,\n",
    "    max_droprate_cohesion=0.98,\n",
    "    max_droprate_leftside_frequency=0.98,\n",
    "    min_left_branching_entropy=0.5,\n",
    "    min_right_branching_entropy=0.5,\n",
    "    min_left_accessor_variety=0,\n",
    "    min_right_accessor_variety=0,\n",
    "    min_word_length=2,\n",
    "    remove_subwords=True,\n",
    "):\n",
    "    if not scores:\n",
    "        scores = word_scores()\n",
    "    scores_ = {}\n",
    "    for word, score in sorted(scores.items(), key=lambda x: len(x[0])):\n",
    "        if (\n",
    "            (score.left_branching_entropy > min_left_branching_entropy)\n",
    "            and (score.right_branching_entropy > min_right_branching_entropy)\n",
    "            # or (score.left_accessor_variety < min_left_accessor_variety)\n",
    "            # or (score.right_accessor_variety < min_right_accessor_variety)\n",
    "            or (\n",
    "                max(score.leftside_frequency, score.rightside_frequency) < min_frequency\n",
    "            )\n",
    "        ):\n",
    "            continue\n",
    "        if (len(word) >= 2) and (\n",
    "            (score.cohesion_forward < min_cohesion_forward)\n",
    "            or (score.cohesion_backward < min_cohesion_backward)\n",
    "        ):\n",
    "            continue\n",
    "        if len(word) < min_word_length:\n",
    "            continue\n",
    "        scores_[word] = score\n",
    "        if not remove_subwords:\n",
    "            continue\n",
    "        subword = word[:-1]\n",
    "        droprate_leftside_frequency = (\n",
    "            0 if not (subword in L) else score.leftside_frequency / L[subword]\n",
    "        )\n",
    "        if (droprate_leftside_frequency > max_droprate_leftside_frequency) and (\n",
    "            subword in scores_\n",
    "        ):\n",
    "            del scores_[subword]\n",
    "    return scores_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "389f84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = extract(word_scores(cps=cps, bes=bes, avs=avs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "52afa014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5- Scores(cohesion_forward=1.0, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.3250829733914482, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "ne Scores(cohesion_forward=0.9090909090909091, cohesion_backward=0.006666666666666667, left_branching_entropy=-0.0, right_branching_entropy=-0.5004024235381879, left_accessor_variety=1, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=2)\n",
      "ha Scores(cohesion_forward=0.7777777777777778, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.410116318288409, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "5-1/ Scores(cohesion_forward=0.9654893846056297, cohesion_backward=0, left_branching_entropy=-0.5004024235381879, right_branching_entropy=-0.6365141682948128, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=9, rightside_frequency=0)\n",
      "expect Scores(cohesion_forward=0.7185035699395549, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.34883209584303193, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=9, rightside_frequency=0)\n",
      "per Scores(cohesion_forward=0.5773502691896257, cohesion_backward=0, left_branching_entropy=-1.0042424730540764, right_branching_entropy=-0.17884491271684755, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=23, rightside_frequency=0)\n",
      "mar Scores(cohesion_forward=0.5683985600588051, cohesion_backward=0, left_branching_entropy=-0.45056120886630463, right_branching_entropy=-0.19144408195771734, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=21, rightside_frequency=0)\n",
      "acti Scores(cohesion_forward=0.5732773111589102, cohesion_backward=0, left_branching_entropy=-0.410116318288409, right_branching_entropy=-0.2711893730418441, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=26, rightside_frequency=0)\n",
      "co Scores(cohesion_forward=0.6, cohesion_backward=0.043478260869565216, left_branching_entropy=-1.2947153675225072, right_branching_entropy=-0.35990548612823986, left_accessor_variety=5, right_accessor_variety=5, leftside_frequency=33, rightside_frequency=2)\n",
      "appro Scores(cohesion_forward=0.583526016818016, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.37677016125643675, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=16, rightside_frequency=0)\n",
      "dis Scores(cohesion_forward=0.6090712125322324, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.46203690946645704, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=23, rightside_frequency=0)\n",
      "mone Scores(cohesion_forward=0.6133748537965212, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5004024235381879, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=0)\n",
      "continu Scores(cohesion_forward=0.6460749217270753, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=4, rightside_frequency=0)\n",
      "inte Scores(cohesion_forward=0.4675224708569822, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.25731864054383163, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=14, rightside_frequency=0)\n",
      "th Scores(cohesion_forward=0.6974789915966386, cohesion_backward=0.7777777777777778, left_branching_entropy=-1.0235570898907984, right_branching_entropy=-0.6686752780804099, left_accessor_variety=7, right_accessor_variety=4, leftside_frequency=166, rightside_frequency=21)\n",
      "moderati Scores(cohesion_forward=0.6932106645207086, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6730116670092565, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=5, rightside_frequency=0)\n",
      "accommodat Scores(cohesion_forward=0.6535053236339364, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      "res Scores(cohesion_forward=0.6005461994758755, cohesion_backward=0.19665714619400756, left_branching_entropy=-1.0154385726179807, right_branching_entropy=-0.5944235381774704, left_accessor_variety=4, right_accessor_variety=3, leftside_frequency=44, rightside_frequency=7)\n",
      "le Scores(cohesion_forward=0.4444444444444444, cohesion_backward=0.03, left_branching_entropy=-0.34883209584303193, right_branching_entropy=-0.3056257729600986, left_accessor_variety=2, right_accessor_variety=5, leftside_frequency=4, rightside_frequency=9)\n",
      "19 Scores(cohesion_forward=0.5714285714285714, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=4, rightside_frequency=0)\n",
      "gr Scores(cohesion_forward=0.6363636363636364, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6829081047004717, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=14, rightside_frequency=0)\n",
      "deci Scores(cohesion_forward=0.48332076050128514, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.410116318288409, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "reduc Scores(cohesion_forward=0.5600223424950026, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=12, rightside_frequency=0)\n",
      "com Scores(cohesion_forward=0.504524979109513, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.509137344082687, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=14, rightside_frequency=0)\n",
      "wi Scores(cohesion_forward=0.5357142857142857, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5799151714181009, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=0)\n",
      "las Scores(cohesion_forward=0.4714045207910317, cohesion_backward=0.18206913871032132, left_branching_entropy=-0.0, right_branching_entropy=-0.45274161561384274, left_accessor_variety=1, right_accessor_variety=3, leftside_frequency=2, rightside_frequency=6)\n",
      "pre Scores(cohesion_forward=0.3992747047523452, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.30463609734923813, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=11, rightside_frequency=0)\n",
      "cont Scores(cohesion_forward=0.5665163349427048, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6730116670092565, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "exp Scores(cohesion_forward=0.564932682866032, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6730116670092565, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=0)\n",
      "di Scores(cohesion_forward=0.5, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-0.5710227109429162, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=31, rightside_frequency=0)\n",
      "ma Scores(cohesion_forward=0.3384615384615385, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.18490739916777568, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=22, rightside_frequency=0)\n",
      "mon Scores(cohesion_forward=0.5547001962252291, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-0.7305880613711224, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=20, rightside_frequency=0)\n",
      "mode Scores(cohesion_forward=0.4757676344092838, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5982695885852573, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "int Scores(cohesion_forward=0.3308912980041792, cohesion_backward=0.0854357657716761, left_branching_entropy=-0.3776852850441228, right_branching_entropy=-0.24493002679463532, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=1)\n",
      "199 Scores(cohesion_forward=0.6546536707079771, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-1.0986122886681096, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=3, rightside_frequency=0)\n",
      "cha Scores(cohesion_forward=0.4264014327112209, cohesion_backward=0, left_branching_entropy=-0.6664058486402755, right_branching_entropy=-0.6730116670092565, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "con Scores(cohesion_forward=0.5393598899705937, cohesion_backward=0, left_branching_entropy=-0.9410307178997295, right_branching_entropy=-0.9214934308679616, left_accessor_variety=3, right_accessor_variety=3, leftside_frequency=16, rightside_frequency=0)\n",
      "tha Scores(cohesion_forward=0.2672612419124244, cohesion_backward=0, left_branching_entropy=-0.9376369622724492, right_branching_entropy=-0.22371807606583377, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=17, rightside_frequency=0)\n",
      "earl Scores(cohesion_forward=0.3996450752350969, cohesion_backward=0, left_branching_entropy=-0.5004024235381879, right_branching_entropy=-0.6365141682948128, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      "bo Scores(cohesion_forward=0.3972602739726027, cohesion_backward=0, left_branching_entropy=-0.4791656239282754, right_branching_entropy=-0.7175045098358934, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=29, rightside_frequency=0)\n",
      "sub Scores(cohesion_forward=0.4775669329409193, cohesion_backward=0, left_branching_entropy=-0.2711893730418441, right_branching_entropy=-0.9251290835720822, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=13, rightside_frequency=0)\n",
      "mo Scores(cohesion_forward=0.5230769230769231, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-1.0473436619739454, left_accessor_variety=2, right_accessor_variety=4, leftside_frequency=34, rightside_frequency=0)\n",
      "rece Scores(cohesion_forward=0.32006146360353477, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=4, rightside_frequency=0)\n",
      "su Scores(cohesion_forward=0.40350877192982454, cohesion_backward=0, left_branching_entropy=-0.30463609734923813, right_branching_entropy=-0.8259567010149779, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=23, rightside_frequency=0)\n",
      "dec Scores(cohesion_forward=0.421211769587116, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.9075352941050092, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=11, rightside_frequency=0)\n",
      "tent Scores(cohesion_forward=0.1613643827228219, cohesion_backward=0.30792608099883834, left_branching_entropy=-0.0, right_branching_entropy=-0.0, left_accessor_variety=1, right_accessor_variety=1, leftside_frequency=1, rightside_frequency=4)\n",
      "re Scores(cohesion_forward=0.7049180327868853, cohesion_backward=0.03333333333333333, left_branching_entropy=-0.8990442722588468, right_branching_entropy=-1.4874733957755382, left_accessor_variety=6, right_accessor_variety=7, leftside_frequency=86, rightside_frequency=10)\n",
      "ac Scores(cohesion_forward=0.21014492753623187, cohesion_backward=0, left_branching_entropy=-0.9745701894462542, right_branching_entropy=-0.3325942143118925, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=29, rightside_frequency=0)\n",
      "rel Scores(cohesion_forward=0.2862991671569341, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6931471805599453, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "ch Scores(cohesion_forward=0.21818181818181817, cohesion_backward=0.07407407407407407, left_branching_entropy=-0.38810736929464623, right_branching_entropy=-0.45056120886630463, left_accessor_variety=4, right_accessor_variety=2, leftside_frequency=12, rightside_frequency=2)\n",
      "go Scores(cohesion_forward=0.2727272727272727, cohesion_backward=0.043478260869565216, left_branching_entropy=-0.0, right_branching_entropy=-0.6931471805599453, left_accessor_variety=1, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=2)\n",
      "fr Scores(cohesion_forward=0.18681318681318682, cohesion_backward=0, left_branching_entropy=-0.34883209584303193, right_branching_entropy=-0.362210557135449, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=17, rightside_frequency=0)\n",
      "sho Scores(cohesion_forward=0.3504383220252312, cohesion_backward=0, left_branching_entropy=-0.45056120886630463, right_branching_entropy=-1.0042424730540764, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=7, rightside_frequency=0)\n",
      "pr Scores(cohesion_forward=0.2028985507246377, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5195798391305154, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=14, rightside_frequency=0)\n",
      "tak Scores(cohesion_forward=0.21498485387337868, cohesion_backward=0, left_branching_entropy=-0.5004024235381879, right_branching_entropy=-0.5859526183035508, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=11, rightside_frequency=0)\n",
      "ba Scores(cohesion_forward=0.273972602739726, cohesion_backward=0, left_branching_entropy=-0.3250829733914482, right_branching_entropy=-0.8568409950394724, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=20, rightside_frequency=0)\n",
      "wh Scores(cohesion_forward=0.21428571428571427, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=0)\n",
      "po Scores(cohesion_forward=0.36231884057971014, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-1.2505119933561475, left_accessor_variety=0, right_accessor_variety=4, leftside_frequency=25, rightside_frequency=0)\n",
      "de Scores(cohesion_forward=0.4032258064516129, cohesion_backward=0, left_branching_entropy=-0.438159320026818, right_branching_entropy=-1.4010689855534109, left_accessor_variety=2, right_accessor_variety=5, leftside_frequency=25, rightside_frequency=0)\n",
      "ea Scores(cohesion_forward=0.1702127659574468, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6615632381579821, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=8, rightside_frequency=0)\n",
      "thr Scores(cohesion_forward=0.11227217828476796, cohesion_backward=0, left_branching_entropy=-0.410116318288409, right_branching_entropy=-0.6365141682948128, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      "fo Scores(cohesion_forward=0.18681318681318682, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-1.1499171377909299, left_accessor_variety=2, right_accessor_variety=4, leftside_frequency=17, rightside_frequency=0)\n",
      "mi Scores(cohesion_forward=0.09230769230769231, cohesion_backward=0, left_branching_entropy=-0.2337916587064593, right_branching_entropy=-0.45056120886630463, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=0)\n",
      "ci Scores(cohesion_forward=0.10909090909090909, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=0)\n",
      "all Scores(cohesion_forward=0.08512565307587486, cohesion_backward=0.2401922307076307, left_branching_entropy=-0.0, right_branching_entropy=-0.5009153717361616, left_accessor_variety=1, right_accessor_variety=3, leftside_frequency=1, rightside_frequency=3)\n",
      "fu Scores(cohesion_forward=0.12087912087912088, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.9347698978582792, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=11, rightside_frequency=0)\n",
      "ag Scores(cohesion_forward=0.050724637681159424, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5982695885852573, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "si Scores(cohesion_forward=0.05263157894736842, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      "al Scores(cohesion_forward=0.07971014492753623, cohesion_backward=0.8653846153846154, left_branching_entropy=-0.39267446722755217, right_branching_entropy=-1.4010394588692539, left_accessor_variety=2, right_accessor_variety=10, leftside_frequency=11, rightside_frequency=45)\n"
     ]
    }
   ],
   "source": [
    "def word_score(score):\n",
    "    return score.cohesion_forward * math.exp(score.right_branching_entropy)\n",
    "\n",
    "for word, score in sorted(words.items(), key=lambda x: word_score(x[1]), reverse=True):\n",
    "    if word in all_words:\n",
    "        continue\n",
    "    print(word, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2ec1f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - inflationary\n",
      " - utilization\n",
      " - york\n",
      " - 1/4\n",
      " - 5-1/4\n",
      " - greenspan\n",
      " - discount\n",
      " - 25\n",
      " - committee\n",
      " - percent\n",
      " - reserve\n",
      " - sustainable\n",
      " - philadelphia\n",
      " - submitted\n",
      " - governors\n",
      " - expected\n",
      " - directors\n",
      " - minneapolis\n",
      " - effective\n",
      " - expansion\n",
      " - louis\n",
      " - pressure\n",
      " - contained\n",
      " - keep\n",
      " - inflation\n",
      " - immediately\n",
      " - 5-1/2\n",
      " - immediate\n",
      " - depository\n",
      " - institutions\n",
      " - conditions\n",
      " - consistent\n",
      "5- Scores(cohesion_forward=1.0, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.3250829733914482, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      " - reduction\n",
      " - short-term\n",
      " - announced\n",
      " - positions\n",
      " - interest\n",
      " - moderating\n",
      " - slightly\n",
      " - increase\n",
      " - reflected\n",
      " - announce\n",
      " - requests\n",
      " - following\n",
      " - district\n",
      " - potential\n",
      " - associated\n",
      " - markets\n",
      " - 1/2\n",
      " - continue\n",
      " - charged\n",
      " - actions\n",
      " - boards\n",
      " - easing\n",
      "th Scores(cohesion_forward=0.6974789915966386, cohesion_backward=0.7777777777777778, left_branching_entropy=-1.0235570898907984, right_branching_entropy=-0.6686752780804099, left_accessor_variety=7, right_accessor_variety=4, leftside_frequency=166, rightside_frequency=21)\n",
      " - dallas\n",
      " - degree\n",
      " - subdued\n",
      "al Scores(cohesion_forward=0.07971014492753623, cohesion_backward=0.8653846153846154, left_branching_entropy=-0.39267446722755217, right_branching_entropy=-1.4010394588692539, left_accessor_variety=2, right_accessor_variety=10, leftside_frequency=11, rightside_frequency=45)\n",
      " - resource\n",
      " - related\n",
      " - federal\n",
      " - kansas\n",
      " - forward\n",
      " - points\n",
      " - borrow\n",
      "ne Scores(cohesion_forward=0.9090909090909091, cohesion_backward=0.006666666666666667, left_branching_entropy=-0.0, right_branching_entropy=-0.5004024235381879, left_accessor_variety=1, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=2)\n",
      " - 3/4\n",
      " - months\n",
      " - reduced\n",
      " - monetary\n",
      " - already\n",
      " - boston\n",
      " - result\n",
      " - about\n",
      " - point\n",
      "ha Scores(cohesion_forward=0.7777777777777778, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.410116318288409, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      " - basis\n",
      "5-1/ Scores(cohesion_forward=0.9654893846056297, cohesion_backward=0, left_branching_entropy=-0.5004024235381879, right_branching_entropy=-0.6365141682948128, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=9, rightside_frequency=0)\n",
      " - cleveland\n",
      " - taking\n",
      "expect Scores(cohesion_forward=0.7185035699395549, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.34883209584303193, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=9, rightside_frequency=0)\n",
      " - thereby\n",
      " - market\n",
      " - approved\n",
      " - sustain\n",
      "per Scores(cohesion_forward=0.5773502691896257, cohesion_backward=0, left_branching_entropy=-1.0042424730540764, right_branching_entropy=-0.17884491271684755, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=23, rightside_frequency=0)\n",
      "las Scores(cohesion_forward=0.4714045207910317, cohesion_backward=0.18206913871032132, left_branching_entropy=-0.0, right_branching_entropy=-0.45274161561384274, left_accessor_variety=1, right_accessor_variety=3, leftside_frequency=2, rightside_frequency=6)\n",
      " - small\n",
      " - recent\n",
      "mar Scores(cohesion_forward=0.5683985600588051, cohesion_backward=0, left_branching_entropy=-0.45056120886630463, right_branching_entropy=-0.19144408195771734, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=21, rightside_frequency=0)\n",
      "tent Scores(cohesion_forward=0.1613643827228219, cohesion_backward=0.30792608099883834, left_branching_entropy=-0.0, right_branching_entropy=-0.0, left_accessor_variety=1, right_accessor_variety=1, leftside_frequency=1, rightside_frequency=4)\n",
      " - price\n",
      " - new\n",
      " - decided\n",
      " - board\n",
      "acti Scores(cohesion_forward=0.5732773111589102, cohesion_backward=0, left_branching_entropy=-0.410116318288409, right_branching_entropy=-0.2711893730418441, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=26, rightside_frequency=0)\n",
      "co Scores(cohesion_forward=0.6, cohesion_backward=0.043478260869565216, left_branching_entropy=-1.2947153675225072, right_branching_entropy=-0.35990548612823986, left_accessor_variety=5, right_accessor_variety=5, leftside_frequency=33, rightside_frequency=2)\n",
      " - slight\n",
      " - richmond\n",
      "res Scores(cohesion_forward=0.6005461994758755, cohesion_backward=0.19665714619400756, left_branching_entropy=-1.0154385726179807, right_branching_entropy=-0.5944235381774704, left_accessor_variety=4, right_accessor_variety=3, leftside_frequency=44, rightside_frequency=7)\n",
      "appro Scores(cohesion_forward=0.583526016818016, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.37677016125643675, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=16, rightside_frequency=0)\n",
      "dis Scores(cohesion_forward=0.6090712125322324, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.46203690946645704, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=23, rightside_frequency=0)\n",
      " - cost\n",
      "mone Scores(cohesion_forward=0.6133748537965212, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5004024235381879, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=0)\n",
      " - growth\n",
      " - going\n",
      "continu Scores(cohesion_forward=0.6460749217270753, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=4, rightside_frequency=0)\n",
      "inte Scores(cohesion_forward=0.4675224708569822, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.25731864054383163, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=14, rightside_frequency=0)\n",
      " - their\n",
      "moderati Scores(cohesion_forward=0.6932106645207086, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6730116670092565, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=5, rightside_frequency=0)\n",
      " - should\n",
      " - chairman\n",
      "le Scores(cohesion_forward=0.4444444444444444, cohesion_backward=0.03, left_branching_entropy=-0.34883209584303193, right_branching_entropy=-0.3056257729600986, left_accessor_variety=2, right_accessor_variety=5, leftside_frequency=4, rightside_frequency=9)\n",
      " - would\n",
      "accommodat Scores(cohesion_forward=0.6535053236339364, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      " - move\n",
      " - taken\n",
      " - agreed\n",
      "19 Scores(cohesion_forward=0.5714285714285714, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=4, rightside_frequency=0)\n",
      " - foster\n",
      "gr Scores(cohesion_forward=0.6363636363636364, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6829081047004717, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=14, rightside_frequency=0)\n",
      "deci Scores(cohesion_forward=0.48332076050128514, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.410116318288409, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "reduc Scores(cohesion_forward=0.5600223424950026, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=12, rightside_frequency=0)\n",
      " - action\n",
      "int Scores(cohesion_forward=0.3308912980041792, cohesion_backward=0.0854357657716761, left_branching_entropy=-0.3776852850441228, right_branching_entropy=-0.24493002679463532, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=1)\n",
      " - bank\n",
      " - economic\n",
      "com Scores(cohesion_forward=0.504524979109513, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.509137344082687, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=14, rightside_frequency=0)\n",
      "wi Scores(cohesion_forward=0.5357142857142857, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5799151714181009, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=0)\n",
      " - when\n",
      " - open\n",
      "pre Scores(cohesion_forward=0.3992747047523452, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.30463609734923813, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=11, rightside_frequency=0)\n",
      " - funds\n",
      " - fully\n",
      "all Scores(cohesion_forward=0.08512565307587486, cohesion_backward=0.2401922307076307, left_branching_entropy=-0.0, right_branching_entropy=-0.5009153717361616, left_accessor_variety=1, right_accessor_variety=3, leftside_frequency=1, rightside_frequency=3)\n",
      "cont Scores(cohesion_forward=0.5665163349427048, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6730116670092565, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "exp Scores(cohesion_forward=0.564932682866032, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6730116670092565, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=0)\n",
      "di Scores(cohesion_forward=0.5, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-0.5710227109429162, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=31, rightside_frequency=0)\n",
      " - trends\n",
      "ma Scores(cohesion_forward=0.3384615384615385, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.18490739916777568, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=22, rightside_frequency=0)\n",
      " - money\n",
      "mon Scores(cohesion_forward=0.5547001962252291, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-0.7305880613711224, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=20, rightside_frequency=0)\n",
      "mode Scores(cohesion_forward=0.4757676344092838, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5982695885852573, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      " - they\n",
      " - atlanta\n",
      " - has\n",
      " - that\n",
      " - release\n",
      " - pressures\n",
      "199 Scores(cohesion_forward=0.6546536707079771, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-1.0986122886681096, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=3, rightside_frequency=0)\n",
      "cha Scores(cohesion_forward=0.4264014327112209, cohesion_backward=0, left_branching_entropy=-0.6664058486402755, right_branching_entropy=-0.6730116670092565, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "con Scores(cohesion_forward=0.5393598899705937, cohesion_backward=0, left_branching_entropy=-0.9410307178997295, right_branching_entropy=-0.9214934308679616, left_accessor_variety=3, right_accessor_variety=3, leftside_frequency=16, rightside_frequency=0)\n",
      "tha Scores(cohesion_forward=0.2672612419124244, cohesion_backward=0, left_branching_entropy=-0.9376369622724492, right_branching_entropy=-0.22371807606583377, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=17, rightside_frequency=0)\n",
      "earl Scores(cohesion_forward=0.3996450752350969, cohesion_backward=0, left_branching_entropy=-0.5004024235381879, right_branching_entropy=-0.6365141682948128, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      " - policy\n",
      " - stance\n",
      "bo Scores(cohesion_forward=0.3972602739726027, cohesion_backward=0, left_branching_entropy=-0.4791656239282754, right_branching_entropy=-0.7175045098358934, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=29, rightside_frequency=0)\n",
      "sub Scores(cohesion_forward=0.4775669329409193, cohesion_backward=0, left_branching_entropy=-0.2711893730418441, right_branching_entropy=-0.9251290835720822, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=13, rightside_frequency=0)\n",
      "ch Scores(cohesion_forward=0.21818181818181817, cohesion_backward=0.07407407407407407, left_branching_entropy=-0.38810736929464623, right_branching_entropy=-0.45056120886630463, left_accessor_variety=4, right_accessor_variety=2, leftside_frequency=12, rightside_frequency=2)\n",
      " - alan\n",
      " - from\n",
      "mo Scores(cohesion_forward=0.5230769230769231, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-1.0473436619739454, left_accessor_variety=2, right_accessor_variety=4, leftside_frequency=34, rightside_frequency=0)\n",
      "rece Scores(cohesion_forward=0.32006146360353477, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=4, rightside_frequency=0)\n",
      "go Scores(cohesion_forward=0.2727272727272727, cohesion_backward=0.043478260869565216, left_branching_entropy=-0.0, right_branching_entropy=-0.6931471805599453, left_accessor_variety=1, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=2)\n",
      "su Scores(cohesion_forward=0.40350877192982454, cohesion_backward=0, left_branching_entropy=-0.30463609734923813, right_branching_entropy=-0.8259567010149779, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=23, rightside_frequency=0)\n",
      "re Scores(cohesion_forward=0.7049180327868853, cohesion_backward=0.03333333333333333, left_branching_entropy=-0.8990442722588468, right_branching_entropy=-1.4874733957755382, left_accessor_variety=6, right_accessor_variety=7, leftside_frequency=86, rightside_frequency=10)\n",
      " - today\n",
      "dec Scores(cohesion_forward=0.421211769587116, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.9075352941050092, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=11, rightside_frequency=0)\n",
      " - on\n",
      " - in\n",
      " - banks\n",
      " - rates\n",
      " - with\n",
      " - these\n",
      "ac Scores(cohesion_forward=0.21014492753623187, cohesion_backward=0, left_branching_entropy=-0.9745701894462542, right_branching_entropy=-0.3325942143118925, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=29, rightside_frequency=0)\n",
      "rel Scores(cohesion_forward=0.2862991671569341, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6931471805599453, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      " - will\n",
      " - the\n",
      " - an\n",
      "fr Scores(cohesion_forward=0.18681318681318682, cohesion_backward=0, left_branching_entropy=-0.34883209584303193, right_branching_entropy=-0.362210557135449, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=17, rightside_frequency=0)\n",
      "sho Scores(cohesion_forward=0.3504383220252312, cohesion_backward=0, left_branching_entropy=-0.45056120886630463, right_branching_entropy=-1.0042424730540764, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=7, rightside_frequency=0)\n",
      " - for\n",
      "pr Scores(cohesion_forward=0.2028985507246377, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5195798391305154, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=14, rightside_frequency=0)\n",
      "tak Scores(cohesion_forward=0.21498485387337868, cohesion_backward=0, left_branching_entropy=-0.5004024235381879, right_branching_entropy=-0.5859526183035508, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=11, rightside_frequency=0)\n",
      " - of\n",
      "ba Scores(cohesion_forward=0.273972602739726, cohesion_backward=0, left_branching_entropy=-0.3250829733914482, right_branching_entropy=-0.8568409950394724, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=20, rightside_frequency=0)\n",
      "wh Scores(cohesion_forward=0.21428571428571427, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=0)\n",
      " - city\n",
      " - rate\n",
      "po Scores(cohesion_forward=0.36231884057971014, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-1.2505119933561475, left_accessor_variety=0, right_accessor_variety=4, leftside_frequency=25, rightside_frequency=0)\n",
      " - this\n",
      "de Scores(cohesion_forward=0.4032258064516129, cohesion_backward=0, left_branching_entropy=-0.438159320026818, right_branching_entropy=-1.4010689855534109, left_accessor_variety=2, right_accessor_variety=5, leftside_frequency=25, rightside_frequency=0)\n",
      " - st\n",
      "ea Scores(cohesion_forward=0.1702127659574468, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6615632381579821, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=8, rightside_frequency=0)\n",
      " - and\n",
      " - be\n",
      " - is\n",
      " - at\n",
      "thr Scores(cohesion_forward=0.11227217828476796, cohesion_backward=0, left_branching_entropy=-0.410116318288409, right_branching_entropy=-0.6365141682948128, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      "fo Scores(cohesion_forward=0.18681318681318682, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-1.1499171377909299, left_accessor_variety=2, right_accessor_variety=4, leftside_frequency=17, rightside_frequency=0)\n",
      "mi Scores(cohesion_forward=0.09230769230769231, cohesion_backward=0, left_branching_entropy=-0.2337916587064593, right_branching_entropy=-0.45056120886630463, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=0)\n",
      "ci Scores(cohesion_forward=0.10909090909090909, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=0)\n",
      " - by\n",
      "fu Scores(cohesion_forward=0.12087912087912088, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.9347698978582792, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=11, rightside_frequency=0)\n",
      " - as\n",
      " - to\n",
      "ag Scores(cohesion_forward=0.050724637681159424, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5982695885852573, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "si Scores(cohesion_forward=0.05263157894736842, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      " - so\n"
     ]
    }
   ],
   "source": [
    "def word_score(score):\n",
    "    return score.cohesion_backward * math.exp(\n",
    "        score.left_branching_entropy\n",
    "    ) + score.cohesion_forward * math.exp(score.right_branching_entropy)\n",
    "\n",
    "\n",
    "for word, score in sorted(words.items(), key=lambda x: word_score(x[1]), reverse=True):\n",
    "    if word in all_words:\n",
    "        print(f\" - {word}\")\n",
    "        continue\n",
    "    print(word, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1ae35884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "al Scores(cohesion_forward=0.07971014492753623, cohesion_backward=0.8653846153846154, left_branching_entropy=-0.39267446722755217, right_branching_entropy=-1.4010394588692539, left_accessor_variety=2, right_accessor_variety=10, leftside_frequency=11, rightside_frequency=45)\n",
      "tent Scores(cohesion_forward=0.1613643827228219, cohesion_backward=0.30792608099883834, left_branching_entropy=-0.0, right_branching_entropy=-0.0, left_accessor_variety=1, right_accessor_variety=1, leftside_frequency=1, rightside_frequency=4)\n",
      "th Scores(cohesion_forward=0.6974789915966386, cohesion_backward=0.7777777777777778, left_branching_entropy=-1.0235570898907984, right_branching_entropy=-0.6686752780804099, left_accessor_variety=7, right_accessor_variety=4, leftside_frequency=166, rightside_frequency=21)\n",
      "all Scores(cohesion_forward=0.08512565307587486, cohesion_backward=0.2401922307076307, left_branching_entropy=-0.0, right_branching_entropy=-0.5009153717361616, left_accessor_variety=1, right_accessor_variety=3, leftside_frequency=1, rightside_frequency=3)\n",
      "las Scores(cohesion_forward=0.4714045207910317, cohesion_backward=0.18206913871032132, left_branching_entropy=-0.0, right_branching_entropy=-0.45274161561384274, left_accessor_variety=1, right_accessor_variety=3, leftside_frequency=2, rightside_frequency=6)\n",
      "res Scores(cohesion_forward=0.6005461994758755, cohesion_backward=0.19665714619400756, left_branching_entropy=-1.0154385726179807, right_branching_entropy=-0.5944235381774704, left_accessor_variety=4, right_accessor_variety=3, leftside_frequency=44, rightside_frequency=7)\n",
      "int Scores(cohesion_forward=0.3308912980041792, cohesion_backward=0.0854357657716761, left_branching_entropy=-0.3776852850441228, right_branching_entropy=-0.24493002679463532, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=1)\n",
      "ch Scores(cohesion_forward=0.21818181818181817, cohesion_backward=0.07407407407407407, left_branching_entropy=-0.38810736929464623, right_branching_entropy=-0.45056120886630463, left_accessor_variety=4, right_accessor_variety=2, leftside_frequency=12, rightside_frequency=2)\n",
      "go Scores(cohesion_forward=0.2727272727272727, cohesion_backward=0.043478260869565216, left_branching_entropy=-0.0, right_branching_entropy=-0.6931471805599453, left_accessor_variety=1, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=2)\n",
      "le Scores(cohesion_forward=0.4444444444444444, cohesion_backward=0.03, left_branching_entropy=-0.34883209584303193, right_branching_entropy=-0.3056257729600986, left_accessor_variety=2, right_accessor_variety=5, leftside_frequency=4, rightside_frequency=9)\n",
      "re Scores(cohesion_forward=0.7049180327868853, cohesion_backward=0.03333333333333333, left_branching_entropy=-0.8990442722588468, right_branching_entropy=-1.4874733957755382, left_accessor_variety=6, right_accessor_variety=7, leftside_frequency=86, rightside_frequency=10)\n",
      "co Scores(cohesion_forward=0.6, cohesion_backward=0.043478260869565216, left_branching_entropy=-1.2947153675225072, right_branching_entropy=-0.35990548612823986, left_accessor_variety=5, right_accessor_variety=5, leftside_frequency=33, rightside_frequency=2)\n",
      "ne Scores(cohesion_forward=0.9090909090909091, cohesion_backward=0.006666666666666667, left_branching_entropy=-0.0, right_branching_entropy=-0.5004024235381879, left_accessor_variety=1, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=2)\n",
      "wh Scores(cohesion_forward=0.21428571428571427, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=0)\n",
      "po Scores(cohesion_forward=0.36231884057971014, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-1.2505119933561475, left_accessor_variety=0, right_accessor_variety=4, leftside_frequency=25, rightside_frequency=0)\n",
      "mi Scores(cohesion_forward=0.09230769230769231, cohesion_backward=0, left_branching_entropy=-0.2337916587064593, right_branching_entropy=-0.45056120886630463, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=0)\n",
      "mo Scores(cohesion_forward=0.5230769230769231, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-1.0473436619739454, left_accessor_variety=2, right_accessor_variety=4, leftside_frequency=34, rightside_frequency=0)\n",
      "bo Scores(cohesion_forward=0.3972602739726027, cohesion_backward=0, left_branching_entropy=-0.4791656239282754, right_branching_entropy=-0.7175045098358934, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=29, rightside_frequency=0)\n",
      "ci Scores(cohesion_forward=0.10909090909090909, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=6, rightside_frequency=0)\n",
      "fr Scores(cohesion_forward=0.18681318681318682, cohesion_backward=0, left_branching_entropy=-0.34883209584303193, right_branching_entropy=-0.362210557135449, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=17, rightside_frequency=0)\n",
      "ha Scores(cohesion_forward=0.7777777777777778, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.410116318288409, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "ag Scores(cohesion_forward=0.050724637681159424, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5982695885852573, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "wi Scores(cohesion_forward=0.5357142857142857, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5799151714181009, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=0)\n",
      "19 Scores(cohesion_forward=0.5714285714285714, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=4, rightside_frequency=0)\n",
      "di Scores(cohesion_forward=0.5, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-0.5710227109429162, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=31, rightside_frequency=0)\n",
      "fu Scores(cohesion_forward=0.12087912087912088, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.9347698978582792, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=11, rightside_frequency=0)\n",
      "gr Scores(cohesion_forward=0.6363636363636364, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6829081047004717, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=14, rightside_frequency=0)\n",
      "ac Scores(cohesion_forward=0.21014492753623187, cohesion_backward=0, left_branching_entropy=-0.9745701894462542, right_branching_entropy=-0.3325942143118925, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=29, rightside_frequency=0)\n",
      "si Scores(cohesion_forward=0.05263157894736842, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      "su Scores(cohesion_forward=0.40350877192982454, cohesion_backward=0, left_branching_entropy=-0.30463609734923813, right_branching_entropy=-0.8259567010149779, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=23, rightside_frequency=0)\n",
      "pr Scores(cohesion_forward=0.2028985507246377, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5195798391305154, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=14, rightside_frequency=0)\n",
      "5- Scores(cohesion_forward=1.0, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.3250829733914482, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "ma Scores(cohesion_forward=0.3384615384615385, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.18490739916777568, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=22, rightside_frequency=0)\n",
      "fo Scores(cohesion_forward=0.18681318681318682, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-1.1499171377909299, left_accessor_variety=2, right_accessor_variety=4, leftside_frequency=17, rightside_frequency=0)\n",
      "ba Scores(cohesion_forward=0.273972602739726, cohesion_backward=0, left_branching_entropy=-0.3250829733914482, right_branching_entropy=-0.8568409950394724, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=20, rightside_frequency=0)\n",
      "ea Scores(cohesion_forward=0.1702127659574468, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6615632381579821, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=8, rightside_frequency=0)\n",
      "de Scores(cohesion_forward=0.4032258064516129, cohesion_backward=0, left_branching_entropy=-0.438159320026818, right_branching_entropy=-1.4010689855534109, left_accessor_variety=2, right_accessor_variety=5, leftside_frequency=25, rightside_frequency=0)\n",
      "rel Scores(cohesion_forward=0.2862991671569341, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6931471805599453, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "cha Scores(cohesion_forward=0.4264014327112209, cohesion_backward=0, left_branching_entropy=-0.6664058486402755, right_branching_entropy=-0.6730116670092565, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "sho Scores(cohesion_forward=0.3504383220252312, cohesion_backward=0, left_branching_entropy=-0.45056120886630463, right_branching_entropy=-1.0042424730540764, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=7, rightside_frequency=0)\n",
      "thr Scores(cohesion_forward=0.11227217828476796, cohesion_backward=0, left_branching_entropy=-0.410116318288409, right_branching_entropy=-0.6365141682948128, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      "tha Scores(cohesion_forward=0.2672612419124244, cohesion_backward=0, left_branching_entropy=-0.9376369622724492, right_branching_entropy=-0.22371807606583377, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=17, rightside_frequency=0)\n",
      "sub Scores(cohesion_forward=0.4775669329409193, cohesion_backward=0, left_branching_entropy=-0.2711893730418441, right_branching_entropy=-0.9251290835720822, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=13, rightside_frequency=0)\n",
      "per Scores(cohesion_forward=0.5773502691896257, cohesion_backward=0, left_branching_entropy=-1.0042424730540764, right_branching_entropy=-0.17884491271684755, left_accessor_variety=3, right_accessor_variety=2, leftside_frequency=23, rightside_frequency=0)\n",
      "com Scores(cohesion_forward=0.504524979109513, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.509137344082687, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=14, rightside_frequency=0)\n",
      "dec Scores(cohesion_forward=0.421211769587116, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.9075352941050092, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=11, rightside_frequency=0)\n",
      "exp Scores(cohesion_forward=0.564932682866032, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6730116670092565, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=0)\n",
      "mon Scores(cohesion_forward=0.5547001962252291, cohesion_backward=0, left_branching_entropy=-0.37677016125643675, right_branching_entropy=-0.7305880613711224, left_accessor_variety=2, right_accessor_variety=3, leftside_frequency=20, rightside_frequency=0)\n",
      "tak Scores(cohesion_forward=0.21498485387337868, cohesion_backward=0, left_branching_entropy=-0.5004024235381879, right_branching_entropy=-0.5859526183035508, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=11, rightside_frequency=0)\n",
      "con Scores(cohesion_forward=0.5393598899705937, cohesion_backward=0, left_branching_entropy=-0.9410307178997295, right_branching_entropy=-0.9214934308679616, left_accessor_variety=3, right_accessor_variety=3, leftside_frequency=16, rightside_frequency=0)\n",
      "199 Scores(cohesion_forward=0.6546536707079771, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-1.0986122886681096, left_accessor_variety=0, right_accessor_variety=3, leftside_frequency=3, rightside_frequency=0)\n",
      "pre Scores(cohesion_forward=0.3992747047523452, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.30463609734923813, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=11, rightside_frequency=0)\n",
      "dis Scores(cohesion_forward=0.6090712125322324, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.46203690946645704, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=23, rightside_frequency=0)\n",
      "mar Scores(cohesion_forward=0.5683985600588051, cohesion_backward=0, left_branching_entropy=-0.45056120886630463, right_branching_entropy=-0.19144408195771734, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=21, rightside_frequency=0)\n",
      "earl Scores(cohesion_forward=0.3996450752350969, cohesion_backward=0, left_branching_entropy=-0.5004024235381879, right_branching_entropy=-0.6365141682948128, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n",
      "rece Scores(cohesion_forward=0.32006146360353477, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=4, rightside_frequency=0)\n",
      "5-1/ Scores(cohesion_forward=0.9654893846056297, cohesion_backward=0, left_branching_entropy=-0.5004024235381879, right_branching_entropy=-0.6365141682948128, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=9, rightside_frequency=0)\n",
      "mode Scores(cohesion_forward=0.4757676344092838, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5982695885852573, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "deci Scores(cohesion_forward=0.48332076050128514, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.410116318288409, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0)\n",
      "acti Scores(cohesion_forward=0.5732773111589102, cohesion_backward=0, left_branching_entropy=-0.410116318288409, right_branching_entropy=-0.2711893730418441, left_accessor_variety=2, right_accessor_variety=2, leftside_frequency=26, rightside_frequency=0)\n",
      "inte Scores(cohesion_forward=0.4675224708569822, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.25731864054383163, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=14, rightside_frequency=0)\n",
      "mone Scores(cohesion_forward=0.6133748537965212, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5004024235381879, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=15, rightside_frequency=0)\n",
      "cont Scores(cohesion_forward=0.5665163349427048, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6730116670092565, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=10, rightside_frequency=0)\n",
      "reduc Scores(cohesion_forward=0.5600223424950026, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=12, rightside_frequency=0)\n",
      "appro Scores(cohesion_forward=0.583526016818016, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.37677016125643675, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=16, rightside_frequency=0)\n",
      "expect Scores(cohesion_forward=0.7185035699395549, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.34883209584303193, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=9, rightside_frequency=0)\n",
      "continu Scores(cohesion_forward=0.6460749217270753, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.5623351446188083, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=4, rightside_frequency=0)\n",
      "moderati Scores(cohesion_forward=0.6932106645207086, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6730116670092565, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=5, rightside_frequency=0)\n",
      "accommodat Scores(cohesion_forward=0.6535053236339364, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=-0.6365141682948128, left_accessor_variety=0, right_accessor_variety=2, leftside_frequency=3, rightside_frequency=0)\n"
     ]
    }
   ],
   "source": [
    "def word_score(score):\n",
    "    return score.cohesion_backward * math.exp(score.left_branching_entropy)\n",
    "\n",
    "\n",
    "for word, score in sorted(words.items(), key=lambda x: word_score(x[1]), reverse=True):\n",
    "    if word in all_words:\n",
    "        continue\n",
    "    print(word, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981dc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "609f8bc3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Uncertanty to word boundary; Accessor Variety & Branching Entropy](https://lovit.github.io/nlp/2018/04/09/branching_entropy_accessor_variety/)\n",
    "- [Fast Word Segmentation of Noisy Text](https://medium.com/towards-data-science/fast-word-segmentation-for-noisy-text-2c2c41f9e8da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9265f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
