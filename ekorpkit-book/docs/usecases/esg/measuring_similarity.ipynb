{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AESO0odcxhzs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Measuring Document Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T12:02:50.293005Z",
     "iopub.status.busy": "2023-02-04T12:02:50.292726Z",
     "iopub.status.idle": "2023-02-04T12:02:53.079486Z",
     "shell.execute_reply": "2023-02-04T12:02:53.078645Z",
     "shell.execute_reply.started": "2023-02-04T12:02:50.292985Z"
    },
    "id": "EJ0AoNl-xif_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.env:Set environment variable EKORPKIT_DATA_ROOT=/workspace/data\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable CACHED_PATH_CACHE_ROOT=/workspace/.cache/cached_path\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable WANDB_DIR=/workspace/projects/ekorpkit-book/exmaples/logs\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable WANDB_PROJECT=ekorpkit-book-exmaples\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable WANDB_NOTEBOOK_NAME=/workspace/projects/ekorpkit-book/exmaples/logs/esg-nb\n",
      "INFO:ekorpkit.hyfi.env:Set environment variable WANDB_SILENT=False\n",
      "INFO:ekorpkit.hyfi.utils.env:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n",
      "INFO:ekorpkit.hyfi.hydra:initialized batcher with <ekorpkit.hyfi.utils.batch.batcher.Batcher object at 0x7fb0c31b2c10>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0.1.40.post0.dev98\n",
      "project_dir: /workspace/projects/ekorpkit-book/exmaples\n",
      "time: 1.21 s (started: 2023-02-10 09:29:29 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "if eKonf.is_colab():\n",
    "    eKonf.mount_google_drive()\n",
    "ws = eKonf.set_workspace(\n",
    "    workspace=\"/workspace\", \n",
    "    project=\"ekorpkit-book/exmaples\", \n",
    "    task=\"esg\", \n",
    "    log_level=\"INFO\",\n",
    "    verbose=True\n",
    ")\n",
    "print(\"version:\", ws.version)\n",
    "print(\"project_dir:\", ws.project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.env:config_module: ekorpkit.conf\n",
      "INFO:ekorpkit.hyfi.env:compose config with overrides: ['+preprocessor/normalizer=formal_ko']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ftfy': {'unescape_html': True, 'remove_terminal_escapes': True, 'fix_encoding': True, 'restore_byte_a0': True, 'replace_lossy_sequences': True, 'decode_inconsistent_utf8': True, 'fix_c1_controls': True, 'fix_latin_ligatures': True, 'fix_character_width': True, 'uncurl_quotes': True, 'fix_line_breaks': True, 'fix_surrogates': True, 'remove_control_chars': True, 'normalization': 'NFKC', 'max_decode_length': 1000000}, 'spaces': {'strip': True, 'fix_whitespaces': True, 'collapse_whitespaces': True, 'replace_tabs': True, 'num_spaces_for_tab': 4}, 'special_characters': {'fix_hyphens': True, 'fix_ellipsis': True, 'fix_slashes': True, 'fix_tildes': True, 'fix_emoticons': False, 'single_quotes_only': False, 'regular_parentheses_only': False}, '_target_': 'ekorpkit.preprocessors.normalizer.Normalizer', 'hanja2hangle': True, 'num_repeats': 2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 167 ms (started: 2023-02-10 09:29:36 +00:00)\n"
     ]
    }
   ],
   "source": [
    "eKonf.compose(\"preprocessor/normalizer=formal_ko\", verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to measure similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T12:03:06.188123Z",
     "iopub.status.busy": "2023-02-04T12:03:06.187109Z",
     "iopub.status.idle": "2023-02-04T12:03:13.047506Z",
     "shell.execute_reply": "2023-02-04T12:03:13.046637Z",
     "shell.execute_reply.started": "2023-02-04T12:03:06.188091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.io.file:Saving dataframe to /workspace/projects/ekorpkit-book/exmaples/esg/data/similarity/source_data.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 14s (started: 2023-02-10 09:29:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "news_data_dir = ws.project_dir / \"esg/data/econ_news_kr/news_slice\"\n",
    "filename = \"esg_news_valid_20221229.parquet\"\n",
    "\n",
    "valid_data = eKonf.load_data(filename, news_data_dir)\n",
    "id_cols = [\"filename\", \"codes\", \"chunk_id\"]\n",
    "valid_data.chunk_id = valid_data.chunk_id.astype(str)\n",
    "valid_data[\"doc_id\"] = valid_data[id_cols].apply(lambda x: \"_\".join(x), axis=1)\n",
    "\n",
    "source_data_file = ws.project_dir / \"esg/data/similarity/source_data.parquet\"\n",
    "eKonf.save_data(valid_data, source_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135590</th>\n",
       "      <td>02100101.20200703114528002.txt_101360_0</td>\n",
       "      <td>\"이엔드디는 그린 신소재 기업으로 정부가 추진하는 그린 뉴딜 정책의 수혜가 기대된...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30748</th>\n",
       "      <td>02100851.20200211070816001.txt_051910_0</td>\n",
       "      <td>국제 신용평가사 무디스는 LG화학의 장기 신용등급을 한 단계 낮췄다   10일 기...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677787</th>\n",
       "      <td>02100601.20211111172031002.txt_271560_8</td>\n",
       "      <td>오리온은 국내 가격을 동결하고 해외에서는 일부 제품 가격을 올리는 투 트랙 전략을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291166</th>\n",
       "      <td>02100601.20220128080812001.txt_270870_0</td>\n",
       "      <td>상상인증권은 28일 뉴트리가 올해 온라인 매출 비중을 확대하며 수익성을 개선할 것...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69410</th>\n",
       "      <td>02100311.20200415060111001.txt_005930_6</td>\n",
       "      <td>삼성전자는 이 같은 YMTC의 추격에 특유의 ‘초격차’로 대응할 방침이다  삼성전자...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         doc_id  \\\n",
       "135590  02100101.20200703114528002.txt_101360_0   \n",
       "30748   02100851.20200211070816001.txt_051910_0   \n",
       "677787  02100601.20211111172031002.txt_271560_8   \n",
       "291166  02100601.20220128080812001.txt_270870_0   \n",
       "69410   02100311.20200415060111001.txt_005930_6   \n",
       "\n",
       "                                                     text  \n",
       "135590   \"이엔드디는 그린 신소재 기업으로 정부가 추진하는 그린 뉴딜 정책의 수혜가 기대된...  \n",
       "30748    국제 신용평가사 무디스는 LG화학의 장기 신용등급을 한 단계 낮췄다   10일 기...  \n",
       "677787  오리온은 국내 가격을 동결하고 해외에서는 일부 제품 가격을 올리는 투 트랙 전략을 ...  \n",
       "291166   상상인증권은 28일 뉴트리가 올해 온라인 매출 비중을 확대하며 수익성을 개선할 것...  \n",
       "69410   삼성전자는 이 같은 YMTC의 추격에 특유의 ‘초격차’로 대응할 방침이다  삼성전자...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.63 s (started: 2023-02-10 09:30:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# load source data\n",
    "source_data_file = ws.project_dir / \"esg/data/similarity/source_data.parquet\"\n",
    "data = eKonf.load_data(source_data_file)\n",
    "cols = [\"doc_id\", \"text\"]\n",
    "data = data[cols].sample(1000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.preprocessors.tokenizer:Initializing mecab with {'userdic_path': '/workspace/projects/ekorpkit/ekorpkit/resources/dictionaries/mecab/ekon_v1.dic', 'backend': 'mecab-python3', 'verbose': True}...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:instantiating ekorpkit.preprocessors.normalizer.Normalizer...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:instantiating ekorpkit.preprocessors.stopwords.Stopwords...\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.stopwords.Stopwords ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:MecabTokenizer initialized with:\n",
      "INFO:ekorpkit.preprocessors.tokenizer:\treturn_as_list: False\n",
      "INFO:ekorpkit.tokenizers.mecab:MeCab uses mecab-python3 as backend.\n",
      "INFO:ekorpkit.tokenizers.mecab:Mecab uses system dictionary: /opt/conda/lib/python3.8/site-packages/mecab_ko_dic/dicdir, user dictionary: /workspace/projects/ekorpkit/ekorpkit/resources/dictionaries/mecab/ekon_v1.dic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 877 ms (started: 2023-02-10 09:31:00 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cfg_norm = eKonf.compose(\"preprocessor/normalizer=formal_ko\")\n",
    "cfg_mcb = eKonf.compose(\"preprocessor/tokenizer=mecab_econ\")\n",
    "cfg_mcb.normalize = cfg_norm\n",
    "mecab = eKonf.instantiate(cfg_mcb, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.pipe:Applying pipe: functools.partial(<function tokenize at 0x7faa3a177310>)\n",
      "INFO:ekorpkit.pipelines.pipe:instantiating tokenizer\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.tokenizer.MecabTokenizer ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:Initializing mecab with {'userdic_path': None, 'backend': 'mecab-python3', 'verbose': True}...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:instantiating ekorpkit.preprocessors.stopwords.Stopwords...\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.stopwords.Stopwords ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:MecabTokenizer initialized with:\n",
      "INFO:ekorpkit.preprocessors.tokenizer:\treturn_as_list: False\n",
      "INFO:ekorpkit.tokenizers.mecab:MeCab uses mecab-python3 as backend.\n",
      "INFO:ekorpkit.tokenizers.mecab:Mecab uses system dictionary: /opt/conda/lib/python3.8/site-packages/mecab_ko_dic/dicdir, user dictionary: None\n",
      "INFO:ekorpkit.hyfi.pipe:Using batcher with minibatch size: 21\n",
      "INFO:ekorpkit.hyfi.utils.batch.batcher: backend: joblib  minibatch_size: 21  procs: 50  input_split: False  merge_output: True  len(data): 1000 len(args): 5\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006125926971435547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Tokenizing column: text",
       "rate": null,
       "total": 48,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42081afc814c4db9b7f0528bf0b41405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing column: text:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.pipelines.pipe: >> elapsed time to segment: 0:00:04.993487\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135590</th>\n",
       "      <td>02100101.20200703114528002.txt_101360_0</td>\n",
       "      <td>\"/SY 이/JKS 엔드/NNG 디/NNG 는/JX /SP 그린/VV+ETM /SP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30748</th>\n",
       "      <td>02100851.20200211070816001.txt_051910_0</td>\n",
       "      <td>국제/NNG /SP 신용/NNG 평가/NNG 사/VV+EC /SP 무디스/NNP 는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677787</th>\n",
       "      <td>02100601.20211111172031002.txt_271560_8</td>\n",
       "      <td>오리온/NNP 은/JX /SP 국내/NNG /SP 가격/NNG 을/JKO /SP 동...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291166</th>\n",
       "      <td>02100601.20220128080812001.txt_270870_0</td>\n",
       "      <td>상상/NNG 인증/NNG 권/XSN 은/JX /SP 28/SN 일/NNBC /SP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69410</th>\n",
       "      <td>02100311.20200415060111001.txt_005930_6</td>\n",
       "      <td>삼성전자/NNP 는/JX /SP 이/MM /SP 같/VA 은/ETM /SP YMTC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         doc_id  \\\n",
       "135590  02100101.20200703114528002.txt_101360_0   \n",
       "30748   02100851.20200211070816001.txt_051910_0   \n",
       "677787  02100601.20211111172031002.txt_271560_8   \n",
       "291166  02100601.20220128080812001.txt_270870_0   \n",
       "69410   02100311.20200415060111001.txt_005930_6   \n",
       "\n",
       "                                                     text  \n",
       "135590  \"/SY 이/JKS 엔드/NNG 디/NNG 는/JX /SP 그린/VV+ETM /SP...  \n",
       "30748   국제/NNG /SP 신용/NNG 평가/NNG 사/VV+EC /SP 무디스/NNP 는...  \n",
       "677787  오리온/NNP 은/JX /SP 국내/NNG /SP 가격/NNG 을/JKO /SP 동...  \n",
       "291166  상상/NNG 인증/NNG 권/XSN 은/JX /SP 28/SN 일/NNBC /SP ...  \n",
       "69410   삼성전자/NNP 는/JX /SP 이/MM /SP 같/VA 은/ETM /SP YMTC...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.24 s (started: 2023-02-10 09:31:10 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "\n",
    "cfg = eKonf.compose(\"pipeline/tokenize\")\n",
    "data = eKonf.pipe(data, cfg)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.hyfi.pipe:Applying pipe: functools.partial(<function extract_tokens at 0x7faa3a1773a0>)\n",
      "INFO:ekorpkit.pipelines.pipe:instantiating tokenizer\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.tokenizer.MecabTokenizer ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:Initializing mecab with {'userdic_path': '/workspace/projects/ekorpkit/ekorpkit/resources/dictionaries/mecab/ekon_v1.dic', 'backend': 'mecab-python3', 'verbose': True}...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:instantiating ekorpkit.preprocessors.stopwords.Stopwords...\n",
      "INFO:ekorpkit.hyfi.hydra:instantiating ekorpkit.preprocessors.stopwords.Stopwords ...\n",
      "INFO:ekorpkit.preprocessors.tokenizer:MecabTokenizer initialized with:\n",
      "INFO:ekorpkit.preprocessors.tokenizer:\treturn_as_list: False\n",
      "INFO:ekorpkit.tokenizers.mecab:MeCab uses mecab-python3 as backend.\n",
      "INFO:ekorpkit.tokenizers.mecab:Mecab uses system dictionary: /opt/conda/lib/python3.8/site-packages/mecab_ko_dic/dicdir, user dictionary: /workspace/projects/ekorpkit/ekorpkit/resources/dictionaries/mecab/ekon_v1.dic\n",
      "INFO:ekorpkit.pipelines.pipe:extract_func: <bound method Tokenizer.extract_tokens of <ekorpkit.preprocessors.tokenizer.MecabTokenizer object at 0x7faa38796520>>\n",
      "INFO:ekorpkit.hyfi.pipe:Using batcher with minibatch size: 21\n",
      "INFO:ekorpkit.hyfi.utils.batch.batcher: backend: joblib  minibatch_size: 21  procs: 50  input_split: False  merge_output: True  len(data): 1000 len(args): 5\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0070531368255615234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting column: text",
       "rate": null,
       "total": 48,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21175835676a479ba01570c30873c377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting column: text:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.pipelines.pipe: >> elapsed time to extract tokens: 0:00:00.777058\n",
      "INFO:ekorpkit.hyfi.io.file:Saving dataframe to /workspace/projects/ekorpkit-book/exmaples/esg/data/similarity/tokenized_data.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.33 s (started: 2023-02-10 09:31:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Extract tokens\n",
    "# stopwords_file = ws.project_dir / \"esg/data/stopwords/stopwords.txt\"\n",
    "tkn_cfg = eKonf.compose(\"preprocessor/tokenizer=mecab_econ\")\n",
    "# tkn_cfg.extract.strip_pos = False\n",
    "\n",
    "cfg = eKonf.compose(\"pipeline/extract_tokens\")\n",
    "cfg.preprocessor.tokenizer = tkn_cfg\n",
    "cfg.nouns_only = False\n",
    "# cfg.stopwords_path = str(stopwords_file)\n",
    "# eKonf.print(cfg)\n",
    "data = eKonf.pipe(data, cfg)\n",
    "\n",
    "tokenized_data_file = ws.project_dir / \"esg/data/similarity/tokenized_data.parquet\"\n",
    "eKonf.save_data(data, tokenized_data_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict similarity\n",
    "\n",
    "Similarity will be measured among the news articles on the same day. The similarity is measured by the cosine similarity of the document vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id_1</th>\n",
       "      <th>doc_id_2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02100101.20200703114528002.txt_101360_0</td>\n",
       "      <td>02100851.20200211070816001.txt_051910_0</td>\n",
       "      <td>0.109638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02100101.20200703114528002.txt_101360_0</td>\n",
       "      <td>02100601.20211111172031002.txt_271560_8</td>\n",
       "      <td>0.120655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02100101.20200703114528002.txt_101360_0</td>\n",
       "      <td>02100601.20220128080812001.txt_270870_0</td>\n",
       "      <td>0.114443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02100101.20200703114528002.txt_101360_0</td>\n",
       "      <td>02100311.20200415060111001.txt_005930_6</td>\n",
       "      <td>0.092372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02100101.20200703114528002.txt_101360_0</td>\n",
       "      <td>02100601.20211014101325001.txt_001680_0</td>\n",
       "      <td>0.108429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499495</th>\n",
       "      <td>02100501.20201210175145002.txt_003670_3</td>\n",
       "      <td>02100701.20210527101452001.txt_007070_2</td>\n",
       "      <td>0.017959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499496</th>\n",
       "      <td>02100501.20201210175145002.txt_003670_3</td>\n",
       "      <td>02100701.20210128164141001.txt_066570_0</td>\n",
       "      <td>0.035580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499497</th>\n",
       "      <td>02100601.20210528162407001.txt_008770_1</td>\n",
       "      <td>02100701.20210527101452001.txt_007070_2</td>\n",
       "      <td>0.034574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499498</th>\n",
       "      <td>02100601.20210528162407001.txt_008770_1</td>\n",
       "      <td>02100701.20210128164141001.txt_066570_0</td>\n",
       "      <td>0.067384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499499</th>\n",
       "      <td>02100701.20210527101452001.txt_007070_2</td>\n",
       "      <td>02100701.20210128164141001.txt_066570_0</td>\n",
       "      <td>0.015912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       doc_id_1  \\\n",
       "0       02100101.20200703114528002.txt_101360_0   \n",
       "1       02100101.20200703114528002.txt_101360_0   \n",
       "2       02100101.20200703114528002.txt_101360_0   \n",
       "3       02100101.20200703114528002.txt_101360_0   \n",
       "4       02100101.20200703114528002.txt_101360_0   \n",
       "...                                         ...   \n",
       "499495  02100501.20201210175145002.txt_003670_3   \n",
       "499496  02100501.20201210175145002.txt_003670_3   \n",
       "499497  02100601.20210528162407001.txt_008770_1   \n",
       "499498  02100601.20210528162407001.txt_008770_1   \n",
       "499499  02100701.20210527101452001.txt_007070_2   \n",
       "\n",
       "                                       doc_id_2  similarity  \n",
       "0       02100851.20200211070816001.txt_051910_0    0.109638  \n",
       "1       02100601.20211111172031002.txt_271560_8    0.120655  \n",
       "2       02100601.20220128080812001.txt_270870_0    0.114443  \n",
       "3       02100311.20200415060111001.txt_005930_6    0.092372  \n",
       "4       02100601.20211014101325001.txt_001680_0    0.108429  \n",
       "...                                         ...         ...  \n",
       "499495  02100701.20210527101452001.txt_007070_2    0.017959  \n",
       "499496  02100701.20210128164141001.txt_066570_0    0.035580  \n",
       "499497  02100701.20210527101452001.txt_007070_2    0.034574  \n",
       "499498  02100701.20210128164141001.txt_066570_0    0.067384  \n",
       "499499  02100701.20210128164141001.txt_066570_0    0.015912  \n",
       "\n",
       "[499500 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.5 s (started: 2023-02-10 09:48:22 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "tokenized_data_file = ws.project_dir / \"esg/data/similarity/tokenized_data.parquet\"\n",
    "data = eKonf.load_data(tokenized_data_file)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Create the TF-IDF matrix using the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(\" \"))\n",
    "tfidf_matrix = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Calculate the cosine similarity between the documents\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Get the indices of the upper triangle of the cosine similarity matrix\n",
    "upper_triangle_indices = np.triu_indices_from(cosine_similarities, k=1)\n",
    "\n",
    "# Get the document ID pairs and similarity values from the cosine similarity matrix\n",
    "doc_id_pairs = [(data['doc_id'][i], data['doc_id'][j]) for i, j in zip(*upper_triangle_indices)]\n",
    "similarities = cosine_similarities[upper_triangle_indices]\n",
    "\n",
    "# Create a result data frame in a long format with document ID pairs and similarity as columns\n",
    "result_df = pd.DataFrame({'doc_id_1': [pair[0] for pair in doc_id_pairs],\n",
    "                         'doc_id_2': [pair[1] for pair in doc_id_pairs],\n",
    "                         'similarity': similarities})\n",
    "\n",
    "# Print the result data frame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime object\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Get the unique dates in the dataframe\n",
    "unique_dates = data['date'].dt.date.unique()\n",
    "\n",
    "# Create a list to store the result data frames for each 7-day interval\n",
    "result_dfs = []\n",
    "\n",
    "# Iterate over the unique dates in the dataframe\n",
    "for i in range(len(unique_dates) - 7):\n",
    "    start_date = unique_dates[i]\n",
    "    end_date = unique_dates[i + 7]\n",
    "    \n",
    "    # Get the rows from the dataframe for the current 7-day interval\n",
    "    interval_data = data[(data['date'].dt.date >= start_date) & (data['date'].dt.date < end_date)]\n",
    "    \n",
    "    # Create the TF-IDF matrix for the current 7-day interval\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(\" \"))\n",
    "    tfidf_matrix = vectorizer.fit_transform(interval_data['text'])\n",
    "    \n",
    "    # Calculate the cosine similarity between the documents in the current 7-day interval\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Get the indices of the upper triangle of the cosine similarity matrix\n",
    "    upper_triangle_indices = np.triu_indices_from(cosine_similarities, k=1)\n",
    "    \n",
    "    # Get the document ID pairs and similarity values from the cosine similarity matrix\n",
    "    doc_id_pairs = [(interval_data['doc_id'][i], interval_data['doc_id'][j]) for i, j in zip(*upper_triangle_indices)]\n",
    "    similarities = cosine_similarities[upper_triangle_indices]\n",
    "    \n",
    "    # Create a result data frame in a long format with document ID pairs, similarity, and start/end dates\n",
    "    result_df = pd.DataFrame({'doc_id_1': [pair[0] for pair in doc_id_pairs],\n",
    "                             'doc_id_2': [pair[1] for pair in doc_id_pairs],\n",
    "                             'similarity': similarities,\n",
    "                             'start_date': start_date,\n",
    "                             'end_date': end_date})\n",
    "    result_dfs.append(result_df)\n",
    "\n",
    "# Concatenate the result data frames for each 7-day interval into a single data frame\n",
    "result_df = pd.concat(result_dfs)\n",
    "\n",
    "# Print the result data frame\n",
    "print(result_df)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "include_colab_link": true,
   "name": "preprocessor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
