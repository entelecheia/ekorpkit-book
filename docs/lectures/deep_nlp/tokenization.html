
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Tokenization &#8212; eKorpkit Tutorials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/ekorpkit.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="SentencePiece Tokenizer" href="sentencepiece.html" />
    <link rel="prev" title="T5: Text-To-Text Transfer Transformer" href="t5.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FN4GFT8HP8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FN4GFT8HP8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/ekorpkit.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">eKorpkit Tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    eKonomic Research Python Toolkit
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/index.html">
   Getting started
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/install.html">
     Installation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/usage.html">
     Usage
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/features/index.html">
   Key features
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/easy_config.html">
     Easy Configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/no_boiler.html">
     No Boilerplate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/workflows.html">
     Workflows
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/share.html">
     Sharable and Reproducible
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/plug.html">
     Pluggable Architece
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/config/index.html">
   Configuring ekorpkit
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/config/eKonf.html">
     Using eKonf class
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/datasets/index.html">
   Datasets
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/corpus.html">
     Build and Load Corpora
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/datasets/dataset.html">
     Build and Load Datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/pipeline.html">
     Corpus task pipelines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/models/index.html">
   Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/automl/index.html">
     Auto ML
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/embeddings/index.html">
     Word Embeddings
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/word2vec_basics.html">
       Word2Vec Basics
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/eval_vectors.html">
       Evaluate pretrained embeddings
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/ngram/index.html">
     N-Grams
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram.html">
       N-Grams
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_lexicons.html">
       N-Gram model for ngram lexicon features
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_words.html">
       N-Gram model for unigram lexicon features
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/sentiment/index.html">
     Sentiment Analyers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/lbsa_en.html">
       Lexicon-based Sentiment Analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/financial_phrasebank_lm.html">
       Evaluate LM with financial phrasebank
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/sentiment.html">
       LM Dictionary vs. finbert vs. T5
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/transformers/index.html">
     Transformers
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/art/index.html">
     AI Art (Text-to-Image)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/disco.html">
       Disco Diffusion
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/dalle-mini.html">
       DALL·E Mini
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/pipelines/index.html">
   Pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/pipelines/pipeline.html">
     Instantiating pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/preprocessors/index.html">
   Preprocessors
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/normalizer/index.html">
     Normalizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/normalizer/normalizer.html">
       Normalizers
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/segmenter/index.html">
     Segmenters
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/segmenter/segmenter.html">
       Segmenters
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/index.html">
     Tokenizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
    <label for="toctree-checkbox-14">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/mecab.html">
       Mecab Tokenizer
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/visualizers/index.html">
   Visualizers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/visualizers/plot.html">
     Plots
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/workflow/index.html">
   Workflows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../corpus/ekorpkot_corpus.html">
   The eKorpkit Corpus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Use Cases
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/fomc/index.html">
   FOMC
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/02_FOMC_numerical_data.html">
     Preparing Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/03_FOMC_corpus.html">
     Preparing Textual Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/04_FOMC_EDA_numericals.html">
     EDA on Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/05_FOMC_features.html">
     Visualizing Features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/06_FOMC_AutoML.html">
     Checking Baseline with AutoML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/07_FOMC_sentiments.html">
     Predicting Sentiments of FOMC Corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/08_FOMC_EDA_sentiments.html">
     EDA on Sentiment Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/09_FOMC_AutoML_with_tones.html">
     Predicting the next decisions with tones
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../usecases/bok/index.html">
   Bank of Korea
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/edgar/index.html">
   EDGAR
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/edgar/predict_edgar.html">
     Prediciting Sentiments
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg/index.html">
   ESG
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/snorkel-polarity.html">
     Preparing polarity classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix.html">
     Improving classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/classifiers.html">
     Training Classifiers for ESG Ratings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/econ_news_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       econ_news_kr
      </span>
     </code>
     corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_news.html">
     Predicting ESG Categories and Polarities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix_classifiers.html">
     Preparing classifiers for active learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_for_learning.html">
     Preparing active learning data
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg-en/index.html">
   ESG (English)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg-en/esg_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       JOCo
      </span>
     </code>
     corpus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/cointax/index.html">
   Taxation on Cryptocurrency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/cointax/coin_dataset.html">
     Improving classification datasets
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/analyst.html">
   ESG Topic Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/rethink_esg.html">
   Rethinking ESG
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../intro_nlp/index.html">
   Introduction to NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/research.html">
     Research Applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/topic.html">
     Topic Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/topic_models.html">
     Topic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/topic_coherence.html">
     Topic Coherence Measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/sentiments.html">
     Sentiment Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/tokenization.html">
     Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_nlp/vectorization.html">
     Vector Semantics and Representation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Deep Learning for NLP
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="zeroshot.html">
     Zero Shot, Prompt, and Search Strategies
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloom.html">
     What is BLOOM?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloom_apps.html">
     Bloom Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="transformers.html">
     Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert.html">
     BERT: Bidirectional Encoder Representations from Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="t5.html">
     T5: Text-To-Text Transfer Transformer
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sentencepiece.html">
     SentencePiece Tokenizer
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../aiart/index.html">
   AI Art (Text-to-Image)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
  <label for="toctree-checkbox-23">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/project-themes.html">
     Project Themes - A Brave New World
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle1.html">
     DALL·E 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle2.html">
     DALL·E 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/imagen.html">
     Imagen
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mlops/index.html">
   Machine Learning Systems Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ds/index.html">
   Data Science for Economics and Finance
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/index.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/cite.html">
   Citation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/entelecheia/ekorpkit-book/main?urlpath=tree/ekorpkit-book/docs/lectures/deep_nlp/tokenization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/entelecheia/ekorpkit-book/blob/main/ekorpkit-book/docs/lectures/deep_nlp/tokenization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book/issues/new?title=Issue%20on%20page%20%2Fdocs/lectures/deep_nlp/tokenization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/docs/lectures/deep_nlp/tokenization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-tokenization">
   What is Tokenization?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-do-we-need-tokenization">
   Why do we need tokenization?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tokenization-methods">
   Tokenization Methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#word-white-space-tokenization">
   Word (White Space) Tokenization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problems-with-word-tokenizer">
     Problems with Word tokenizer
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#character-tokenization">
   Character Tokenization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problems-with-character-tokenizer">
     Problems with Character tokenizer
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subword-tokenization">
   Subword Tokenization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-decide-which-subwords-to-use">
     How to decide which subwords to use?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#normalization-and-pre-tokenization">
   Normalization and pre-tokenization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pre-tokenization">
     Pre-tokenization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#byte-pair-encoding-bpe">
   Byte Pair Encoding (BPE)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-learn-subwords-from-data">
     How to learn subwords from data?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bpe-in-practice">
     BPE in Practice
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bpe-step-by-step-implementation">
       BPE Step-by-Step Implementation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#encoding-and-decoding">
     Encoding and Decoding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decoding">
       Decoding
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#encoding">
       Encoding
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bpe-encoding-and-decoding-in-practice">
     BPE Encoding and Decoding in Practice
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#byte-level-byte-pair-encoding-bbpe">
   Byte-level Byte Pair Encoding (BBPE)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#wordpiece">
   Wordpiece
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization-algorithm">
     Tokenization Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wordpiece-in-practice">
     WordPiece in Practice
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#wordpiece-step-by-step-implementation">
       WordPiece Step-by-Step Implementation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unigram">
   Unigram
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sentencepiece">
   SentencePiece
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparisons-with-other-implementations">
     Comparisons with other implementations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subword-sampling">
   Subword Sampling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bpe-dropout">
   BPE-Dropout
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Tokenization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-tokenization">
   What is Tokenization?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-do-we-need-tokenization">
   Why do we need tokenization?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tokenization-methods">
   Tokenization Methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#word-white-space-tokenization">
   Word (White Space) Tokenization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problems-with-word-tokenizer">
     Problems with Word tokenizer
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#character-tokenization">
   Character Tokenization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problems-with-character-tokenizer">
     Problems with Character tokenizer
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subword-tokenization">
   Subword Tokenization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-decide-which-subwords-to-use">
     How to decide which subwords to use?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#normalization-and-pre-tokenization">
   Normalization and pre-tokenization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pre-tokenization">
     Pre-tokenization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#byte-pair-encoding-bpe">
   Byte Pair Encoding (BPE)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-learn-subwords-from-data">
     How to learn subwords from data?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bpe-in-practice">
     BPE in Practice
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bpe-step-by-step-implementation">
       BPE Step-by-Step Implementation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#encoding-and-decoding">
     Encoding and Decoding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decoding">
       Decoding
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#encoding">
       Encoding
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bpe-encoding-and-decoding-in-practice">
     BPE Encoding and Decoding in Practice
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#byte-level-byte-pair-encoding-bbpe">
   Byte-level Byte Pair Encoding (BBPE)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#wordpiece">
   Wordpiece
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization-algorithm">
     Tokenization Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wordpiece-in-practice">
     WordPiece in Practice
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#wordpiece-step-by-step-implementation">
       WordPiece Step-by-Step Implementation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unigram">
   Unigram
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sentencepiece">
   SentencePiece
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparisons-with-other-implementations">
     Comparisons with other implementations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subword-sampling">
   Subword Sampling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bpe-dropout">
   BPE-Dropout
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="tokenization">
<h1>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">#</a></h1>
<p><img alt="" src="../../../_images/entelecheia_puzzle_pieces1.png" /></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">#</a></h2>
<p>NLP systems have three main components that help machines understand natural language:</p>
<ul class="simple">
<li><p><strong>Tokenization</strong>: Splitting a string into a list of tokens.</p></li>
<li><p><strong>Embedding</strong>: Mapping tokens to vectors.</p></li>
<li><p><strong>Model</strong>: A neural network that takes token vectors as input and outputs predictions.</p></li>
</ul>
<p>Tokenization is the first step in the NLP pipeline.</p>
<ul class="simple">
<li><p>Tokenization is the process of splitting a string into a list of tokens.</p></li>
<li><p>For example, the sentence “I like to eat apples” can be tokenized into the list of tokens <code class="docutils literal notranslate"><span class="pre">[&quot;I&quot;,</span> <span class="pre">&quot;like&quot;,</span> <span class="pre">&quot;to&quot;,</span> <span class="pre">&quot;eat&quot;,</span> <span class="pre">&quot;apples&quot;]</span></code>.</p></li>
<li><p>The tokens can be words, characters, or subwords.</p></li>
</ul>
<blockquote>
<div><p>In deep learning, tokenization is the process of converting a sequence of characters into a sequence of tokens, then converting each token into a numerical vector to be used as input to a neural network.</p>
</div></blockquote>
</section>
<section id="what-is-tokenization">
<h2>What is Tokenization?<a class="headerlink" href="#what-is-tokenization" title="Permalink to this headline">#</a></h2>
<ul>
<li><p>Tokenization is the process of representing a text in smaller units called tokens.</p></li>
<li><p>In a very simple case, we can simply map every word in the text to a numerical index.</p></li>
<li><p>For example, the sentence “I like to eat apples” can be tokenized into the list of tokens:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">[&quot;I&quot;,</span> <span class="pre">&quot;like&quot;,</span> <span class="pre">&quot;to&quot;,</span> <span class="pre">&quot;eat&quot;,</span> <span class="pre">&quot;apples&quot;]</span></code>.</p>
</div></blockquote>
</li>
<li><p>Then, each token can be mapped to a unique index, such as:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">{&quot;I&quot;:</span> <span class="pre">0,</span> <span class="pre">&quot;like&quot;:</span> <span class="pre">1,</span> <span class="pre">&quot;to&quot;:</span> <span class="pre">2,</span> <span class="pre">&quot;eat&quot;:</span> <span class="pre">3,</span> <span class="pre">&quot;apples&quot;:</span> <span class="pre">4}</span></code>.</p>
</div></blockquote>
</li>
<li><p>There are more linguistic features to consider when tokenizing a text, such as punctuation, capitalization, and so on.</p></li>
</ul>
</section>
<section id="why-do-we-need-tokenization">
<h2>Why do we need tokenization?<a class="headerlink" href="#why-do-we-need-tokenization" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>“How can we make a machine read a sentence?”</p></li>
<li><p>Machines don’t know any language, nor do they understand sounds or phonetics.</p></li>
<li><p>They need to be taught from scratch.</p></li>
<li><p>The first step is to break down the sentence into smaller units that the machine can process.</p></li>
<li><p>Tokenization determines how the input is represented to the model.</p></li>
<li><p>This decision has a huge impact on the performance of the model.</p></li>
</ul>
</section>
<section id="tokenization-methods">
<h2>Tokenization Methods<a class="headerlink" href="#tokenization-methods" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Word-level tokenization: Split a sentence into words.</p></li>
<li><p>Character-level tokenization: Split a sentence into characters.</p></li>
<li><p>Subword-level tokenization: Split a sentence into subwords.</p></li>
</ul>
</section>
<section id="word-white-space-tokenization">
<h2>Word (White Space) Tokenization<a class="headerlink" href="#word-white-space-tokenization" title="Permalink to this headline">#</a></h2>
<ul>
<li><p>The simplest tokenization method is to split a sentence into words.</p></li>
<li><p>This is also called white space tokenization.</p></li>
<li><p>The sentence “I like to eat apples” can be tokenized into the list of tokens:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">[&quot;I&quot;,</span> <span class="pre">&quot;like&quot;,</span> <span class="pre">&quot;to&quot;,</span> <span class="pre">&quot;eat&quot;,</span> <span class="pre">&quot;apples&quot;]</span></code>.</p>
</div></blockquote>
</li>
<li><p>This method is very fast and easy to implement.</p></li>
<li><p>However, it has some limitations.</p></li>
</ul>
<section id="problems-with-word-tokenizer">
<h3>Problems with Word tokenizer<a class="headerlink" href="#problems-with-word-tokenizer" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Out-of-vocabulary (OOV) words:</p>
<ul>
<li><p>The risk of missing words that are not in the vocabulary.</p></li>
<li><p>The model will not recognize the variants of words that were not in the training set.</p></li>
<li><p>For example, even though the words <code class="docutils literal notranslate"><span class="pre">pine</span></code> and <code class="docutils literal notranslate"><span class="pre">apple</span></code> exist in the training set, the model will not recognize the word <code class="docutils literal notranslate"><span class="pre">pineapple</span></code>.</p></li>
</ul>
</li>
<li><p>Punctuation and abbreviations:</p>
<ul>
<li><p>The tokenizer will not recognize punctuation and abbreviations.</p></li>
<li><p>For example, the word <code class="docutils literal notranslate"><span class="pre">don't</span></code> will be tokenized as <code class="docutils literal notranslate"><span class="pre">[&quot;do&quot;,</span> <span class="pre">&quot;n't&quot;]</span></code>.</p></li>
</ul>
</li>
<li><p>Slang and informal language:</p>
<ul>
<li><p>The tokenizer will not recognize slang and informal language.</p></li>
<li><p>For example, the word <code class="docutils literal notranslate"><span class="pre">gonna</span></code> will be tokenized as <code class="docutils literal notranslate"><span class="pre">[&quot;gon&quot;,</span> <span class="pre">&quot;na&quot;]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tl;dr</span></code> will be tokenized as <code class="docutils literal notranslate"><span class="pre">[&quot;tl&quot;,</span> <span class="pre">&quot;;&quot;,</span> <span class="pre">&quot;dr&quot;]</span></code>.</p></li>
</ul>
</li>
<li><p>What if language does not use spaces for separating words?</p>
<ul>
<li><p>Chinese, Japanese, and Korean do not use spaces to separate words.</p></li>
<li><p>The tokenizer will not work for these languages.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="character-tokenization">
<h2>Character Tokenization<a class="headerlink" href="#character-tokenization" title="Permalink to this headline">#</a></h2>
<ul>
<li><p>To solve the problems of word tokenization, we can split a sentence into characters.</p></li>
<li><p>The sentence “I like to eat apples” can be tokenized into the list of tokens:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">[&quot;I&quot;,</span> <span class="pre">&quot;</span> <span class="pre">&quot;,</span> <span class="pre">&quot;l&quot;,</span> <span class="pre">&quot;i&quot;,</span> <span class="pre">&quot;k&quot;,</span> <span class="pre">&quot;e&quot;,</span> <span class="pre">&quot;</span> <span class="pre">&quot;,</span> <span class="pre">&quot;t&quot;,</span> <span class="pre">&quot;o&quot;,</span> <span class="pre">&quot;</span> <span class="pre">&quot;,</span> <span class="pre">&quot;e&quot;,</span> <span class="pre">&quot;a&quot;,</span> <span class="pre">&quot;t&quot;,</span> <span class="pre">&quot;</span> <span class="pre">&quot;,</span> <span class="pre">&quot;a&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;l&quot;,</span> <span class="pre">&quot;e&quot;,</span> <span class="pre">&quot;s&quot;]</span></code>.</p>
</div></blockquote>
</li>
<li><p>However, this method has its own problems.</p></li>
</ul>
<section id="problems-with-character-tokenizer">
<h3>Problems with Character tokenizer<a class="headerlink" href="#problems-with-character-tokenizer" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The number of tokens is very large.</p>
<ul>
<li><p>This requires more computation and memory.</p></li>
</ul>
</li>
<li><p>Limit the application of the model.</p>
<ul>
<li><p>Only certain types of models can be used.</p></li>
<li><p>It is inefficient for the certain types of applications, such as NER.</p></li>
</ul>
</li>
<li><p>It would be difficult to understand the relationship between the tokens.</p>
<ul>
<li><p>For example, the tokens <code class="docutils literal notranslate"><span class="pre">[&quot;a&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;l&quot;,</span> <span class="pre">&quot;e&quot;]</span></code> do not represent the word <code class="docutils literal notranslate"><span class="pre">apple</span></code>.</p></li>
<li><p>The tokens <code class="docutils literal notranslate"><span class="pre">[&quot;a&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;l&quot;,</span> <span class="pre">&quot;e&quot;]</span></code> do not have any relationship with the tokens <code class="docutils literal notranslate"><span class="pre">[&quot;a&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;l&quot;,</span> <span class="pre">&quot;e&quot;,</span> <span class="pre">&quot;s&quot;]</span></code>.</p></li>
</ul>
</li>
<li><p>Incorrect spelling could be generated.</p></li>
</ul>
</section>
</section>
<section id="subword-tokenization">
<h2>Subword Tokenization<a class="headerlink" href="#subword-tokenization" title="Permalink to this headline">#</a></h2>
<ul>
<li><p>With character-level tokenization, we risk losing the semantic features of the words.</p></li>
<li><p>With word-level tokenization, we have out-of-vocabulary (OOV) words or very large vocabulary sizes.</p></li>
<li><p>To solve the problems of word tokenization and character tokenization, an algorithm should be able to:</p>
<ul class="simple">
<li><p>Retain the semantic features of the words.</p></li>
<li><p>Tokenize any words without the need for a huge vocabulary.</p></li>
</ul>
</li>
<li><p>Subword tokenization is a method that can solve these problems.</p></li>
<li><p>For example, the sentence “I like to eat pineapples” can be tokenized into the list of tokens:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">[&quot;I&quot;,</span> <span class="pre">&quot;like&quot;,</span> <span class="pre">&quot;to&quot;,</span> <span class="pre">&quot;eat&quot;,</span> <span class="pre">&quot;pine&quot;,</span> <span class="pre">&quot;##app&quot;,</span> <span class="pre">&quot;##les&quot;]</span></code>.</p>
</div></blockquote>
</li>
<li><p>The model only learns a few subwords that can be used to represent any word.</p></li>
<li><p>This solves the problem of OOV words.</p></li>
</ul>
<section id="how-to-decide-which-subwords-to-use">
<h3>How to decide which subwords to use?<a class="headerlink" href="#how-to-decide-which-subwords-to-use" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>There are several algorithms that can be used to decide which subwords to use.</p>
<ul>
<li><p>Byte Pair Encoding (BPE)</p></li>
<li><p>Byte-level BPE</p></li>
<li><p>WordPiece</p></li>
<li><p>Unigram</p></li>
<li><p>SentencePiece</p></li>
<li><p>Subword Sampling</p></li>
<li><p>BPE-dropout</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="normalization-and-pre-tokenization">
<h2>Normalization and pre-tokenization<a class="headerlink" href="#normalization-and-pre-tokenization" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Before tokenization, we need to normalize the text.</p></li>
<li><p>Normalization is the process of converting a text to a standard form.</p></li>
<li><p>For example, we can convert all characters to lowercase.</p></li>
</ul>
<p><img alt="" src="../../../_images/tokenization_pipeline.png" /></p>
<p>The Transformers tokenizer has an attribute called backend_tokenizer that provides access to the underlying tokenizer from the Tokenizers library:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;tokenizers.Tokenizer&#39;&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="s2">&quot;Héllò hôw are ü?&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hello how are u?
</pre></div>
</div>
</div>
</div>
<section id="pre-tokenization">
<h3>Pre-tokenization<a class="headerlink" href="#pre-tokenization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A tokenizer cannot be trained on raw text alone.</p></li>
<li><p>First, we need to split the text into smaller units, like words or characters.</p></li>
<li><p>Pre-tokenization is the process of splitting the text into smaller units before tokenization.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="s2">&quot;Hello, how are  you?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;Hello&#39;, (0, 5)),
 (&#39;,&#39;, (5, 6)),
 (&#39;how&#39;, (7, 10)),
 (&#39;are&#39;, (11, 14)),
 (&#39;you&#39;, (16, 19)),
 (&#39;?&#39;, (19, 20))]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="s2">&quot;Hello, how are  you?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;Hello&#39;, (0, 5)),
 (&#39;,&#39;, (5, 6)),
 (&#39;Ġhow&#39;, (6, 10)),
 (&#39;Ġare&#39;, (10, 14)),
 (&#39;Ġ&#39;, (14, 15)),
 (&#39;Ġyou&#39;, (15, 19)),
 (&#39;?&#39;, (19, 20))]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="s2">&quot;Hello, how are  you?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;▁Hello,&#39;, (0, 6)),
 (&#39;▁how&#39;, (7, 10)),
 (&#39;▁are&#39;, (11, 14)),
 (&#39;▁you?&#39;, (16, 20))]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="byte-pair-encoding-bpe">
<h2>Byte Pair Encoding (BPE)<a class="headerlink" href="#byte-pair-encoding-bpe" title="Permalink to this headline">#</a></h2>
<p>Sennrich et al. (2016) proposed a method called Byte Pair Encoding (BPE) to learn subword units.
<span id="id1">[<a class="reference internal" href="../../about/index.html#id17" title="Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1715–1725. Berlin, Germany, August 2016. Association for Computational Linguistics. URL: https://aclanthology.org/P16-1162, doi:10.18653/v1/P16-1162.">Sennrich <em>et al.</em>, 2016</a>]</span></p>
<ul class="simple">
<li><p>Byte Pair Encoding algorithm is originally used for compressing text.</p></li>
<li><p>It splits words into sequences of characters and iteratively combines the most frequent character pairs.</p></li>
</ul>
<section id="how-to-learn-subwords-from-data">
<h3>How to learn subwords from data?<a class="headerlink" href="#how-to-learn-subwords-from-data" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Count the frequency of each word shown in the corpus.</p></li>
<li><p>For each word, append a special stop token <code class="docutils literal notranslate"><span class="pre">&lt;/w&gt;</span></code> at the end of the word.</p></li>
<li><p>Then, split the word into characters.</p></li>
<li><p>Initially, the tokens of the word are all of its characters plus the additional <code class="docutils literal notranslate"><span class="pre">&lt;/w&gt;</span></code> token.</p></li>
<li><p>For example, the tokens for word <code class="docutils literal notranslate"><span class="pre">low</span></code> are [<code class="docutils literal notranslate"><span class="pre">l</span></code>, <code class="docutils literal notranslate"><span class="pre">o</span></code>, <code class="docutils literal notranslate"><span class="pre">w</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;/w&gt;</span></code>] in order.</p></li>
<li><p>So after counting all the words in the dataset, we will get a vocabulary for the tokenized word with its corresponding counts</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;l o w &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;l o w e r &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;n e w e s t &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;w i d e s t &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>In each iteration, count the frequency of each consecutive byte pair, find out the most frequent one, and merge the two byte pair tokens to one token.</p></li>
<li><p>For the above example, in the first iteration of the merge, because byte pair <code class="docutils literal notranslate"><span class="pre">e</span></code> and <code class="docutils literal notranslate"><span class="pre">s</span></code> occurred 6 + 3 = 9 times which is the most frequent, merge these into a new token <code class="docutils literal notranslate"><span class="pre">es</span></code>.</p></li>
<li><p>Note that token <code class="docutils literal notranslate"><span class="pre">s</span></code> is also gone in this particular example.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;l o w &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;l o w e r &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;n e w es t &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;w i d es t &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>In the second iteration of merge, token <code class="docutils literal notranslate"><span class="pre">es</span></code> and <code class="docutils literal notranslate"><span class="pre">t</span></code> occurred 6 + 3 = 9 times, which is the most frequent.</p></li>
<li><p>Merge these to into a new token <code class="docutils literal notranslate"><span class="pre">est</span></code>.</p></li>
<li><p>Note that token <code class="docutils literal notranslate"><span class="pre">es</span></code> and <code class="docutils literal notranslate"><span class="pre">t</span></code> are also gone.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;l o w &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;l o w e r &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;n e w est &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;w i d est &lt;/w&gt;&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>In the third iteration of the merge, token <code class="docutils literal notranslate"><span class="pre">est</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;/w&gt;</span></code> pair is the most frequent, etc.</p></li>
<li><p>Do this until we have the desired number of tokens or reach the maximum number of iterations.</p></li>
</ul>
<p>Stop token <code class="docutils literal notranslate"><span class="pre">&lt;/w&gt;</span></code> is also important.</p>
<ul class="simple">
<li><p>Without <code class="docutils literal notranslate"><span class="pre">&lt;/w&gt;</span></code>, say if there is a token <code class="docutils literal notranslate"><span class="pre">st</span></code>, this token could be in the word <code class="docutils literal notranslate"><span class="pre">st</span> <span class="pre">ar</span></code>, or the wold <code class="docutils literal notranslate"><span class="pre">wide</span> <span class="pre">st</span></code>.</p></li>
<li><p>Those two words are very different in meaning, but the token <code class="docutils literal notranslate"><span class="pre">st</span></code> is the same.</p></li>
<li><p>With <code class="docutils literal notranslate"><span class="pre">&lt;/w&gt;</span></code>, if there is a token <code class="docutils literal notranslate"><span class="pre">st&lt;/w&gt;</span></code>, the model immediately know that it is the token for the wold <code class="docutils literal notranslate"><span class="pre">wide</span> <span class="pre">st&lt;/w&gt;</span></code> but not <code class="docutils literal notranslate"><span class="pre">st</span> <span class="pre">ar&lt;/w&gt;</span></code>.</p></li>
</ul>
<p>To summarize, the algorithm is as follows:</p>
<ol class="simple">
<li><p>Extract the words from the given dataset along with their count.</p></li>
<li><p>Define the vocabulary size.</p></li>
<li><p>Split the words into a character sequence.</p></li>
<li><p>Add all the unique characters in our character sequence to the vocabulary.</p></li>
<li><p>Select and merge the symbol pair that has a high frequency.</p></li>
<li><p>Repeat step 5 until the vocabulary size is reached.</p></li>
</ol>
</section>
<section id="bpe-in-practice">
<h3>BPE in Practice<a class="headerlink" href="#bpe-in-practice" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ekorpkit</span> <span class="kn">import</span> <span class="n">eKonf</span>

<span class="n">cfg</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="s2">&quot;path&quot;</span><span class="p">)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">uri</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/us_equities_news_sampled.zip&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;us_equities_news_sampled.parquet&quot;</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">cached_path</span><span class="p">)</span>
<span class="n">texts</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">text</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ekorpkit.models.tokenizer.bpe</span> <span class="kn">import</span> <span class="n">BytePairEncoder</span>

<span class="n">bpe</span> <span class="o">=</span> <span class="n">BytePairEncoder</span><span class="p">()</span>

<span class="n">num_merges</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">indices_to_print</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_merges</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">bpe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="p">,</span> <span class="n">indices_to_print</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Merge 0: (&#39;e&#39;, &#39;▁&#39;) with count 10203
All tokens: dict_keys([&#39;i&#39;, &#39;n&#39;, &#39;v&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;, &#39;g&#39;, &#39;▁&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;a&#39;, &#39;k&#39;, &#39;r&#39;, &#39;w&#39;, &#39;e▁&#39;, &#39;b&#39;, &#39;d&#39;, &#39;l&#39;, &#39;y&#39;, &#39;f&#39;, &#39;h&#39;, &#39;u&#39;, &#39;p&#39;, &#39;x&#39;, &#39;1&#39;, &#39;5&#39;, &#39;2&#39;, &#39;0&#39;, &#39;j&#39;, &#39;7&#39;, &#39;9&#39;, &#39;3&#39;, &#39;q&#39;, &#39;6&#39;, &#39;8&#39;, &#39;4&#39;, &#39;z&#39;])
Number of tokens: 38
Merge 1: (&#39;s&#39;, &#39;▁&#39;) with count 9226
All tokens: dict_keys([&#39;i&#39;, &#39;n&#39;, &#39;v&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;, &#39;g&#39;, &#39;▁&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;a&#39;, &#39;k&#39;, &#39;r&#39;, &#39;s▁&#39;, &#39;w&#39;, &#39;e▁&#39;, &#39;b&#39;, &#39;d&#39;, &#39;l&#39;, &#39;y&#39;, &#39;f&#39;, &#39;h&#39;, &#39;u&#39;, &#39;p&#39;, &#39;x&#39;, &#39;1&#39;, &#39;5&#39;, &#39;2&#39;, &#39;0&#39;, &#39;j&#39;, &#39;7&#39;, &#39;9&#39;, &#39;3&#39;, &#39;q&#39;, &#39;6&#39;, &#39;8&#39;, &#39;4&#39;, &#39;z&#39;])
Number of tokens: 39
Merge 2: (&#39;i&#39;, &#39;n&#39;) with count 6437
All tokens: dict_keys([&#39;in&#39;, &#39;v&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;, &#39;g&#39;, &#39;▁&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;a&#39;, &#39;i&#39;, &#39;n&#39;, &#39;k&#39;, &#39;r&#39;, &#39;s▁&#39;, &#39;w&#39;, &#39;e▁&#39;, &#39;b&#39;, &#39;d&#39;, &#39;l&#39;, &#39;y&#39;, &#39;f&#39;, &#39;h&#39;, &#39;u&#39;, &#39;p&#39;, &#39;x&#39;, &#39;1&#39;, &#39;5&#39;, &#39;2&#39;, &#39;0&#39;, &#39;j&#39;, &#39;7&#39;, &#39;9&#39;, &#39;3&#39;, &#39;q&#39;, &#39;6&#39;, &#39;8&#39;, &#39;4&#39;, &#39;z&#39;])
Number of tokens: 40
Merge 999: (&#39;he&#39;, &#39;al&#39;) with count 31
All tokens: dict_keys([&#39;invest&#39;, &#39;ing▁&#39;, &#39;com&#39;, &#39;▁&#39;, &#39;as&#39;, &#39;ian▁&#39;, &#39;stock▁&#39;, &#39;markets▁&#39;, &#39;were▁&#39;, &#39;bro&#39;, &#39;ad&#39;, &#39;ly▁&#39;, &#39;lower▁&#39;, &#39;for▁&#39;, &#39;a▁&#39;, &#39;second▁&#39;, &#39;day▁&#39;, &#39;on▁&#39;, &#39;th&#39;, &#39;ur&#39;, &#39;s&#39;, &#39;as▁&#39;, &#39;we&#39;, &#39;ak▁&#39;, &#39;u▁&#39;, &#39;s▁&#39;, &#39;data▁&#39;, &#39;dur&#39;, &#39;able▁&#39;, &#39;goo&#39;, &#39;ds▁&#39;, &#39;or&#39;, &#39;d&#39;, &#39;ers▁&#39;, &#39;ded▁&#39;, &#39;to▁&#39;, &#39;con&#39;, &#39;cer&#39;, &#39;ns▁&#39;, &#39;over▁&#39;, &#39;the▁&#39;, &#39;global▁&#39;, &#39;growth▁&#39;, &#39;out&#39;, &#39;look▁&#39;, &#39;while▁&#39;, &#39;declin&#39;, &#39;corpor&#39;, &#39;ate▁&#39;, &#39;prof&#39;, &#39;its▁&#39;, &#39;also▁&#39;, &#39;igh&#39;, &#39;ed▁&#39;, &#39;l&#39;, &#39;trade▁&#39;, &#39;h&#39;, &#39;ong▁&#39;, &#39;k&#39;, &#39;an&#39;, &#39;g▁&#39;, &#39;sen&#39;, &#39;index▁&#39;, &#39;t&#39;, &#39;um&#39;, &#39;bl&#39;, &#39;1▁&#39;, &#39;5&#39;, &#39;5▁&#39;, &#39;au&#39;, &#39;str&#39;, &#39;al&#39;, &#39;ia▁&#39;, &#39;x▁&#39;, &#39;20&#39;, &#39;0▁&#39;, &#39;di&#39;, &#39;pped▁&#39;, &#39;jap&#39;, &#39;an▁&#39;, &#39;n&#39;, &#39;i&#39;, &#39;ke&#39;, &#39;i▁&#39;, &#39;2&#39;, &#39;25▁&#39;, &#39;sh&#39;, &#39;7▁&#39;, &#39;c&#39;, &#39;ame▁&#39;, &#39;further▁&#39;, &#39;off▁&#39;, &#39;one▁&#39;, &#39;year▁&#39;, &#39;clo&#39;, &#39;sing▁&#39;, &#39;high▁&#39;, &#39;it▁&#39;, &#39;earlier▁&#39;, &#39;in▁&#39;, &#39;week▁&#39;, &#39;investors▁&#39;, &#39;cas&#39;, &#39;a&#39;, &#39;head▁&#39;, &#39;of▁&#39;, &#39;ese▁&#39;, &#39;fisc&#39;, &#39;al▁&#39;, &#39;end▁&#39;, &#39;mar&#39;, &#39;ch▁&#39;, &#39;is▁&#39;, &#39;fin&#39;, &#39;month▁&#39;, &#39;and▁&#39;, &#39;market▁&#39;, &#39;par&#39;, &#39;ticip&#39;, &#39;ts▁&#39;, &#39;have▁&#39;, &#39;expected▁&#39;, &#39;many▁&#39;, &#39;fun&#39;, &#39;oc&#39;, &#39;k▁&#39;, &#39;from▁&#39;, &#39;me&#39;, &#39;te&#39;, &#39;ic▁&#39;, &#39;1&#39;, &#39;9▁&#39;, &#39;r&#39;, &#39;ally▁&#39;, &#39;j&#39;, &#39;u&#39;, &#39;ary▁&#39;, &#39;peri&#39;, &#39;o&#39;, &#39;d▁&#39;, &#39;after▁&#39;, &#39;e&#39;, &#39;ding▁&#39;, &#39;more▁&#39;, &#39;than▁&#39;, &#39;13▁&#39;, &#39;ap&#39;, &#39;il▁&#39;, &#39;dec&#39;, &#39;ember▁&#39;, &#39;ex&#39;, &#39;port&#39;, &#39;which▁&#39;, &#39;gain&#39;, &#39;shar&#39;, &#39;p&#39;, &#39;first▁&#39;, &#39;quarter▁&#39;, &#39;back▁&#39;, &#39;ak&#39;, &#39;en&#39;, &#39;yen▁&#39;, &#39;om&#39;, &#39;to&#39;, &#39;y&#39;, &#39;is&#39;, &#39;ped▁&#39;, &#39;6&#39;, &#39;8▁&#39;, &#39;res&#39;, &#39;pec&#39;, &#39;ti&#39;, &#39;vely▁&#39;, &#39;consumer▁&#39;, &#39;elec&#39;, &#39;tr&#39;, &#39;on&#39;, &#39;ic&#39;, &#39;gi&#39;, &#39;ant▁&#39;, &#39;y▁&#39;, &#39;re&#39;, &#39;ated▁&#39;, &#39;up&#39;, &#39;side▁&#39;, &#39;p▁&#39;, &#39;sa&#39;, &#39;w▁&#39;, &#39;shares▁&#39;, &#39;ump▁&#39;, &#39;6▁&#39;, &#39;ten&#39;, &#39;pre&#39;, &#39;vi&#39;, &#39;ous▁&#39;, &#39;15▁&#39;, &#39;follow&#39;, &#39;repor&#39;, &#39;that▁&#39;, &#39;ta&#39;, &#39;w&#39;, &#39;ha&#39;, &#39;ci&#39;, &#39;sion▁&#39;, &#39;industry▁&#39;, &#39;bu&#39;, &#39;ying▁&#39;, &#39;10▁&#39;, &#39;manufac&#39;, &#39;tur&#39;, &#39;er▁&#39;, &#39;9&#39;, &#39;billion▁&#39;, &#39;with▁&#39;, &#39;two▁&#39;, &#39;form▁&#39;, &#39;e▁&#39;, &#39;up▁&#39;, &#39;li&#39;, &#39;qu&#39;, &#39;id▁&#39;, &#39;cr&#39;, &#39;st&#39;, &#39;pl&#39;, &#39;ay▁&#39;, &#39;production▁&#39;, &#39;el&#39;, &#39;se&#39;, &#39;where▁&#39;, &#39;under▁&#39;, &#39;pres&#39;, &#39;sure▁&#39;, &#39;am&#39;, &#39;ing&#39;, &#39;er&#39;, &#39;f&#39;, &#39;ear&#39;, &#39;ard▁&#39;, &#39;china▁&#39;, &#39;wor&#39;, &#39;ries▁&#39;, &#39;pic&#39;, &#39;c▁&#39;, &#39;pro&#39;, &#39;per&#39;, &#39;ty▁&#39;, &#39;big&#39;, &#39;gest▁&#39;, &#39;fe▁&#39;, &#39;in&#39;, &#39;sur&#39;, &#39;dro&#39;, &#39;4▁&#39;, &#39;ting▁&#39;, &#39;201&#39;, &#39;net▁&#39;, &#39;come▁&#39;, &#39;rose▁&#39;, &#39;ny&#39;, &#39;0&#39;, &#39;3▁&#39;, &#39;mis&#39;, &#39;expec&#39;, &#39;ations▁&#39;, &#39;ip&#39;, &#39;tain&#39;, &#39;lin&#39;, &#39;es▁&#39;, &#39;fell▁&#39;, &#39;ation▁&#39;, &#39;lar&#39;, &#39;car&#39;, &#39;ri&#39;, &#39;reported▁&#39;, &#39;lo&#39;, &#39;ss▁&#39;, &#39;2▁&#39;, &#39;7&#39;, &#39;last▁&#39;, &#39;wi&#39;, &#39;der▁&#39;, &#39;average▁&#39;, &#39;estimate▁&#39;, &#39;ris&#39;, &#39;fu&#39;, &#39;el▁&#39;, &#39;co&#39;, &#39;sts▁&#39;, &#39;demand▁&#39;, &#39;port▁&#39;, &#39;oper&#39;, &#39;at&#39;, &#39;or▁&#39;, &#39;m&#39;, &#39;chan&#39;, &#39;old&#39;, &#39;ings▁&#39;, &#39;ann&#39;, &#39;due▁&#39;, &#39;ar&#39;, &#39;b&#39;, &#39;de&#39;, &#39;trad&#39;, &#39;chin&#39;, &#39;ban&#39;, &#39;ks▁&#39;, &#39;indu&#39;, &#39;stri&#39;, &#39;comm&#39;, &#39;ial▁&#39;, &#39;bank▁&#39;, &#39;down▁&#39;, &#39;their▁&#39;, &#39;earnings▁&#39;, &#39;lat&#39;, &#39;ra&#39;, &#39;mat&#39;, &#39;produc&#39;, &#39;cont&#39;, &#39;ted▁&#39;, &#39;ses▁&#39;, &#39;op&#39;, &#39;per▁&#39;, &#39;min&#39;, &#39;g&#39;, &#39;x&#39;, &#39;company▁&#39;, &#39;um▁&#39;, &#39;ch&#39;, &#39;co▁&#39;, &#39;oil▁&#39;, &#39;maj&#39;, &#39;ors▁&#39;, &#39;pe&#39;, &#39;ro&#39;, &#39;no&#39;, &#39;but▁&#39;, &#39;perform&#39;, &#39;onal▁&#39;, &#39;equ&#39;, &#39;ities▁&#39;, &#39;remain&#39;, &#39;se▁&#39;, &#39;four▁&#39;, &#39;was▁&#39;, &#39;by▁&#39;, &#39;line▁&#39;, &#39;le&#39;, &#39;coun&#39;, &#39;try▁&#39;, &#39;tion▁&#39;, &#39;un&#39;, &#39;der&#39;, &#39;3&#39;, &#39;will▁&#39;, &#39;be▁&#39;, &#39;between▁&#39;, &#39;4&#39;, &#39;00▁&#39;, &#39;million▁&#39;, &#39;50▁&#39;, &#39;below▁&#39;, &#39;it&#39;, &#39;increased▁&#39;, &#39;t▁&#39;, &#39;ther▁&#39;, &#39;v&#39;, &#39;ity▁&#39;, &#39;euro&#39;, &#39;il&#39;, &#39;en▁&#39;, &#39;sup&#39;, &#39;por&#39;, &#39;o▁&#39;, &#39;z&#39;, &#39;lead&#39;, &#39;increase▁&#39;, &#39;si&#39;, &#39;deb&#39;, &#39;fi&#39;, &#39;all▁&#39;, &#39;at▁&#39;, &#39;fr&#39;, &#39;ance▁&#39;, &#39;ac&#39;, &#39;da&#39;, &#39;ged▁&#39;, &#39;pu&#39;, &#39;sh▁&#39;, &#39;offic&#39;, &#39;employ&#39;, &#39;ment▁&#39;, &#39;change▁&#39;, &#39;rele&#39;, &#39;ase▁&#39;, &#39;iti&#39;, &#39;jo&#39;, &#39;ess▁&#39;, &#39;cl&#39;, &#39;im&#39;, &#39;soli&#39;, &#39;ec&#39;, &#39;product▁&#39;, &#39;ver&#39;, &#39;strong▁&#39;, &#39;br&#39;, &#39;rec&#39;, &#39;og&#39;, &#39;hel&#39;, &#39;pepsico▁&#39;, &#39;inc▁&#39;, &#39;nyse▁&#39;, &#39;n▁&#39;, &#39;ir&#39;, &#39;performance▁&#39;, &#39;tt&#39;, &#39;le▁&#39;, &#39;mi&#39;, &#39;revenue▁&#39;, &#39;eps▁&#39;, &#39;bar&#39;, &#39;ely▁&#39;, &#39;consensus▁&#39;, &#39;targ&#39;, &#39;et▁&#39;, &#39;for&#39;, &#39;ex▁&#39;, &#39;fl&#39;, &#39;tu&#39;, &#39;of&#39;, &#39;ter▁&#39;, &#39;so&#39;, &#39;sales▁&#39;, &#39;some▁&#39;, &#39;po&#39;, &#39;ght▁&#39;, &#39;results▁&#39;, &#39;us▁&#39;, &#39;low▁&#39;, &#39;do&#39;, &#39;ad▁&#39;, &#39;ture▁&#39;, &#39;this▁&#39;, &#39;analy&#39;, &#39;tic&#39;, &#39;what▁&#39;, &#39;you▁&#39;, &#39;can▁&#39;, &#39;be&#39;, &#39;ond▁&#39;, &#39;ap▁&#39;, &#39;est▁&#39;, &#39;q&#39;, &#39;high&#39;, &#39;gener&#39;, &#39;18▁&#39;, &#39;shor&#39;, &#39;ago▁&#39;, &#39;ful&#39;, &#39;l▁&#39;, &#39;over&#39;, &#39;view▁&#39;, &#39;sted▁&#39;, &#39;11▁&#39;, &#39;show&#39;, &#39;st▁&#39;, &#39;years▁&#39;, &#39;gu&#39;, &#39;id&#39;, &#39;loo&#39;, &#39;king▁&#39;, &#39;however▁&#39;, &#39;call&#39;, &#39;ill&#39;, &#39;about▁&#39;, &#39;es&#39;, &#39;boo&#39;, &#39;get▁&#39;, &#39;ning▁&#39;, &#39;mon&#39;, &#39;ey▁&#39;, &#39;r▁&#39;, &#39;see&#39;, &#39;recent▁&#39;, &#39;he&#39;, &#39;av&#39;, &#39;ily▁&#39;, &#39;dri&#39;, &#39;ve▁&#39;, &#39;through▁&#39;, &#39;ff&#39;, &#39;enti&#39;, &#39;products▁&#39;, &#39;estim&#39;, &#39;early▁&#39;, &#39;has▁&#39;, &#39;la&#39;, &#39;num&#39;, &#39;ber▁&#39;, &#39;new▁&#39;, &#39;ach▁&#39;, &#39;ating▁&#39;, &#39;retail▁&#39;, &#39;addi&#39;, &#39;int&#39;, &#39;duc&#39;, &#39;ef&#39;, &#39;are▁&#39;, &#39;ow&#39;, &#39;expan&#39;, &#39;stre&#39;, &#39;am▁&#39;, &#39;su&#39;, &#39;verage▁&#39;, &#39;business▁&#39;, &#39;inclu&#39;, &#39;ative▁&#39;, &#39;pac&#39;, &#39;ag&#39;, &#39;reas&#39;, &#39;ons▁&#39;, &#39;all&#39;, &#39;people▁&#39;, &#39;increas&#39;, &#39;bec&#39;, &#39;wee&#39;, &#39;us&#39;, &#39;heal&#39;, &#39;th▁&#39;, &#39;they▁&#39;, &#39;such▁&#39;, &#39;if&#39;, &#39;enc&#39;, &#39;our&#39;, &#39;age▁&#39;, &#39;ke▁&#39;, &#39;sc&#39;, &#39;consum&#39;, &#39;pur&#39;, &#39;comp&#39;, &#39;em&#39;, &#39;ent&#39;, &#39;mo&#39;, &#39;sal&#39;, &#39;acks▁&#39;, &#39;world▁&#39;, &#39;ence▁&#39;, &#39;develop&#39;, &#39;econom&#39;, &#39;ies▁&#39;, &#39;ge▁&#39;, &#39;opportun&#39;, &#39;go&#39;, &#39;gre&#39;, &#39;adv&#39;, &#39;ys▁&#39;, &#39;ple▁&#39;, &#39;de▁&#39;, &#39;ju&#39;, &#39;because▁&#39;, &#39;when▁&#39;, &#39;cu&#39;, &#39;als▁&#39;, &#39;other▁&#39;, &#39;ving▁&#39;, &#39;benef&#39;, &#39;om▁&#39;, &#39;today▁&#39;, &#39;eg&#39;, &#39;ory▁&#39;, &#39;ack▁&#39;, &#39;off&#39;, &#39;set▁&#39;, &#39;tn&#39;, &#39;grow&#39;, &#39;manag&#39;, &#39;ement▁&#39;, &#39;gro&#39;, &#39;point▁&#39;, &#39;third▁&#39;, &#39;coming▁&#39;, &#39;gres&#39;, &#39;sive▁&#39;, &#39;marke&#39;, &#39;ft▁&#39;, &#39;ile▁&#39;, &#39;ran&#39;, &#39;past▁&#39;, &#39;sp&#39;, &#39;ast▁&#39;, &#39;ul&#39;, &#39;pr&#39;, &#39;techn&#39;, &#39;most▁&#39;, &#39;fac&#39;, &#39;pa&#39;, &#39;ig&#39;, &#39;into▁&#39;, &#39;ou&#39;, &#39;bre&#39;, &#39;chi&#39;, &#39;ps▁&#39;, &#39;these▁&#39;, &#39;margin▁&#39;, &#39;provi&#39;, &#39;bo&#39;, &#39;support▁&#39;, &#39;been▁&#39;, &#39;out▁&#39;, &#39;gr&#39;, &#39;ph&#39;, &#39;wh&#39;, &#39;ves▁&#39;, &#39;way▁&#39;, &#39;another▁&#39;, &#39;ati&#39;, &#39;oun&#39;, &#39;current▁&#39;, &#39;m▁&#39;, &#39;2019▁&#39;, &#39;pri&#39;, &#39;bet&#39;, &#39;cap&#39;, &#39;sim&#39;, &#39;iz&#39;, &#39;intern&#39;, &#39;ational▁&#39;, &#39;ear▁&#39;, &#39;capit&#39;, &#39;still▁&#39;, &#39;ven▁&#39;, &#39;there▁&#39;, &#39;so▁&#39;, &#39;much▁&#39;, &#39;make▁&#39;, &#39;mor&#39;, &#39;ce▁&#39;, &#39;ans▁&#39;, &#39;no▁&#39;, &#39;may▁&#39;, &#39;exp&#39;, &#39;share▁&#39;, &#39;marg&#39;, &#39;ne&#39;, &#39;impac&#39;, &#39;ents▁&#39;, &#39;busin&#39;, &#39;read&#39;, &#39;ates▁&#39;, &#39;revenues▁&#39;, &#39;do▁&#39;, &#39;those▁&#39;, &#39;contin&#39;, &#39;ues▁&#39;, &#39;inv&#39;, &#39;20▁&#39;, &#39;ked▁&#39;, &#39;investment▁&#39;, &#39;mex&#39;, &#39;ico▁&#39;, &#39;next▁&#39;, &#39;thir&#39;, &#39;seen▁&#39;, &#39;eng&#39;, &#39;term▁&#39;, &#39;currency▁&#39;, &#39;low&#39;, &#39;lim&#39;, &#39;vol&#39;, &#39;ar▁&#39;, &#39;ough▁&#39;, &#39;well▁&#39;, &#39;not▁&#39;, &#39;ol&#39;, &#39;group▁&#39;, &#39;att&#39;, &#39;ind&#39;, &#39;eh&#39;, &#39;retur&#39;, &#39;value▁&#39;, &#39;divid&#39;, &#39;ently▁&#39;, &#39;ned▁&#39;, &#39;aliz&#39;, &#39;before▁&#39;, &#39;spec&#39;, &#39;same▁&#39;, &#39;disc&#39;, &#39;sed▁&#39;, &#39;end&#39;, &#39;earch▁&#39;, &#39;report▁&#39;, &#39;only▁&#39;, &#39;sel&#39;, &#39;ect▁&#39;, &#39;secur&#39;, &#39;companies▁&#39;, &#39;can&#39;, &#39;sub&#39;, &#39;sti&#39;, &#39;te▁&#39;, &#39;any▁&#39;, &#39;lu&#39;, &#39;ure▁&#39;, &#39;ited▁&#39;, &#39;based▁&#39;, &#39;form&#39;, &#39;cur&#39;, &#39;sec&#39;, &#39;sul&#39;, &#39;ob&#39;, &#39;belie&#39;, &#39;ved▁&#39;, &#39;ther&#39;, &#39;fore▁&#39;, &#39;estimates▁&#39;, &#39;best▁&#39;, &#39;with&#39;, &#39;mic&#39;, &#39;technology▁&#39;, &#39;nasdaq▁&#39;, &#39;higher▁&#39;, &#39;price▁&#39;, &#39;hi&#39;, &#39;analysts▁&#39;, &#39;ific&#39;, &#39;60▁&#39;, &#39;ll▁&#39;, &#39;just▁&#39;, &#39;12▁&#39;, &#39;14▁&#39;, &#39;equity▁&#39;, &#39;buy▁&#39;, &#39;better▁&#39;, &#39;hold▁&#39;, &#39;surprise▁&#39;, &#39;tim&#39;, &#39;quar&#39;, &#39;ter&#39;, &#39;tor▁&#39;, &#39;ite▁&#39;, &#39;three▁&#39;, &#39;ight▁&#39;, &#39;time▁&#39;, &#39;around▁&#39;, &#39;tions▁&#39;, &#39;pric&#39;, &#39;fri&#39;, &#39;trading▁&#39;, &#39;move▁&#39;, &#39;fed▁&#39;, &#39;rates▁&#39;, &#39;tw&#39;, &#39;days▁&#39;, &#39;red▁&#39;, &#39;8&#39;, &#39;sig&#39;, &#39;ain▁&#39;, &#39;h▁&#39;, &#39;acti&#39;, &#39;000▁&#39;, &#39;compar&#39;, &#39;trac&#39;, &#39;posi&#39;, &#39;gold▁&#39;, &#39;silver▁&#39;, &#39;f▁&#39;, &#39;seas&#39;, &#39;ma&#39;, &#39;vel&#39;, &#39;resul&#39;, &#39;ru&#39;, &#39;rati&#39;, &#39;tal▁&#39;, &#39;like▁&#39;, &#39;ger▁&#39;, &#39;ever▁&#39;, &#39;cy&#39;, &#39;ical▁&#39;, &#39;now▁&#39;, &#39;major▁&#39;, &#39;economy▁&#39;, &#39;rate▁&#39;, &#39;key▁&#39;, &#39;val&#39;, &#39;gh&#39;, &#39;est&#39;, &#39;news▁&#39;, &#39;cre&#39;, &#39;som&#39;, &#39;think▁&#39;, &#39;rel&#39;, &#39;prices▁&#39;, &#39;since▁&#39;, &#39;should▁&#39;, &#39;ue▁&#39;, &#39;months▁&#39;, &#39;du&#39;, &#39;pp&#39;, &#39;see▁&#39;, &#39;very▁&#39;, &#39;ver▁&#39;, &#39;govern&#39;, &#39;valu&#39;, &#39;stor&#39;, &#39;leg&#39;, &#39;ste&#39;, &#39;old▁&#39;, &#39;man&#39;, &#39;infl&#39;, &#39;ent▁&#39;, &#39;kers▁&#39;, &#39;them▁&#39;, &#39;main&#39;, &#39;medi&#39;, &#39;goldman▁&#39;, &#39;ach&#39;, &#39;long▁&#39;, &#39;forec&#39;, &#39;who▁&#39;, &#39;tal&#39;, &#39;ain&#39;, &#39;consid&#39;, &#39;against▁&#39;, &#39;re▁&#39;, &#39;americ&#39;, &#39;oo&#39;, &#39;me▁&#39;, &#39;ab&#39;, &#39;inter&#39;, &#39;trend▁&#39;, &#39;sy&#39;, &#39;stoc&#39;, &#39;new&#39;, &#39;indic&#39;, &#39;sector▁&#39;, &#39;fir&#39;, &#39;fe&#39;, &#39;likely▁&#39;, &#39;ail&#39;, &#39;war&#39;, &#39;son▁&#39;, &#39;ill▁&#39;, &#39;yn&#39;, &#39;ber&#39;, &#39;our▁&#39;, &#39;ments▁&#39;, &#39;if▁&#39;, &#39;illion▁&#39;, &#39;stocks▁&#39;, &#39;we▁&#39;, &#39;ms▁&#39;, &#39;reta&#39;, &#39;portfoli&#39;, &#39;tors▁&#39;, &#39;deli&#39;, &#39;tive▁&#39;, &#39;curr&#39;, &#39;ans&#39;, &#39;servic&#39;, &#39;qui&#39;, &#39;rou&#39;, &#39;anc&#39;, &#39;wa&#39;, &#39;operating▁&#39;, &#39;30▁&#39;, &#39;bas&#39;, &#39;poin&#39;, &#39;zacks▁&#39;, &#39;rank▁&#39;, &#39;currently▁&#39;, &#39;would▁&#39;, &#39;strateg&#39;, &#39;2017▁&#39;, &#39;500▁&#39;, &#39;even▁&#39;, &#39;top▁&#39;, &#39;reuters▁&#39;, &#39;ward▁&#39;, &#39;recor&#39;, &#39;10&#39;, &#39;level▁&#39;, &#39;level&#39;, &#39;above▁&#39;, &#39;ree▁&#39;, &#39;star&#39;, &#39;how▁&#39;, &#39;ow▁&#39;, &#39;here▁&#39;, &#39;dollar▁&#39;, &#39;sum&#39;, &#39;cor&#39;, &#39;had▁&#39;, &#39;could▁&#39;, &#39;fol&#39;, &#39;his▁&#39;, &#39;dow&#39;, &#39;ade▁&#39;, &#39;inc&#39;, &#39;dem&#39;, &#39;presid&#39;, &#39;divi&#39;, &#39;tly▁&#39;, &#39;esday▁&#39;, &#39;ir▁&#39;, &#39;ser&#39;, &#39;ble▁&#39;, &#39;ener&#39;, &#39;financ&#39;, &#39;technolog&#39;, &#39;ere▁&#39;, &#39;hn&#39;, &#39;del&#39;, &#39;thin&#39;, &#39;led▁&#39;, &#39;un▁&#39;, &#39;bri&#39;, &#39;according▁&#39;, &#39;and&#39;, &#39;et&#39;, &#39;ratio▁&#39;, &#39;cash▁&#39;, &#39;yiel&#39;, &#39;b▁&#39;, &#39;dge▁&#39;, &#39;he▁&#39;, &#39;compan&#39;, &#39;ven&#39;, &#39;ore▁&#39;, &#39;21▁&#39;, &#39;acc&#39;, &#39;mu&#39;, &#39;fund▁&#39;, &#39;tec&#39;, &#39;vic&#39;, &#39;ures▁&#39;, &#39;sil&#39;, &#39;custom&#39;, &#39;lik&#39;, &#39;az&#39;, &#39;currenc&#39;, &#39;sid&#39;, &#39;said▁&#39;, &#39;percent▁&#39;, &#39;sell▁&#39;, &#39;200&#39;, &#39;cru&#39;, &#39;res▁&#39;, &#39;dn&#39;, &#39;president▁&#39;, &#39;illi&#39;, &#39;oli&#39;, &#39;sin&#39;, &#39;ose▁&#39;, &#39;olog&#39;, &#39;eu&#39;, &#39;ce&#39;, &#39;cents▁&#39;, &#39;positive▁&#39;, &#39;ters▁&#39;, &#39;man▁&#39;, &#39;ie&#39;, &#39;ben&#39;, &#39;tg&#39;, &#39;ell▁&#39;, &#39;rise▁&#39;, &#39;fer&#39;, &#39;emplo&#39;, &#39;polic&#39;, &#39;ound▁&#39;, &#39;glob&#39;, &#39;plo&#39;, &#39;ich▁&#39;, &#39;amer&#39;, &#39;strat&#39;, &#39;year&#39;, &#39;trump▁&#39;, &#39;security▁&#39;, &#39;dollar&#39;, &#39;vie&#39;, &#39;total▁&#39;, &#39;cloud▁&#39;, &#39;oud▁&#39;, &#39;aly&#39;, &#39;you&#39;, &#39;stry▁&#39;, &#39;accor&#39;, &#39;activi&#39;, &#39;ace▁&#39;, &#39;pol&#39;, &#39;bel&#39;, &#39;gh▁&#39;, &#39;overn&#39;, &#39;uters▁&#39;, &#39;glo&#39;, &#39;month&#39;, &#39;arg&#39;, &#39;surpri&#39;, &#39;financial▁&#39;, &#39;gold&#39;, &#39;af&#39;, &#39;ld▁&#39;, &#39;earn&#39;, &#39;the&#39;, &#39;q▁&#39;, &#39;throu&#39;, &#39;ps&#39;, &#39;sus▁&#39;, &#39;isc&#39;, &#39;ata▁&#39;, &#39;dol&#39;, &#39;ext▁&#39;, &#39;ause▁&#39;, &#39;fur&#39;, &#39;econ&#39;, &#39;cent▁&#39;, &#39;lier▁&#39;, &#39;reven&#39;, &#39;ince▁&#39;, &#39;nas&#39;, &#39;how&#39;, &#39;iel&#39;, &#39;devel&#39;, &#39;fro&#39;, &#39;manu&#39;])
Number of tokens: 1025
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bpe</span><span class="o">.</span><span class="n">print_enocoded_word</span><span class="p">(</span><span class="s1">&#39;investors▁&#39;</span><span class="p">)</span>
<span class="n">bpe</span><span class="o">.</span><span class="n">print_enocoded_word</span><span class="p">(</span><span class="s1">&#39;dogecoin▁&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoding word: investors▁...
Encoding of the known word:
[&#39;investors▁&#39;]
Encoding treating the known word as unknown:
[&#39;investors▁&#39;]
Encoding word: dogecoin▁...
Encoding of the unknown word:
[&#39;d&#39;, &#39;og&#39;, &#39;e&#39;, &#39;co&#39;, &#39;in▁&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">bpe</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;Investment opportunities in the company&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;investment▁&#39;, &#39;opportun&#39;, &#39;ities▁&#39;, &#39;in▁&#39;, &#39;the▁&#39;, &#39;company▁&#39;]
</pre></div>
</div>
</div>
</div>
<section id="bpe-step-by-step-implementation">
<h4>BPE Step-by-Step Implementation<a class="headerlink" href="#bpe-step-by-step-implementation" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span><span class="o">,</span> <span class="nn">collections</span>

<span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">{}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">format_word</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">space_token</span><span class="o">=</span><span class="s1">&#39;▁&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">text</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">space_token</span>

<span class="k">def</span> <span class="nf">initialize_vocab</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="p">{}</span>
        
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">lowercase</span><span class="p">:</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\s+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="n">all_words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">all_words</span><span class="p">:</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">format_word</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">vocab</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span>  <span class="o">=</span> <span class="n">initialize_vocab</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of words: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of words: 7847
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_tokens_from_vocab</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">vocab_tokenization</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">word_tokens</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">word_tokens</span><span class="p">:</span>
            <span class="n">tokens</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>
        <span class="n">vocab_tokenization</span><span class="p">[</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">word_tokens</span>
    <span class="k">return</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">vocab_tokenization</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span><span class="p">,</span> <span class="n">vocab_tokenization</span> <span class="o">=</span> <span class="n">get_tokens_from_vocab</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of tokens: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of tokens: 37
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_bigram_counts</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">pairs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">count</span>
    <span class="k">return</span> <span class="n">pairs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">merge_vocab</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">vocab_in</span><span class="p">):</span>
    <span class="n">vocab_out</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">bigram</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">))</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;(?&lt;!\S)&#39;</span> <span class="o">+</span> <span class="n">bigram</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;(?!\S)&#39;</span><span class="p">)</span>
    <span class="n">bytepair</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab_in</span><span class="p">:</span>
        <span class="n">w_out</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">bytepair</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span>
        <span class="n">vocab_out</span><span class="p">[</span><span class="n">w_out</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_in</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">vocab_out</span><span class="p">,</span> <span class="p">(</span><span class="n">bigram</span><span class="p">,</span> <span class="n">bytepair</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">find_merges</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">num_merges</span><span class="p">,</span> <span class="n">indices_to_print</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]):</span>
    <span class="n">merges</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="n">get_bigram_counts</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
        <span class="n">best_pair</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pairs</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
        <span class="n">best_count</span> <span class="o">=</span> <span class="n">pairs</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span>
        <span class="n">vocab</span><span class="p">,</span> <span class="p">(</span><span class="n">bigram</span><span class="p">,</span> <span class="n">bytepair</span><span class="p">)</span> <span class="o">=</span> <span class="n">merge_vocab</span><span class="p">(</span><span class="n">best_pair</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
        <span class="n">merges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="sa">r</span><span class="s2">&quot;(?&lt;!\S)&quot;</span> <span class="o">+</span> <span class="n">bigram</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;(?!\S)&quot;</span><span class="p">,</span> <span class="n">bytepair</span><span class="p">))</span>
        <span class="n">tokens</span><span class="p">,</span> <span class="n">vocab_tokenization</span> <span class="o">=</span> <span class="n">get_tokens_from_vocab</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices_to_print</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Merge </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">best_pair</span><span class="si">}</span><span class="s2"> with count </span><span class="si">{</span><span class="n">best_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All tokens: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of tokens: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">keys</span><span class="p">())))</span>

    <span class="k">return</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">merges</span><span class="p">,</span> <span class="n">vocab_tokenization</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_merges</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">indices_to_print</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_merges</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">vocab</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">merges</span><span class="p">,</span> <span class="n">vocab_tokenization</span> <span class="o">=</span> <span class="n">find_merges</span><span class="p">(</span>
    <span class="n">vocab</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">num_merges</span><span class="p">,</span> <span class="n">indices_to_print</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Merge 0: (&#39;e&#39;, &#39;▁&#39;) with count 10203
All tokens: dict_keys([&#39;i&#39;, &#39;n&#39;, &#39;v&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;, &#39;g&#39;, &#39;▁&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;a&#39;, &#39;k&#39;, &#39;r&#39;, &#39;w&#39;, &#39;e▁&#39;, &#39;b&#39;, &#39;d&#39;, &#39;l&#39;, &#39;y&#39;, &#39;f&#39;, &#39;h&#39;, &#39;u&#39;, &#39;p&#39;, &#39;x&#39;, &#39;1&#39;, &#39;5&#39;, &#39;2&#39;, &#39;0&#39;, &#39;j&#39;, &#39;7&#39;, &#39;9&#39;, &#39;3&#39;, &#39;q&#39;, &#39;6&#39;, &#39;8&#39;, &#39;4&#39;, &#39;z&#39;])
Number of tokens: 38
Merge 1: (&#39;s&#39;, &#39;▁&#39;) with count 9226
All tokens: dict_keys([&#39;i&#39;, &#39;n&#39;, &#39;v&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;, &#39;g&#39;, &#39;▁&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;a&#39;, &#39;k&#39;, &#39;r&#39;, &#39;s▁&#39;, &#39;w&#39;, &#39;e▁&#39;, &#39;b&#39;, &#39;d&#39;, &#39;l&#39;, &#39;y&#39;, &#39;f&#39;, &#39;h&#39;, &#39;u&#39;, &#39;p&#39;, &#39;x&#39;, &#39;1&#39;, &#39;5&#39;, &#39;2&#39;, &#39;0&#39;, &#39;j&#39;, &#39;7&#39;, &#39;9&#39;, &#39;3&#39;, &#39;q&#39;, &#39;6&#39;, &#39;8&#39;, &#39;4&#39;, &#39;z&#39;])
Number of tokens: 39
Merge 2: (&#39;i&#39;, &#39;n&#39;) with count 6437
All tokens: dict_keys([&#39;in&#39;, &#39;v&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;, &#39;g&#39;, &#39;▁&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;a&#39;, &#39;i&#39;, &#39;n&#39;, &#39;k&#39;, &#39;r&#39;, &#39;s▁&#39;, &#39;w&#39;, &#39;e▁&#39;, &#39;b&#39;, &#39;d&#39;, &#39;l&#39;, &#39;y&#39;, &#39;f&#39;, &#39;h&#39;, &#39;u&#39;, &#39;p&#39;, &#39;x&#39;, &#39;1&#39;, &#39;5&#39;, &#39;2&#39;, &#39;0&#39;, &#39;j&#39;, &#39;7&#39;, &#39;9&#39;, &#39;3&#39;, &#39;q&#39;, &#39;6&#39;, &#39;8&#39;, &#39;4&#39;, &#39;z&#39;])
Number of tokens: 40
Merge 999: (&#39;he&#39;, &#39;al&#39;) with count 31
All tokens: dict_keys([&#39;invest&#39;, &#39;ing▁&#39;, &#39;com&#39;, &#39;▁&#39;, &#39;as&#39;, &#39;ian▁&#39;, &#39;stock▁&#39;, &#39;markets▁&#39;, &#39;were▁&#39;, &#39;bro&#39;, &#39;ad&#39;, &#39;ly▁&#39;, &#39;lower▁&#39;, &#39;for▁&#39;, &#39;a▁&#39;, &#39;second▁&#39;, &#39;day▁&#39;, &#39;on▁&#39;, &#39;th&#39;, &#39;ur&#39;, &#39;s&#39;, &#39;as▁&#39;, &#39;we&#39;, &#39;ak▁&#39;, &#39;u▁&#39;, &#39;s▁&#39;, &#39;data▁&#39;, &#39;dur&#39;, &#39;able▁&#39;, &#39;goo&#39;, &#39;ds▁&#39;, &#39;or&#39;, &#39;d&#39;, &#39;ers▁&#39;, &#39;ded▁&#39;, &#39;to▁&#39;, &#39;con&#39;, &#39;cer&#39;, &#39;ns▁&#39;, &#39;over▁&#39;, &#39;the▁&#39;, &#39;global▁&#39;, &#39;growth▁&#39;, &#39;out&#39;, &#39;look▁&#39;, &#39;while▁&#39;, &#39;declin&#39;, &#39;corpor&#39;, &#39;ate▁&#39;, &#39;prof&#39;, &#39;its▁&#39;, &#39;also▁&#39;, &#39;igh&#39;, &#39;ed▁&#39;, &#39;l&#39;, &#39;trade▁&#39;, &#39;h&#39;, &#39;ong▁&#39;, &#39;k&#39;, &#39;an&#39;, &#39;g▁&#39;, &#39;sen&#39;, &#39;index▁&#39;, &#39;t&#39;, &#39;um&#39;, &#39;bl&#39;, &#39;1▁&#39;, &#39;5&#39;, &#39;5▁&#39;, &#39;au&#39;, &#39;str&#39;, &#39;al&#39;, &#39;ia▁&#39;, &#39;x▁&#39;, &#39;20&#39;, &#39;0▁&#39;, &#39;di&#39;, &#39;pped▁&#39;, &#39;jap&#39;, &#39;an▁&#39;, &#39;n&#39;, &#39;i&#39;, &#39;ke&#39;, &#39;i▁&#39;, &#39;2&#39;, &#39;25▁&#39;, &#39;sh&#39;, &#39;7▁&#39;, &#39;c&#39;, &#39;ame▁&#39;, &#39;further▁&#39;, &#39;off▁&#39;, &#39;one▁&#39;, &#39;year▁&#39;, &#39;clo&#39;, &#39;sing▁&#39;, &#39;high▁&#39;, &#39;it▁&#39;, &#39;earlier▁&#39;, &#39;in▁&#39;, &#39;week▁&#39;, &#39;investors▁&#39;, &#39;cas&#39;, &#39;a&#39;, &#39;head▁&#39;, &#39;of▁&#39;, &#39;ese▁&#39;, &#39;fisc&#39;, &#39;al▁&#39;, &#39;end▁&#39;, &#39;mar&#39;, &#39;ch▁&#39;, &#39;is▁&#39;, &#39;fin&#39;, &#39;month▁&#39;, &#39;and▁&#39;, &#39;market▁&#39;, &#39;par&#39;, &#39;ticip&#39;, &#39;ts▁&#39;, &#39;have▁&#39;, &#39;expected▁&#39;, &#39;many▁&#39;, &#39;fun&#39;, &#39;oc&#39;, &#39;k▁&#39;, &#39;from▁&#39;, &#39;me&#39;, &#39;te&#39;, &#39;ic▁&#39;, &#39;1&#39;, &#39;9▁&#39;, &#39;r&#39;, &#39;ally▁&#39;, &#39;j&#39;, &#39;u&#39;, &#39;ary▁&#39;, &#39;peri&#39;, &#39;o&#39;, &#39;d▁&#39;, &#39;after▁&#39;, &#39;e&#39;, &#39;ding▁&#39;, &#39;more▁&#39;, &#39;than▁&#39;, &#39;13▁&#39;, &#39;ap&#39;, &#39;il▁&#39;, &#39;dec&#39;, &#39;ember▁&#39;, &#39;ex&#39;, &#39;port&#39;, &#39;which▁&#39;, &#39;gain&#39;, &#39;shar&#39;, &#39;p&#39;, &#39;first▁&#39;, &#39;quarter▁&#39;, &#39;back▁&#39;, &#39;ak&#39;, &#39;en&#39;, &#39;yen▁&#39;, &#39;om&#39;, &#39;to&#39;, &#39;y&#39;, &#39;is&#39;, &#39;ped▁&#39;, &#39;6&#39;, &#39;8▁&#39;, &#39;res&#39;, &#39;pec&#39;, &#39;ti&#39;, &#39;vely▁&#39;, &#39;consumer▁&#39;, &#39;elec&#39;, &#39;tr&#39;, &#39;on&#39;, &#39;ic&#39;, &#39;gi&#39;, &#39;ant▁&#39;, &#39;y▁&#39;, &#39;re&#39;, &#39;ated▁&#39;, &#39;up&#39;, &#39;side▁&#39;, &#39;p▁&#39;, &#39;sa&#39;, &#39;w▁&#39;, &#39;shares▁&#39;, &#39;ump▁&#39;, &#39;6▁&#39;, &#39;ten&#39;, &#39;pre&#39;, &#39;vi&#39;, &#39;ous▁&#39;, &#39;15▁&#39;, &#39;follow&#39;, &#39;repor&#39;, &#39;that▁&#39;, &#39;ta&#39;, &#39;w&#39;, &#39;ha&#39;, &#39;ci&#39;, &#39;sion▁&#39;, &#39;industry▁&#39;, &#39;bu&#39;, &#39;ying▁&#39;, &#39;10▁&#39;, &#39;manufac&#39;, &#39;tur&#39;, &#39;er▁&#39;, &#39;9&#39;, &#39;billion▁&#39;, &#39;with▁&#39;, &#39;two▁&#39;, &#39;form▁&#39;, &#39;e▁&#39;, &#39;up▁&#39;, &#39;li&#39;, &#39;qu&#39;, &#39;id▁&#39;, &#39;cr&#39;, &#39;st&#39;, &#39;pl&#39;, &#39;ay▁&#39;, &#39;production▁&#39;, &#39;el&#39;, &#39;se&#39;, &#39;where▁&#39;, &#39;under▁&#39;, &#39;pres&#39;, &#39;sure▁&#39;, &#39;am&#39;, &#39;ing&#39;, &#39;er&#39;, &#39;f&#39;, &#39;ear&#39;, &#39;ard▁&#39;, &#39;china▁&#39;, &#39;wor&#39;, &#39;ries▁&#39;, &#39;pic&#39;, &#39;c▁&#39;, &#39;pro&#39;, &#39;per&#39;, &#39;ty▁&#39;, &#39;big&#39;, &#39;gest▁&#39;, &#39;fe▁&#39;, &#39;in&#39;, &#39;sur&#39;, &#39;dro&#39;, &#39;4▁&#39;, &#39;ting▁&#39;, &#39;201&#39;, &#39;net▁&#39;, &#39;come▁&#39;, &#39;rose▁&#39;, &#39;ny&#39;, &#39;0&#39;, &#39;3▁&#39;, &#39;mis&#39;, &#39;expec&#39;, &#39;ations▁&#39;, &#39;ip&#39;, &#39;tain&#39;, &#39;lin&#39;, &#39;es▁&#39;, &#39;fell▁&#39;, &#39;ation▁&#39;, &#39;lar&#39;, &#39;car&#39;, &#39;ri&#39;, &#39;reported▁&#39;, &#39;lo&#39;, &#39;ss▁&#39;, &#39;2▁&#39;, &#39;7&#39;, &#39;last▁&#39;, &#39;wi&#39;, &#39;der▁&#39;, &#39;average▁&#39;, &#39;estimate▁&#39;, &#39;ris&#39;, &#39;fu&#39;, &#39;el▁&#39;, &#39;co&#39;, &#39;sts▁&#39;, &#39;demand▁&#39;, &#39;port▁&#39;, &#39;oper&#39;, &#39;at&#39;, &#39;or▁&#39;, &#39;m&#39;, &#39;chan&#39;, &#39;old&#39;, &#39;ings▁&#39;, &#39;ann&#39;, &#39;due▁&#39;, &#39;ar&#39;, &#39;b&#39;, &#39;de&#39;, &#39;trad&#39;, &#39;chin&#39;, &#39;ban&#39;, &#39;ks▁&#39;, &#39;indu&#39;, &#39;stri&#39;, &#39;comm&#39;, &#39;ial▁&#39;, &#39;bank▁&#39;, &#39;down▁&#39;, &#39;their▁&#39;, &#39;earnings▁&#39;, &#39;lat&#39;, &#39;ra&#39;, &#39;mat&#39;, &#39;produc&#39;, &#39;cont&#39;, &#39;ted▁&#39;, &#39;ses▁&#39;, &#39;op&#39;, &#39;per▁&#39;, &#39;min&#39;, &#39;g&#39;, &#39;x&#39;, &#39;company▁&#39;, &#39;um▁&#39;, &#39;ch&#39;, &#39;co▁&#39;, &#39;oil▁&#39;, &#39;maj&#39;, &#39;ors▁&#39;, &#39;pe&#39;, &#39;ro&#39;, &#39;no&#39;, &#39;but▁&#39;, &#39;perform&#39;, &#39;onal▁&#39;, &#39;equ&#39;, &#39;ities▁&#39;, &#39;remain&#39;, &#39;se▁&#39;, &#39;four▁&#39;, &#39;was▁&#39;, &#39;by▁&#39;, &#39;line▁&#39;, &#39;le&#39;, &#39;coun&#39;, &#39;try▁&#39;, &#39;tion▁&#39;, &#39;un&#39;, &#39;der&#39;, &#39;3&#39;, &#39;will▁&#39;, &#39;be▁&#39;, &#39;between▁&#39;, &#39;4&#39;, &#39;00▁&#39;, &#39;million▁&#39;, &#39;50▁&#39;, &#39;below▁&#39;, &#39;it&#39;, &#39;increased▁&#39;, &#39;t▁&#39;, &#39;ther▁&#39;, &#39;v&#39;, &#39;ity▁&#39;, &#39;euro&#39;, &#39;il&#39;, &#39;en▁&#39;, &#39;sup&#39;, &#39;por&#39;, &#39;o▁&#39;, &#39;z&#39;, &#39;lead&#39;, &#39;increase▁&#39;, &#39;si&#39;, &#39;deb&#39;, &#39;fi&#39;, &#39;all▁&#39;, &#39;at▁&#39;, &#39;fr&#39;, &#39;ance▁&#39;, &#39;ac&#39;, &#39;da&#39;, &#39;ged▁&#39;, &#39;pu&#39;, &#39;sh▁&#39;, &#39;offic&#39;, &#39;employ&#39;, &#39;ment▁&#39;, &#39;change▁&#39;, &#39;rele&#39;, &#39;ase▁&#39;, &#39;iti&#39;, &#39;jo&#39;, &#39;ess▁&#39;, &#39;cl&#39;, &#39;im&#39;, &#39;soli&#39;, &#39;ec&#39;, &#39;product▁&#39;, &#39;ver&#39;, &#39;strong▁&#39;, &#39;br&#39;, &#39;rec&#39;, &#39;og&#39;, &#39;hel&#39;, &#39;pepsico▁&#39;, &#39;inc▁&#39;, &#39;nyse▁&#39;, &#39;n▁&#39;, &#39;ir&#39;, &#39;performance▁&#39;, &#39;tt&#39;, &#39;le▁&#39;, &#39;mi&#39;, &#39;revenue▁&#39;, &#39;eps▁&#39;, &#39;bar&#39;, &#39;ely▁&#39;, &#39;consensus▁&#39;, &#39;targ&#39;, &#39;et▁&#39;, &#39;for&#39;, &#39;ex▁&#39;, &#39;fl&#39;, &#39;tu&#39;, &#39;of&#39;, &#39;ter▁&#39;, &#39;so&#39;, &#39;sales▁&#39;, &#39;some▁&#39;, &#39;po&#39;, &#39;ght▁&#39;, &#39;results▁&#39;, &#39;us▁&#39;, &#39;low▁&#39;, &#39;do&#39;, &#39;ad▁&#39;, &#39;ture▁&#39;, &#39;this▁&#39;, &#39;analy&#39;, &#39;tic&#39;, &#39;what▁&#39;, &#39;you▁&#39;, &#39;can▁&#39;, &#39;be&#39;, &#39;ond▁&#39;, &#39;ap▁&#39;, &#39;est▁&#39;, &#39;q&#39;, &#39;high&#39;, &#39;gener&#39;, &#39;18▁&#39;, &#39;shor&#39;, &#39;ago▁&#39;, &#39;ful&#39;, &#39;l▁&#39;, &#39;over&#39;, &#39;view▁&#39;, &#39;sted▁&#39;, &#39;11▁&#39;, &#39;show&#39;, &#39;st▁&#39;, &#39;years▁&#39;, &#39;gu&#39;, &#39;id&#39;, &#39;loo&#39;, &#39;king▁&#39;, &#39;however▁&#39;, &#39;call&#39;, &#39;ill&#39;, &#39;about▁&#39;, &#39;es&#39;, &#39;boo&#39;, &#39;get▁&#39;, &#39;ning▁&#39;, &#39;mon&#39;, &#39;ey▁&#39;, &#39;r▁&#39;, &#39;see&#39;, &#39;recent▁&#39;, &#39;he&#39;, &#39;av&#39;, &#39;ily▁&#39;, &#39;dri&#39;, &#39;ve▁&#39;, &#39;through▁&#39;, &#39;ff&#39;, &#39;enti&#39;, &#39;products▁&#39;, &#39;estim&#39;, &#39;early▁&#39;, &#39;has▁&#39;, &#39;la&#39;, &#39;num&#39;, &#39;ber▁&#39;, &#39;new▁&#39;, &#39;ach▁&#39;, &#39;ating▁&#39;, &#39;retail▁&#39;, &#39;addi&#39;, &#39;int&#39;, &#39;duc&#39;, &#39;ef&#39;, &#39;are▁&#39;, &#39;ow&#39;, &#39;expan&#39;, &#39;stre&#39;, &#39;am▁&#39;, &#39;su&#39;, &#39;verage▁&#39;, &#39;business▁&#39;, &#39;inclu&#39;, &#39;ative▁&#39;, &#39;pac&#39;, &#39;ag&#39;, &#39;reas&#39;, &#39;ons▁&#39;, &#39;all&#39;, &#39;people▁&#39;, &#39;increas&#39;, &#39;bec&#39;, &#39;wee&#39;, &#39;us&#39;, &#39;heal&#39;, &#39;th▁&#39;, &#39;they▁&#39;, &#39;such▁&#39;, &#39;if&#39;, &#39;enc&#39;, &#39;our&#39;, &#39;age▁&#39;, &#39;ke▁&#39;, &#39;sc&#39;, &#39;consum&#39;, &#39;pur&#39;, &#39;comp&#39;, &#39;em&#39;, &#39;ent&#39;, &#39;mo&#39;, &#39;sal&#39;, &#39;acks▁&#39;, &#39;world▁&#39;, &#39;ence▁&#39;, &#39;develop&#39;, &#39;econom&#39;, &#39;ies▁&#39;, &#39;ge▁&#39;, &#39;opportun&#39;, &#39;go&#39;, &#39;gre&#39;, &#39;adv&#39;, &#39;ys▁&#39;, &#39;ple▁&#39;, &#39;de▁&#39;, &#39;ju&#39;, &#39;because▁&#39;, &#39;when▁&#39;, &#39;cu&#39;, &#39;als▁&#39;, &#39;other▁&#39;, &#39;ving▁&#39;, &#39;benef&#39;, &#39;om▁&#39;, &#39;today▁&#39;, &#39;eg&#39;, &#39;ory▁&#39;, &#39;ack▁&#39;, &#39;off&#39;, &#39;set▁&#39;, &#39;tn&#39;, &#39;grow&#39;, &#39;manag&#39;, &#39;ement▁&#39;, &#39;gro&#39;, &#39;point▁&#39;, &#39;third▁&#39;, &#39;coming▁&#39;, &#39;gres&#39;, &#39;sive▁&#39;, &#39;marke&#39;, &#39;ft▁&#39;, &#39;ile▁&#39;, &#39;ran&#39;, &#39;past▁&#39;, &#39;sp&#39;, &#39;ast▁&#39;, &#39;ul&#39;, &#39;pr&#39;, &#39;techn&#39;, &#39;most▁&#39;, &#39;fac&#39;, &#39;pa&#39;, &#39;ig&#39;, &#39;into▁&#39;, &#39;ou&#39;, &#39;bre&#39;, &#39;chi&#39;, &#39;ps▁&#39;, &#39;these▁&#39;, &#39;margin▁&#39;, &#39;provi&#39;, &#39;bo&#39;, &#39;support▁&#39;, &#39;been▁&#39;, &#39;out▁&#39;, &#39;gr&#39;, &#39;ph&#39;, &#39;wh&#39;, &#39;ves▁&#39;, &#39;way▁&#39;, &#39;another▁&#39;, &#39;ati&#39;, &#39;oun&#39;, &#39;current▁&#39;, &#39;m▁&#39;, &#39;2019▁&#39;, &#39;pri&#39;, &#39;bet&#39;, &#39;cap&#39;, &#39;sim&#39;, &#39;iz&#39;, &#39;intern&#39;, &#39;ational▁&#39;, &#39;ear▁&#39;, &#39;capit&#39;, &#39;still▁&#39;, &#39;ven▁&#39;, &#39;there▁&#39;, &#39;so▁&#39;, &#39;much▁&#39;, &#39;make▁&#39;, &#39;mor&#39;, &#39;ce▁&#39;, &#39;ans▁&#39;, &#39;no▁&#39;, &#39;may▁&#39;, &#39;exp&#39;, &#39;share▁&#39;, &#39;marg&#39;, &#39;ne&#39;, &#39;impac&#39;, &#39;ents▁&#39;, &#39;busin&#39;, &#39;read&#39;, &#39;ates▁&#39;, &#39;revenues▁&#39;, &#39;do▁&#39;, &#39;those▁&#39;, &#39;contin&#39;, &#39;ues▁&#39;, &#39;inv&#39;, &#39;20▁&#39;, &#39;ked▁&#39;, &#39;investment▁&#39;, &#39;mex&#39;, &#39;ico▁&#39;, &#39;next▁&#39;, &#39;thir&#39;, &#39;seen▁&#39;, &#39;eng&#39;, &#39;term▁&#39;, &#39;currency▁&#39;, &#39;low&#39;, &#39;lim&#39;, &#39;vol&#39;, &#39;ar▁&#39;, &#39;ough▁&#39;, &#39;well▁&#39;, &#39;not▁&#39;, &#39;ol&#39;, &#39;group▁&#39;, &#39;att&#39;, &#39;ind&#39;, &#39;eh&#39;, &#39;retur&#39;, &#39;value▁&#39;, &#39;divid&#39;, &#39;ently▁&#39;, &#39;ned▁&#39;, &#39;aliz&#39;, &#39;before▁&#39;, &#39;spec&#39;, &#39;same▁&#39;, &#39;disc&#39;, &#39;sed▁&#39;, &#39;end&#39;, &#39;earch▁&#39;, &#39;report▁&#39;, &#39;only▁&#39;, &#39;sel&#39;, &#39;ect▁&#39;, &#39;secur&#39;, &#39;companies▁&#39;, &#39;can&#39;, &#39;sub&#39;, &#39;sti&#39;, &#39;te▁&#39;, &#39;any▁&#39;, &#39;lu&#39;, &#39;ure▁&#39;, &#39;ited▁&#39;, &#39;based▁&#39;, &#39;form&#39;, &#39;cur&#39;, &#39;sec&#39;, &#39;sul&#39;, &#39;ob&#39;, &#39;belie&#39;, &#39;ved▁&#39;, &#39;ther&#39;, &#39;fore▁&#39;, &#39;estimates▁&#39;, &#39;best▁&#39;, &#39;with&#39;, &#39;mic&#39;, &#39;technology▁&#39;, &#39;nasdaq▁&#39;, &#39;higher▁&#39;, &#39;price▁&#39;, &#39;hi&#39;, &#39;analysts▁&#39;, &#39;ific&#39;, &#39;60▁&#39;, &#39;ll▁&#39;, &#39;just▁&#39;, &#39;12▁&#39;, &#39;14▁&#39;, &#39;equity▁&#39;, &#39;buy▁&#39;, &#39;better▁&#39;, &#39;hold▁&#39;, &#39;surprise▁&#39;, &#39;tim&#39;, &#39;quar&#39;, &#39;ter&#39;, &#39;tor▁&#39;, &#39;ite▁&#39;, &#39;three▁&#39;, &#39;ight▁&#39;, &#39;time▁&#39;, &#39;around▁&#39;, &#39;tions▁&#39;, &#39;pric&#39;, &#39;fri&#39;, &#39;trading▁&#39;, &#39;move▁&#39;, &#39;fed▁&#39;, &#39;rates▁&#39;, &#39;tw&#39;, &#39;days▁&#39;, &#39;red▁&#39;, &#39;8&#39;, &#39;sig&#39;, &#39;ain▁&#39;, &#39;h▁&#39;, &#39;acti&#39;, &#39;000▁&#39;, &#39;compar&#39;, &#39;trac&#39;, &#39;posi&#39;, &#39;gold▁&#39;, &#39;silver▁&#39;, &#39;f▁&#39;, &#39;seas&#39;, &#39;ma&#39;, &#39;vel&#39;, &#39;resul&#39;, &#39;ru&#39;, &#39;rati&#39;, &#39;tal▁&#39;, &#39;like▁&#39;, &#39;ger▁&#39;, &#39;ever▁&#39;, &#39;cy&#39;, &#39;ical▁&#39;, &#39;now▁&#39;, &#39;major▁&#39;, &#39;economy▁&#39;, &#39;rate▁&#39;, &#39;key▁&#39;, &#39;val&#39;, &#39;gh&#39;, &#39;est&#39;, &#39;news▁&#39;, &#39;cre&#39;, &#39;som&#39;, &#39;think▁&#39;, &#39;rel&#39;, &#39;prices▁&#39;, &#39;since▁&#39;, &#39;should▁&#39;, &#39;ue▁&#39;, &#39;months▁&#39;, &#39;du&#39;, &#39;pp&#39;, &#39;see▁&#39;, &#39;very▁&#39;, &#39;ver▁&#39;, &#39;govern&#39;, &#39;valu&#39;, &#39;stor&#39;, &#39;leg&#39;, &#39;ste&#39;, &#39;old▁&#39;, &#39;man&#39;, &#39;infl&#39;, &#39;ent▁&#39;, &#39;kers▁&#39;, &#39;them▁&#39;, &#39;main&#39;, &#39;medi&#39;, &#39;goldman▁&#39;, &#39;ach&#39;, &#39;long▁&#39;, &#39;forec&#39;, &#39;who▁&#39;, &#39;tal&#39;, &#39;ain&#39;, &#39;consid&#39;, &#39;against▁&#39;, &#39;re▁&#39;, &#39;americ&#39;, &#39;oo&#39;, &#39;me▁&#39;, &#39;ab&#39;, &#39;inter&#39;, &#39;trend▁&#39;, &#39;sy&#39;, &#39;stoc&#39;, &#39;new&#39;, &#39;indic&#39;, &#39;sector▁&#39;, &#39;fir&#39;, &#39;fe&#39;, &#39;likely▁&#39;, &#39;ail&#39;, &#39;war&#39;, &#39;son▁&#39;, &#39;ill▁&#39;, &#39;yn&#39;, &#39;ber&#39;, &#39;our▁&#39;, &#39;ments▁&#39;, &#39;if▁&#39;, &#39;illion▁&#39;, &#39;stocks▁&#39;, &#39;we▁&#39;, &#39;ms▁&#39;, &#39;reta&#39;, &#39;portfoli&#39;, &#39;tors▁&#39;, &#39;deli&#39;, &#39;tive▁&#39;, &#39;curr&#39;, &#39;ans&#39;, &#39;servic&#39;, &#39;qui&#39;, &#39;rou&#39;, &#39;anc&#39;, &#39;wa&#39;, &#39;operating▁&#39;, &#39;30▁&#39;, &#39;bas&#39;, &#39;poin&#39;, &#39;zacks▁&#39;, &#39;rank▁&#39;, &#39;currently▁&#39;, &#39;would▁&#39;, &#39;strateg&#39;, &#39;2017▁&#39;, &#39;500▁&#39;, &#39;even▁&#39;, &#39;top▁&#39;, &#39;reuters▁&#39;, &#39;ward▁&#39;, &#39;recor&#39;, &#39;10&#39;, &#39;level▁&#39;, &#39;level&#39;, &#39;above▁&#39;, &#39;ree▁&#39;, &#39;star&#39;, &#39;how▁&#39;, &#39;ow▁&#39;, &#39;here▁&#39;, &#39;dollar▁&#39;, &#39;sum&#39;, &#39;cor&#39;, &#39;had▁&#39;, &#39;could▁&#39;, &#39;fol&#39;, &#39;his▁&#39;, &#39;dow&#39;, &#39;ade▁&#39;, &#39;inc&#39;, &#39;dem&#39;, &#39;presid&#39;, &#39;divi&#39;, &#39;tly▁&#39;, &#39;esday▁&#39;, &#39;ir▁&#39;, &#39;ser&#39;, &#39;ble▁&#39;, &#39;ener&#39;, &#39;financ&#39;, &#39;technolog&#39;, &#39;ere▁&#39;, &#39;hn&#39;, &#39;del&#39;, &#39;thin&#39;, &#39;led▁&#39;, &#39;un▁&#39;, &#39;bri&#39;, &#39;according▁&#39;, &#39;and&#39;, &#39;et&#39;, &#39;ratio▁&#39;, &#39;cash▁&#39;, &#39;yiel&#39;, &#39;b▁&#39;, &#39;dge▁&#39;, &#39;he▁&#39;, &#39;compan&#39;, &#39;ven&#39;, &#39;ore▁&#39;, &#39;21▁&#39;, &#39;acc&#39;, &#39;mu&#39;, &#39;fund▁&#39;, &#39;tec&#39;, &#39;vic&#39;, &#39;ures▁&#39;, &#39;sil&#39;, &#39;custom&#39;, &#39;lik&#39;, &#39;az&#39;, &#39;currenc&#39;, &#39;sid&#39;, &#39;said▁&#39;, &#39;percent▁&#39;, &#39;sell▁&#39;, &#39;200&#39;, &#39;cru&#39;, &#39;res▁&#39;, &#39;dn&#39;, &#39;president▁&#39;, &#39;illi&#39;, &#39;oli&#39;, &#39;sin&#39;, &#39;ose▁&#39;, &#39;olog&#39;, &#39;eu&#39;, &#39;ce&#39;, &#39;cents▁&#39;, &#39;positive▁&#39;, &#39;ters▁&#39;, &#39;man▁&#39;, &#39;ie&#39;, &#39;ben&#39;, &#39;tg&#39;, &#39;ell▁&#39;, &#39;rise▁&#39;, &#39;fer&#39;, &#39;emplo&#39;, &#39;polic&#39;, &#39;ound▁&#39;, &#39;glob&#39;, &#39;plo&#39;, &#39;ich▁&#39;, &#39;amer&#39;, &#39;strat&#39;, &#39;year&#39;, &#39;trump▁&#39;, &#39;security▁&#39;, &#39;dollar&#39;, &#39;vie&#39;, &#39;total▁&#39;, &#39;cloud▁&#39;, &#39;oud▁&#39;, &#39;aly&#39;, &#39;you&#39;, &#39;stry▁&#39;, &#39;accor&#39;, &#39;activi&#39;, &#39;ace▁&#39;, &#39;pol&#39;, &#39;bel&#39;, &#39;gh▁&#39;, &#39;overn&#39;, &#39;uters▁&#39;, &#39;glo&#39;, &#39;month&#39;, &#39;arg&#39;, &#39;surpri&#39;, &#39;financial▁&#39;, &#39;gold&#39;, &#39;af&#39;, &#39;ld▁&#39;, &#39;earn&#39;, &#39;the&#39;, &#39;q▁&#39;, &#39;throu&#39;, &#39;ps&#39;, &#39;sus▁&#39;, &#39;isc&#39;, &#39;ata▁&#39;, &#39;dol&#39;, &#39;ext▁&#39;, &#39;ause▁&#39;, &#39;fur&#39;, &#39;econ&#39;, &#39;cent▁&#39;, &#39;lier▁&#39;, &#39;reven&#39;, &#39;ince▁&#39;, &#39;nas&#39;, &#39;how&#39;, &#39;iel&#39;, &#39;devel&#39;, &#39;fro&#39;, &#39;manu&#39;])
Number of tokens: 1025
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="encoding-and-decoding">
<h3>Encoding and Decoding<a class="headerlink" href="#encoding-and-decoding" title="Permalink to this headline">#</a></h3>
<section id="decoding">
<h4>Decoding<a class="headerlink" href="#decoding" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Decoding is extremely simple.</p></li>
<li><p>Just concatenate all the tokens together and remove the stop token <code class="docutils literal notranslate"><span class="pre">&lt;/w&gt;</span></code>.</p></li>
<li><p>For example, if the encoded sequence is [<code class="docutils literal notranslate"><span class="pre">the&lt;/w&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">high</span></code>, <code class="docutils literal notranslate"><span class="pre">est&lt;/w&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">moun</span></code>, <code class="docutils literal notranslate"><span class="pre">tain&lt;/w&gt;</span></code>], the decoded sequence is <code class="docutils literal notranslate"><span class="pre">the</span> <span class="pre">highest</span> <span class="pre">mountain</span></code>.</p></li>
</ul>
</section>
<section id="encoding">
<h4>Encoding<a class="headerlink" href="#encoding" title="Permalink to this headline">#</a></h4>
<p>For the sentence <code class="docutils literal notranslate"><span class="pre">the</span> <span class="pre">highest</span> <span class="pre">mountain</span></code>,</p>
<ul class="simple">
<li><p>List all the tokens in the vocabulary in the order of their length.</p></li>
<li><p>For each word, find the longest token that is a subword of the word.</p></li>
<li><p>Assume that the vocabulary is <code class="docutils literal notranslate"><span class="pre">[&quot;errrr&lt;/w&gt;&quot;,</span> <span class="pre">&quot;tain&lt;/w&gt;&quot;,</span> <span class="pre">&quot;moun&quot;,</span> <span class="pre">&quot;est&lt;/w&gt;&quot;,</span> <span class="pre">&quot;high&quot;,</span> <span class="pre">&quot;the&lt;/w&gt;&quot;,</span> <span class="pre">&quot;a&lt;/w&gt;&quot;]</span></code>.</p></li>
<li><p>Iterate from the longest token <code class="docutils literal notranslate"><span class="pre">errrr&lt;/w&gt;</span></code> to the shortest token <code class="docutils literal notranslate"><span class="pre">a&lt;/w&gt;</span></code> trying to find the longest token that is a subword of the word.</p></li>
<li><p>After all the tokens are checked, all the substrings of the word will be replaced by the tokens.</p></li>
<li><p>If there is no token that is a subword of the word, then the word is replaced by the unknown token <code class="docutils literal notranslate"><span class="pre">&lt;/u&gt;</span></code>.</p></li>
<li><p>In this example, the word <code class="docutils literal notranslate"><span class="pre">the</span></code> is replaced by <code class="docutils literal notranslate"><span class="pre">the&lt;/w&gt;</span></code>, the word <code class="docutils literal notranslate"><span class="pre">highest</span></code> is replaced by <code class="docutils literal notranslate"><span class="pre">high</span> <span class="pre">est&lt;/w&gt;</span></code>, and the word <code class="docutils literal notranslate"><span class="pre">mountain</span></code> is replaced by <code class="docutils literal notranslate"><span class="pre">moun</span> <span class="pre">tain&lt;/w&gt;</span></code>.</p></li>
<li><p>Encoding is very computationally expensive.</p></li>
</ul>
</section>
</section>
<section id="bpe-encoding-and-decoding-in-practice">
<h3>BPE Encoding and Decoding in Practice<a class="headerlink" href="#bpe-encoding-and-decoding-in-practice" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">measure_token_length</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">space_token</span><span class="o">=</span><span class="s2">&quot;▁&quot;</span><span class="p">):</span>
    <span class="n">space_token_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">space_token</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">token</span><span class="p">[</span><span class="o">-</span><span class="n">space_token_len</span><span class="p">:]</span> <span class="o">==</span> <span class="n">space_token</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">-</span> <span class="n">space_token_len</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">encode_word</span><span class="p">(</span><span class="n">string</span><span class="p">,</span> <span class="n">sorted_tokens</span><span class="p">,</span> <span class="n">unknown_token</span><span class="o">=</span><span class="s2">&quot;&lt;/u&gt;&quot;</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">string</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
    <span class="n">sorted_tokens</span> <span class="o">=</span> <span class="n">sorted_tokens</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">sorted_tokens</span> <span class="o">==</span> <span class="p">[]:</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">unknown_token</span><span class="p">]</span>

    <span class="n">string_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sorted_tokens</span><span class="p">)):</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">sorted_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">token_reg</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;[.]&quot;</span><span class="p">))</span>

        <span class="n">matched_positions</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">end</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">re</span><span class="o">.</span><span class="n">finditer</span><span class="p">(</span><span class="n">token_reg</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">matched_positions</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">substring_end_positions</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">matched_position</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">matched_position</span> <span class="ow">in</span> <span class="n">matched_positions</span>
        <span class="p">]</span>

        <span class="n">substring_start_position</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">substring_end_position</span> <span class="ow">in</span> <span class="n">substring_end_positions</span><span class="p">:</span>
            <span class="n">substring</span> <span class="o">=</span> <span class="n">string</span><span class="p">[</span><span class="n">substring_start_position</span><span class="p">:</span><span class="n">substring_end_position</span><span class="p">]</span>
            <span class="n">string_tokens</span> <span class="o">+=</span> <span class="n">encode_word</span><span class="p">(</span>
                <span class="n">string</span><span class="o">=</span><span class="n">substring</span><span class="p">,</span>
                <span class="n">sorted_tokens</span><span class="o">=</span><span class="n">sorted_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:],</span>
                <span class="n">unknown_token</span><span class="o">=</span><span class="n">unknown_token</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">string_tokens</span> <span class="o">+=</span> <span class="p">[</span><span class="n">token</span><span class="p">]</span>
            <span class="n">substring_start_position</span> <span class="o">=</span> <span class="n">substring_end_position</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">remaining_substring</span> <span class="o">=</span> <span class="n">string</span><span class="p">[</span><span class="n">substring_start_position</span><span class="p">:]</span>
        <span class="n">string_tokens</span> <span class="o">+=</span> <span class="n">encode_word</span><span class="p">(</span>
            <span class="n">string</span><span class="o">=</span><span class="n">remaining_substring</span><span class="p">,</span>
            <span class="n">sorted_tokens</span><span class="o">=</span><span class="n">sorted_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:],</span>
            <span class="n">unknown_token</span><span class="o">=</span><span class="n">unknown_token</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">break</span>
    <span class="k">return</span> <span class="n">string_tokens</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_tokenization</span><span class="p">(</span><span class="n">word_given</span><span class="p">,</span> <span class="n">sorted_tokens</span><span class="p">,</span> <span class="n">vocab_tokenization</span><span class="p">):</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenizing word: </span><span class="si">{}</span><span class="s2">...&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word_given</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">word_given</span> <span class="ow">in</span> <span class="n">vocab_tokenization</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenization of the known word:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">vocab_tokenization</span><span class="p">[</span><span class="n">word_given</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenization treating the known word as unknown:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="n">encode_word</span><span class="p">(</span>
                <span class="n">string</span><span class="o">=</span><span class="n">word_given</span><span class="p">,</span> <span class="n">sorted_tokens</span><span class="o">=</span><span class="n">sorted_tokens</span><span class="p">,</span> <span class="n">unknown_token</span><span class="o">=</span><span class="s2">&quot;&lt;/u&gt;&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenizating of the unknown word:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="n">encode_word</span><span class="p">(</span>
                <span class="n">string</span><span class="o">=</span><span class="n">word_given</span><span class="p">,</span> <span class="n">sorted_tokens</span><span class="o">=</span><span class="n">sorted_tokens</span><span class="p">,</span> <span class="n">unknown_token</span><span class="o">=</span><span class="s2">&quot;&lt;/u&gt;&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span><span class="p">,</span> <span class="n">vocab_tokenization</span> <span class="o">=</span> <span class="n">get_tokens_from_vocab</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

<span class="n">sorted_tokens_tuple</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="p">(</span><span class="n">measure_token_length</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sorted_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">freq</span><span class="p">)</span> <span class="ow">in</span> <span class="n">sorted_tokens_tuple</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sorted_tokens</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;performance▁&#39;, &#39;investment▁&#39;, &#39;technology▁&#39;, &#39;production▁&#39;, &#39;investors▁&#39;, &#39;companies▁&#39;, &#39;consensus▁&#39;, &#39;currently▁&#39;, &#39;financial▁&#39;, &#39;increased▁&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_given_known</span> <span class="o">=</span> <span class="s1">&#39;investors▁&#39;</span>

<span class="n">print_tokenization</span><span class="p">(</span><span class="n">word_given_known</span><span class="p">,</span> <span class="n">sorted_tokens</span><span class="p">,</span> <span class="n">vocab_tokenization</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokenizing word: investors▁...
Tokenization of the known word:
[&#39;investors▁&#39;]
Tokenization treating the known word as unknown:
[&#39;investors▁&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_given_unknown</span> <span class="o">=</span> <span class="s1">&#39;dogecoin▁&#39;</span>

<span class="n">print_tokenization</span><span class="p">(</span><span class="n">word_given_unknown</span><span class="p">,</span> <span class="n">sorted_tokens</span><span class="p">,</span> <span class="n">vocab_tokenization</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokenizing word: dogecoin&lt;/w&gt;...
Tokenizating of the unknown word:
[&#39;do&#39;, &#39;g&#39;, &#39;ec&#39;, &#39;o&#39;, &#39;in&#39;, &#39;w&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">space_token</span><span class="o">=</span><span class="s1">&#39;▁&#39;</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;\s+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="o">+</span> <span class="n">space_token</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)]</span>
    <span class="n">encoded_words</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">encode_word</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">sorted_tokens</span><span class="p">,</span> <span class="n">unknown_token</span><span class="o">=</span><span class="s2">&quot;&lt;/u&gt;&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">encoded_words</span><span class="p">,</span> <span class="p">[])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;Investment opportunities in the company&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;investment▁&#39;, &#39;opportun&#39;, &#39;ities▁&#39;, &#39;in▁&#39;, &#39;the▁&#39;, &#39;company▁&#39;]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="byte-level-byte-pair-encoding-bbpe">
<h2>Byte-level Byte Pair Encoding (BBPE)<a class="headerlink" href="#byte-level-byte-pair-encoding-bbpe" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Byte-level BPE is a variant of BPE.</p></li>
<li><p>It splits words into sequences of bytes instead of characters.</p></li>
</ul>
</section>
<section id="wordpiece">
<h2>Wordpiece<a class="headerlink" href="#wordpiece" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>WordPiece is a subword tokenization algorithm that was proposed by Google in 2016.</p></li>
<li><p>Wordpiece works almost the same as BPE, but it uses a different way to merge the tokens.</p></li>
<li><p>Wordpiece merges tokens based on likelihood of the tokens instead of frequency.</p></li>
<li><p>The likelihood is calculated with the <span class="math notranslate nohighlight">\(p(w_{i}, w_{j})/p(w_{i})p(w_{j})\)</span> formula.</p></li>
</ul>
<section id="tokenization-algorithm">
<h3>Tokenization Algorithm<a class="headerlink" href="#tokenization-algorithm" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Tokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned.</p></li>
<li><p>Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it.</p></li>
<li><p>For instance, for the word <code class="docutils literal notranslate"><span class="pre">pineapple</span></code>, the longest subword in the vocabulary is <code class="docutils literal notranslate"><span class="pre">pine</span></code>, so the word is split into <code class="docutils literal notranslate"><span class="pre">pine</span></code> and <code class="docutils literal notranslate"><span class="pre">##apple</span></code>.</p></li>
<li><p>Then, the algorithm finds the longest subword in the vcabulary that is in <code class="docutils literal notranslate"><span class="pre">##apple</span></code>, which is <code class="docutils literal notranslate"><span class="pre">##apple</span></code>, so the word <code class="docutils literal notranslate"><span class="pre">pineapple</span></code> is tokenized into <code class="docutils literal notranslate"><span class="pre">pine</span></code> and <code class="docutils literal notranslate"><span class="pre">##apple</span></code>.</p></li>
</ul>
</section>
<section id="wordpiece-in-practice">
<h3>WordPiece in Practice<a class="headerlink" href="#wordpiece-in-practice" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ekorpkit.models.tokenizer.wordpiece</span> <span class="kn">import</span> <span class="n">WordPieceTokenizer</span>

<span class="n">wp</span> <span class="o">=</span> <span class="n">WordPieceTokenizer</span><span class="p">()</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">wp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0, vocab size: 79
Best pair: (&#39;8&#39;, &#39;##9&#39;)
New token: 89
Iteration: 100, vocab size: 179
Best pair: (&#39;##1&#39;, &#39;##6&#39;)
New token: ##16
Iteration: 200, vocab size: 279
Best pair: (&#39;4&#39;, &#39;##14&#39;)
New token: 414
Iteration: 300, vocab size: 379
Best pair: (&#39;n&#39;, &#39;##225&#39;)
New token: n225
Iteration: 400, vocab size: 479
Best pair: (&#39;50&#39;, &#39;##0&#39;)
New token: 500
Iteration: 500, vocab size: 579
Best pair: (&#39;57&#39;, &#39;##0&#39;)
New token: 570
Iteration: 600, vocab size: 679
Best pair: (&#39;e&#39;, &#39;##q&#39;)
New token: eq
Iteration: 700, vocab size: 779
Best pair: (&#39;exx&#39;, &#39;##o&#39;)
New token: exxo
Iteration: 800, vocab size: 879
Best pair: (&#39;quantif&#39;, &#39;##y&#39;)
New token: quantify
Iteration: 900, vocab size: 979
Best pair: (&#39;lymp&#39;, &#39;##h&#39;)
New token: lymph
Final vocab size: 1000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">encode_word</span><span class="p">(</span><span class="s2">&quot;company&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">encode_word</span><span class="p">(</span><span class="s2">&quot;companies&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">encode_word</span><span class="p">(</span><span class="s2">&quot;회사&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;company&#39;]
[&#39;compani&#39;, &#39;##e&#39;, &#39;##s&#39;]
[&#39;[UNK]&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;Investment opportunities in the company&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;i&#39;, &#39;##n&#39;, &#39;##v&#39;, &#39;##e&#39;, &#39;##s&#39;, &#39;##t&#39;, &#39;##m&#39;, &#39;##e&#39;, &#39;##n&#39;, &#39;##t&#39;, &#39;###&#39;, &#39;###&#39;, &#39;opportuniti&#39;, &#39;##e&#39;, &#39;##s&#39;, &#39;###&#39;, &#39;###&#39;, &#39;i&#39;, &#39;##n&#39;, &#39;###&#39;, &#39;###&#39;, &#39;th&#39;, &#39;##e&#39;, &#39;###&#39;, &#39;###&#39;, &#39;company&#39;, &#39;###&#39;, &#39;###&#39;]
</pre></div>
</div>
</div>
</div>
<section id="wordpiece-step-by-step-implementation">
<h4>WordPiece Step-by-Step Implementation<a class="headerlink" href="#wordpiece-step-by-step-implementation" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>


<span class="k">def</span> <span class="nf">pre_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">lowercase</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\s+&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">initialize_vocab</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">pre_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lowercase</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">vocab</span>


<span class="n">word_freqs</span> <span class="o">=</span> <span class="n">initialize_vocab</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of words: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_freqs</span><span class="o">.</span><span class="n">keys</span><span class="p">())))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of words: 7847
</pre></div>
</div>
</div>
</div>
<p>The alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by ##:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">characters</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">characters</span><span class="p">:</span>
        <span class="n">characters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">letter</span> <span class="ow">in</span> <span class="n">word</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="k">if</span> <span class="sa">f</span><span class="s2">&quot;##</span><span class="si">{</span><span class="n">letter</span><span class="si">}</span><span class="s2">&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">characters</span><span class="p">:</span>
            <span class="n">characters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;##</span><span class="si">{</span><span class="n">letter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">characters</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">characters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;##0&#39;, &#39;##1&#39;, &#39;##2&#39;, &#39;##3&#39;, &#39;##4&#39;, &#39;##5&#39;, &#39;##6&#39;, &#39;##7&#39;, &#39;##8&#39;, &#39;##9&#39;, &#39;##a&#39;, &#39;##b&#39;, &#39;##c&#39;, &#39;##d&#39;, &#39;##e&#39;, &#39;##f&#39;, &#39;##g&#39;, &#39;##h&#39;, &#39;##i&#39;, &#39;##j&#39;, &#39;##k&#39;, &#39;##l&#39;, &#39;##m&#39;, &#39;##n&#39;, &#39;##o&#39;, &#39;##p&#39;, &#39;##q&#39;, &#39;##r&#39;, &#39;##s&#39;, &#39;##t&#39;, &#39;##u&#39;, &#39;##v&#39;, &#39;##w&#39;, &#39;##x&#39;, &#39;##y&#39;, &#39;##z&#39;, &#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;l&#39;, &#39;m&#39;, &#39;n&#39;, &#39;o&#39;, &#39;p&#39;, &#39;q&#39;, &#39;r&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;v&#39;, &#39;w&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;]
</pre></div>
</div>
</div>
</div>
<p>Add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it’s the list [“[PAD]”, “[UNK]”, “[CLS]”, “[SEP]”, “[MASK]”]:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">characters</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Split each word, with all the letters that are not the first prefixed by ##:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">splits</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">word</span><span class="p">:</span> <span class="p">[</span><span class="n">c</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;##</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>A function to compute the score of each pair:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_pair_scores</span><span class="p">(</span><span class="n">splits</span><span class="p">):</span>
    <span class="n">letter_freqs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">pair_freqs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">letter_freqs</span><span class="p">[</span><span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>
            <span class="k">continue</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">split</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">letter_freqs</span><span class="p">[</span><span class="n">split</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>
            <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>
        <span class="n">letter_freqs</span><span class="p">[</span><span class="n">split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">pair</span><span class="p">:</span> <span class="n">freq</span> <span class="o">/</span> <span class="p">(</span><span class="n">letter_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">*</span> <span class="n">letter_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">pair_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">scores</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pair_scores</span> <span class="o">=</span> <span class="n">compute_pair_scores</span><span class="p">(</span><span class="n">splits</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pair_scores</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">pair_scores</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;i&#39;, &#39;##n&#39;): 3.065532997859911e-05
(&#39;##n&#39;, &#39;##v&#39;): 5.217525332521506e-06
(&#39;##v&#39;, &#39;##e&#39;): 2.2967510416892118e-05
(&#39;##e&#39;, &#39;##s&#39;): 6.2108678847586545e-06
(&#39;##s&#39;, &#39;##t&#39;): 7.931201514160114e-06
(&#39;##t&#39;, &#39;##i&#39;): 8.905730802064189e-06
</pre></div>
</div>
</div>
</div>
<p>Find the pair with the highest score:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_pair</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="n">max_score</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">pair_scores</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">max_score</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">max_score</span> <span class="o">&lt;</span> <span class="n">score</span><span class="p">:</span>
        <span class="n">best_pair</span> <span class="o">=</span> <span class="n">pair</span>
        <span class="n">max_score</span> <span class="o">=</span> <span class="n">score</span>

<span class="nb">print</span><span class="p">(</span><span class="n">best_pair</span><span class="p">,</span> <span class="n">max_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;8&#39;, &#39;##9&#39;) 0.0004752098843655948
</pre></div>
</div>
</div>
</div>
<p>So the first merge to learn is (<code class="docutils literal notranslate"><span class="pre">8</span></code>, <code class="docutils literal notranslate"><span class="pre">##9</span></code>) -&gt; <code class="docutils literal notranslate"><span class="pre">89</span></code>. Add it to the vocabulary:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;89&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To continue, we need to apply that merge in our splits dictionary. A function for this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">merge_pair</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">splits</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">:</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">split</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">a</span> <span class="ow">and</span> <span class="n">split</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">b</span><span class="p">:</span>
                <span class="n">merge</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span> <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;##&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
                <span class="n">split</span> <span class="o">=</span> <span class="n">split</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">merge</span><span class="p">]</span> <span class="o">+</span> <span class="n">split</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span> <span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">splits</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">split</span>
    <span class="k">return</span> <span class="n">splits</span>
</pre></div>
</div>
</div>
</div>
<p>And we can have a look at the result of the first merge:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">splits</span> <span class="o">=</span> <span class="n">merge_pair</span><span class="p">(</span><span class="s2">&quot;8&quot;</span><span class="p">,</span> <span class="s2">&quot;##9&quot;</span><span class="p">,</span> <span class="n">splits</span><span class="p">)</span>
<span class="n">splits</span><span class="p">[</span><span class="s2">&quot;8920&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;89&#39;, &#39;##2&#39;, &#39;##0&#39;]
</pre></div>
</div>
</div>
</div>
<p>Now we have everything we need to loop until we have learned all the merges we want. For example, we can loop until we have a vocabulary of size 1000:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">vocab_size</span><span class="p">:</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">compute_pair_scores</span><span class="p">(</span><span class="n">splits</span><span class="p">)</span>
    <span class="n">best_pair</span><span class="p">,</span> <span class="n">max_score</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">scores</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">max_score</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">max_score</span> <span class="o">&lt;</span> <span class="n">score</span><span class="p">:</span>
            <span class="n">best_pair</span> <span class="o">=</span> <span class="n">pair</span>
            <span class="n">max_score</span> <span class="o">=</span> <span class="n">score</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="n">merge_pair</span><span class="p">(</span><span class="o">*</span><span class="n">best_pair</span><span class="p">,</span> <span class="n">splits</span><span class="p">)</span>
    <span class="n">new_token</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;##&quot;</span><span class="p">)</span>
        <span class="k">else</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">vocab</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First 10 tokens: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vocab</span><span class="p">[:</span><span class="mi">10</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Last 50 tokens: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="o">-</span><span class="mi">50</span><span class="p">:]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First 10 tokens: [&#39;[PAD]&#39;, &#39;[UNK]&#39;, &#39;[CLS]&#39;, &#39;[SEP]&#39;, &#39;[MASK]&#39;, &#39;##0&#39;, &#39;##1&#39;, &#39;##2&#39;, &#39;##3&#39;, &#39;##4&#39;]
Last 50 tokens: [&#39;thompso&#39;, &#39;accompan&#39;, &#39;accompany&#39;, &#39;accompani&#39;, &#39;thompson&#39;, &#39;compa&#39;, &#39;compan&#39;, &#39;company&#39;, &#39;compani&#39;, &#39;compar&#39;, &#39;compac&#39;, &#39;compas&#39;, &#39;compass&#39;, &#39;compact&#39;, &#39;employm&#39;, &#39;##mpl&#39;, &#39;##mply&#39;, &#39;##imply&#39;, &#39;simply&#39;, &#39;##amp&#39;, &#39;camp&#39;, &#39;ramp&#39;, &#39;hamp&#39;, &#39;damp&#39;, &#39;##vamp&#39;, &#39;##ymp&#39;, &#39;lymp&#39;, &#39;lymph&#39;, &#39;##lymp&#39;, &#39;olymp&#39;, &#39;olympi&#39;, &#39;olympic&#39;, &#39;lympho&#39;, &#39;lymphom&#39;, &#39;lymphoma&#39;, &#39;campa&#39;, &#39;campai&#39;, &#39;campaig&#39;, &#39;campaign&#39;, &#39;##rump&#39;, &#39;trump&#39;, &#39;bump&#39;, &#39;gump&#39;, &#39;pump&#39;, &#39;pumps&#39;, &#39;##lump&#39;, &#39;slump&#39;, &#39;##sump&#39;, &#39;##sumpt&#39;, &#39;##sumpti&#39;]
</pre></div>
</div>
</div>
</div>
<p>Encoding is done by finding the biggest subword in the vocabulary that is in the word, and splitting on it. Iterating on the word until it is empty:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">encode_word</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">word</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="n">i</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">]</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[:</span><span class="n">i</span><span class="p">])</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">word</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;##</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">return</span> <span class="n">tokens</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encode_word</span><span class="p">(</span><span class="s2">&quot;company&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encode_word</span><span class="p">(</span><span class="s2">&quot;companies&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encode_word</span><span class="p">(</span><span class="s2">&quot;회사&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;company&#39;]
[&#39;compani&#39;, &#39;##e&#39;, &#39;##s&#39;]
[&#39;[UNK]&#39;]
</pre></div>
</div>
</div>
</div>
<p>To tokenize a sentence, we can apply this function to each word:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">pre_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">encoded_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">encode_word</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">encoded_words</span><span class="p">,</span> <span class="p">[])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;Investment opportunities in the company&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;i&#39;, &#39;##n&#39;, &#39;##v&#39;, &#39;##e&#39;, &#39;##s&#39;, &#39;##t&#39;, &#39;##m&#39;, &#39;##e&#39;, &#39;##n&#39;, &#39;##t&#39;, &#39;opportuniti&#39;, &#39;##e&#39;, &#39;##s&#39;, &#39;i&#39;, &#39;##n&#39;, &#39;th&#39;, &#39;##e&#39;, &#39;company&#39;]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="unigram">
<h2>Unigram<a class="headerlink" href="#unigram" title="Permalink to this headline">#</a></h2>
<p>Kudo (2018) proposed the Unigram model based subword segmentation algorithm which outputs multiple subword segmentation along with their probabilities.</p>
<ul class="simple">
<li><p>The model assumes that each subword occurs independently.</p></li>
<li><p>The probability of a subword sequence <span class="math notranslate nohighlight">\(x=(x_{1}, x_{2}, \cdots, x_{n})\)</span> is calculated as <span class="math notranslate nohighlight">\(p(x)=\prod_{i=1}^{n} p(x_{i})\)</span>.</p></li>
<li><p>The most probable segmentation <span class="math notranslate nohighlight">\(x^*\)</span> for the sentence <span class="math notranslate nohighlight">\(X\)</span> is given by <span class="math notranslate nohighlight">\(x^*=\underset{x \in S(X)}{\operatorname{argmax}} P(x)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(S(X)\)</span> is a set of all possible segmentations for the sentence <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>Subword occurrence probabilities <span class="math notranslate nohighlight">\(x_{i}\)</span> are estimated using the <a class="reference external" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">expectation maximization (EM) algorithm</a> by maximizing the log-likelihood <span class="math notranslate nohighlight">\(L\)</span> of the training data <span class="math notranslate nohighlight">\(D\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L=\sum_{s = 1}^{|D|} \log(P(X^{(s)})) = \sum_{s = 1}^{|D|} \log(\sum_{x \in S(X^{(S)})} P(x))
\]</div>
<p>The procedure of obtaining the vocabulary V with a desired size.</p>
<ol class="simple">
<li><p>Initialize a reasonably big seed vocabulary.</p></li>
<li><p>Define a desired vocabulary size.</p></li>
<li><p>Optimize the subword occurrence probabilities using the EM algorithm by fixing the vocabulary.</p></li>
<li><p>Compute the loss for each subword.</p>
<ul class="simple">
<li><p>The loss of a subword depicts the decrement in the aforementioned likelihood <span class="math notranslate nohighlight">\(L\)</span> when that subword is removed from the vocabulary.</p></li>
</ul>
</li>
<li><p>Sort the subwords by loss and keep the top n% of subwords.</p>
<ul class="simple">
<li><p>Keep the subwords with a single character to avoid the out of vocabulary problem.</p></li>
</ul>
</li>
<li><p>Repeat step 3 to 5 until it reaches the desired vocabulary size defined in step 2.</p></li>
</ol>
<p>The most common way to prepare the seed vocabulary is to use the most frequent substrings and characters in the corpus. This unigram language model based subword segmentation consists of characters, subwords and words.</p>
<section id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;xlnet-base-cased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>


<span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
        <span class="n">words_with_offsets</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">offset</span> <span class="ow">in</span> <span class="n">words_with_offsets</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">vocab</span>


<span class="n">word_freqs</span> <span class="o">=</span> <span class="n">get_vocab</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">text</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of words: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_freqs</span><span class="o">.</span><span class="n">keys</span><span class="p">())))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of words: 8855
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Then, we need to initialize our vocabulary to something larger than the vocabulary size we want.</p></li>
<li><p>We have to include all the basic characters (otherwise we won’t be able to tokenize every word).</p></li>
<li><p>For the bigger substrings, we can use the most frequent substrings in the corpus.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">char_freqs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">subwords_freqs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)):</span>
        <span class="n">char_freqs</span><span class="p">[</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>
        <span class="c1"># Loop through the subwords of length at least 2</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">subwords_freqs</span><span class="p">[</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">j</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>

<span class="c1"># Sort subwords by frequency</span>
<span class="n">sorted_subwords</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">subwords_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sorted_subwords</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;▁t&#39;, 7127), (&#39;in&#39;, 5997), (&#39;▁a&#39;, 5589), (&#39;th&#39;, 5514), (&#39;he&#39;, 4739), (&#39;er&#39;, 4573), (&#39;▁th&#39;, 4262), (&#39;re&#39;, 4215), (&#39;an&#39;, 4210), (&#39;▁s&#39;, 3953)]
</pre></div>
</div>
</div>
</div>
<p>We group the characters with the best subwords to arrive at an initial vocabulary of size 2000:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">token_freqs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">char_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="o">+</span> <span class="n">sorted_subwords</span><span class="p">[:</span> <span class="mi">2000</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">char_freqs</span><span class="p">)]</span>
<span class="n">token_freqs</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">freq</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">token_freqs</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we compute the sum of all frequencies, to convert the frequencies into probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">log</span>

<span class="n">total_sum</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">freq</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">token_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
<span class="n">model</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">freq</span> <span class="o">/</span> <span class="n">total_sum</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">token_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>
</div>
</div>
</div>
<p>The main function is the one that tokenizes words using the Viterbi algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">encode_word</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">best_segmentations</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;start&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span> <span class="o">+</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;start&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)):</span>
        <span class="c1"># This should be properly filled by the previous steps of the loop</span>
        <span class="n">best_score_at_start</span> <span class="o">=</span> <span class="n">best_segmentations</span><span class="p">[</span><span class="n">start_idx</span><span class="p">][</span><span class="s2">&quot;score&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">end_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">word</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">model</span> <span class="ow">and</span> <span class="n">best_score_at_start</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">+</span> <span class="n">best_score_at_start</span>
                <span class="c1"># If we have found a better segmentation ending at end_idx, we update</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">best_segmentations</span><span class="p">[</span><span class="n">end_idx</span><span class="p">][</span><span class="s2">&quot;score&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="ow">or</span> <span class="n">best_segmentations</span><span class="p">[</span><span class="n">end_idx</span><span class="p">][</span><span class="s2">&quot;score&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">score</span>
                <span class="p">):</span>
                    <span class="n">best_segmentations</span><span class="p">[</span><span class="n">end_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;start&quot;</span><span class="p">:</span> <span class="n">start_idx</span><span class="p">,</span> <span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="n">score</span><span class="p">}</span>

    <span class="n">segmentation</span> <span class="o">=</span> <span class="n">best_segmentations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">segmentation</span><span class="p">[</span><span class="s2">&quot;score&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># We did not find a tokenization of the word -&gt; unknown</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">],</span> <span class="kc">None</span>

    <span class="n">score</span> <span class="o">=</span> <span class="n">segmentation</span><span class="p">[</span><span class="s2">&quot;score&quot;</span><span class="p">]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">segmentation</span><span class="p">[</span><span class="s2">&quot;start&quot;</span><span class="p">]</span>
    <span class="n">end</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">word</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
        <span class="n">next_start</span> <span class="o">=</span> <span class="n">best_segmentations</span><span class="p">[</span><span class="n">start</span><span class="p">][</span><span class="s2">&quot;start&quot;</span><span class="p">]</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">next_start</span>
    <span class="n">tokens</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">word</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">score</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encode_word</span><span class="p">(</span><span class="s2">&quot;apple&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encode_word</span><span class="p">(</span><span class="s2">&quot;investment&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>([&#39;app&#39;, &#39;le&#39;], 16.827095174401983)
([&#39;investment&#39;], 10.304598661839819)
</pre></div>
</div>
</div>
</div>
<p>Compute the loss:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">word_loss</span> <span class="o">=</span> <span class="n">encode_word</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">freq</span> <span class="o">*</span> <span class="n">word_loss</span>
    <span class="k">return</span> <span class="n">loss</span>


<span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>953811.2763311649
</pre></div>
</div>
</div>
</div>
<p>Computing the scores for each token:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>


<span class="k">def</span> <span class="nf">compute_scores</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">model_loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># We always keep tokens of length 1</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">model_without_token</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">model_without_token</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">scores</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model_without_token</span><span class="p">)</span> <span class="o">-</span> <span class="n">model_loss</span>
    <span class="k">return</span> <span class="n">scores</span>


<span class="n">scores</span> <span class="o">=</span> <span class="n">compute_scores</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;app&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;le&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;investment&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;invest&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;ment&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>43.05223229911644
206.79810601787176
0.0
1.416147324256599
718.6751414387254
</pre></div>
</div>
</div>
</div>
<p>Iterate until we have the desired vocabulary size:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">percent_to_remove</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1000</span><span class="p">:</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">compute_scores</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">sorted_scores</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># Remove percent_to_remove tokens with the lowest scores.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="o">*</span> <span class="n">percent_to_remove</span><span class="p">)):</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">token_freqs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">sorted_scores</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">total_sum</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">freq</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">token_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
    <span class="n">model</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">freq</span> <span class="o">/</span> <span class="n">total_sum</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">token_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>
</div>
</div>
</div>
<p>To tokenize a sentence, we can apply this function to each word:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">words_with_offsets</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="o">.</span><span class="n">normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pre_tokenized_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">offset</span> <span class="ow">in</span> <span class="n">words_with_offsets</span><span class="p">]</span>
    <span class="n">encoded_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">encode_word</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">model</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">pre_tokenized_text</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">encoded_words</span><span class="p">,</span> <span class="p">[])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;investment opportunities in the company&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;▁investment&#39;, &#39;▁o&#39;, &#39;pport&#39;, &#39;un&#39;, &#39;ities&#39;, &#39;▁in&#39;, &#39;▁the&#39;, &#39;▁company&#39;]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="sentencepiece">
<h2>SentencePiece<a class="headerlink" href="#sentencepiece" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/google/sentencepiece">SentencePiece</a> is a tokenization algorithm for the preprocessing of text that you can use with any of the above models.</p></li>
<li><p>It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, ▁.</p></li>
<li><p>Used in conjunction with the Unigram algorithm, it doesn’t even require a pre-tokenization step, which is very useful for languages where the space character is not used to separate words, such as Chinese, Japanese, and Thai.</p></li>
<li><p>The other main feature of SentencePiece is reversible tokenization: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the _s with spaces — this results in the normalized text.</p></li>
</ul>
<section id="comparisons-with-other-implementations">
<h3>Comparisons with other implementations<a class="headerlink" href="#comparisons-with-other-implementations" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Feature</p></th>
<th class="text-align:center head"><p>SentencePiece</p></th>
<th class="text-align:center head"><p>subword-nmt</p></th>
<th class="text-align:center head"><p>WordPiece</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Supported algorithm</p></td>
<td class="text-align:center"><p>BPE, unigram, char, word</p></td>
<td class="text-align:center"><p>BPE</p></td>
<td class="text-align:center"><p>BPE*</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>OSS?</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Google internal</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Subword regularization</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>No</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Python Library (pip)</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>N/A</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>C++ Library</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>N/A</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Pre-segmentation required?</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Customizable normalization (e.g., NFKC)[Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>N/A</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Direct id generation</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>N/A</p></td>
</tr>
</tbody>
</table>
<p>Note that BPE algorithm used in WordPiece is slightly different from the original BPE.</p>
</section>
</section>
<section id="subword-sampling">
<h2>Subword Sampling<a class="headerlink" href="#subword-sampling" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>The models are trained with multiple subword segmentation based on a unigram language model and those are probabilistically sampled during training.</p></li>
<li><p><span class="math notranslate nohighlight">\(L\)</span>-best segmentation is an approach that can be used for approximate sampling.</p></li>
<li><p>First, the <span class="math notranslate nohighlight">\(l\)</span>-best segmentations are obtained and after performing <span class="math notranslate nohighlight">\(l\)</span>-best search, one segmentation is sampled.</p></li>
<li><p>Subword regularization has two hyperparameters which are the size of sampling candidates (<span class="math notranslate nohighlight">\(l\)</span>) and smoothing constant (<span class="math notranslate nohighlight">\(\alpha\)</span>).</p></li>
<li><p>Theoretically, setting <span class="math notranslate nohighlight">\(l \to \infty\)</span> means considering all possible segmentations.</p></li>
<li><p>But it is infeasible since the number of characters exponentially increases with the length of the sentence.</p></li>
<li><p>Therefore, the Forward-Filtering and Backward-Sampling algorithm is used for sampling.</p></li>
<li><p>Further, if <span class="math notranslate nohighlight">\(\alpha\)</span> is small, the distribution is more uniform and if <span class="math notranslate nohighlight">\(\alpha\)</span> is large, it tends towards the Viterbi segmentation.</p></li>
</ul>
</section>
<section id="bpe-dropout">
<h2>BPE-Dropout<a class="headerlink" href="#bpe-dropout" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>BPE-dropout is an effective subword regularization method based on BPE.</p></li>
<li><p>This keeps the BPE vocabulary and the merge table as original while changing the segmentation procedure.</p></li>
<li><p>Here, some merges are randomly removed with a probability of p at each merge step, thus giving multiple segmentations for the same word.</p></li>
<li><p>If the probability is zero, the subword segmentation is equal to the original BPE.</p></li>
<li><p>If the probability is one, the subword segmentation is equal to character segmentation.</p></li>
<li><p>If the probability is varied from 0 to 1, it gives multiple segmentations with various granularities.</p></li>
<li><p>Since this method exposes the models to various subword segmentation, it gives the ability to have a better understanding of words and subwords.</p></li>
<li><p>BPE-dropout is a simple procedure since training can be done without training any segmentations other than BPE and inference uses the standard BPE.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://medium.com/nerd-for-tech/nlp-tokenization-2fdec7536d17">NLP Tokenization</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/course/chapter6/1?fw=pt">Hugging Face Tokenizers</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/lectures/deep_nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="t5.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">T5: Text-To-Text Transfer Transformer</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="sentencepiece.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">SentencePiece Tokenizer</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Young Joon Lee<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>