{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc871e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Zero Shot, Prompt, and Search Strategies\n",
    "\n",
    "![bloom](../figs/entelecheia_Zero_Shot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3e881",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Zero Shot and Few Shot Learners\n",
    "\n",
    "![prompt](../figs/entelecheia_Prompt_Engineering.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46fccc-6d3d-497d-bf93-197e83b64db9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Large Language Models can show good enough performance for some tasks based on just a few examples.\n",
    "These examples are called `prompts` to a language model.\n",
    "\n",
    "For clarity, we will define a prompting task as one that requires no fine-tuning to the base language model. \n",
    "This is done by inputting some prompts into the language model and asking it to return a response. \n",
    "The model does not see any training data for this task and is expected to generalize from these few examples. \n",
    "\n",
    "Formatting the examples as input is referred to as `prompt engineering` and is a process that comes with some trial and error.\n",
    "The goal of prompt engineering is to take your prompts and format them in a way, so they are easy to input into the model.\n",
    "\n",
    "Language generation based on prompts is a brilliant concept, and it can be done in two ways, mainly - Zero Shot predictions and Few Shot predictions. \n",
    "Zero-Shot prediction is where the model is not trained on any data for that specific task, and Few Shot predictions are where the model is trained on a very few amount of data for that specific task. \n",
    "In both cases, we need some sort of prompt or seed text to get started with so that the model can generate new text from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3564e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Prompting on LLMs\n",
    "\n",
    " All you need to do is input several examples of your prompts into the `generate` function on either `gpt2` or `gpt3`. You don't even have to specify what kind of task it is, just give it some example inputs and let it figure out how to generalize from there! The only parameters required are:\n",
    "\n",
    " - `text`, which contains the text you want to be generated for (this should be your prompt set)  \n",
    " - `length`, which specifies how long you want each generated sequence returned by the model   \n",
    " - `num_return_sequences`, which specifies how many sequences you want to be returned by the model\n",
    "\n",
    "In Zero-shot predictions, you mainly pass prompts which give a task description to the LLM to generate text. For example, for zero-shot summarization, you can present a body of text to the LLM along with an instruction for it to follow, like 'In summary', or 'tldr:', or even 'To explain to a 5-year-old'.\n",
    "\n",
    "In Few-Shot summarization, you can preset a few examples of text & their summary to an LLM. You can then present a text to the model and could expect the summary generated by the model. In other words, you give it a few examples vs. none.\n",
    "Language generation based on prompts is a brilliant concept and it is so much simpler than fine-tuning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e04ef0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Zero-shot \n",
    "\n",
    "The model predicts the answer when provided only a description of the task. \n",
    "No gradient updates are performed on the model. \n",
    "\n",
    "- prompt => Translate English to French: (This is the task description)\n",
    "- Cheese => (this is you prompting the LLM to complete the sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba726bed-e6e5-4698-b8e2-2c8e12026a31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### One-shot\n",
    "\n",
    "In addition to task description, you provide the model with one example of what you are expecting it to produce. \n",
    "\n",
    "- prompt => Translate English to French: (Task description for the model)\n",
    "- Sea Otter => loutre de mer (One example for the model to learn from)\n",
    "- Cheese => (providing a prompt to LLM to follow the lead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76c6c3-fb25-4a33-a078-edfad32319c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Few-shot\n",
    "\n",
    "On addition to task description, the model is provided with a few examples of the task. \n",
    "\n",
    "- prompt => Translate English to French: (Task description for the model)\n",
    "- Sea Otter => loutre de mer (a few examples for the model to learn from)\n",
    "- Plush girafe => girafe poivree\n",
    "- Cheese => (providing a prompt to LLM to follow the lead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e4aefc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilgpt2 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Using pad_token, but it is not set yet.\n",
      "Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'One of the hottest areas of investing in recent years has been ESG',\n",
       " 'labels': ['business', 'politics', 'education'],\n",
       " 'scores': [0.37581318616867065, 0.31898653507232666, 0.3052002489566803]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"distilgpt2\")\n",
    "classifier(\n",
    "    \"One of the hottest areas of investing in recent years has been ESG\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a90025",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Zero Shot Reasoners and Chain-of-Thought\n",
    "\n",
    "The [paper](https://arxiv.org/abs/2205.11916) from University of Tokyo and Google Brain team suggests that LLMs have fundamental zero-shot capabilities in high-level broad cognitive tasks and that these capabilities can be extracted by simple Chain-of-Thought (or CoT) prompting.\n",
    "\n",
    "Another [paper](https://arxiv.org/abs/2201.11903) by Google Brain team has further investigated the CoT prompting. They noted that by generating a chain-of-thought (or a series of intermediate reasoning steps) LLMs significantly improve their ability to perform complex reasoning. Their experiments on three large language models have shown that chain-of-thought prompting improves performance on a range of arithmetic, common sense, and symbolic reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12915a7-c602-4aa9-b906-2e9d00919740",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "One exmaple:\n",
    "\n",
    "- Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis  balls. How many tennis balls does he have now? \n",
    "\n",
    "- A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Additionally: \n",
    "\n",
    "- Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have\n",
    "\n",
    "- A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.\n",
    "\n",
    "> Chain of thought reasoning allows models to decompose complex problems into intermediate steps that are solved individually. Moreover, the language-based nature of chain of thought makes it applicable to any task that a person could solve via language. We find through empirical experiments that chain of thought prompting can improve performance on various reasoning tasks, and that successful chain of thought reasoning is an emergent property of model scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e5692",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Decoding / search strategies\n",
    "\n",
    "[How to generate text](https://huggingface.co/blog/how-to-generate) - using different decoding methods for language generation with Transformers\n",
    "by Patrick von Platen (Huggingface)\n",
    "\n",
    "- In recent years, there has been an increasing interest in open-ended language generation thanks to the rise of LLMs trained on millions of documents, such as GPT2, XLNet, OpenAi-GPT, CTRL, TransfoXL, XLM, Bart, T5, GPT3, and BLOOM. \n",
    "\n",
    "- Such models have achieved promising results on several generation tasks, including open-ended dialogue, summarization, and story generation.\n",
    "\n",
    "- For these models, better decoding methods have played an important role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d761687-8144-42a9-95b2-79a79287b006",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Auto-regressive language generation is based on the assumption that the text being generated can be decomposed into a sequence of subparts.\n",
    "Each part is dependent on the previous parts, thus we can use an auto-regressive decoder to generate text one token at a time based on its predecessors.\n",
    "\n",
    "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
    "\n",
    "> $W_0$ being the initial *context* word sequence. The length $T$ of the word sequence is usually determined *on-the-fly* and corresponds to the timestep $t=T$ the EOS token is generated from $P(w_{t} | w_{1: t-1}, W_{0})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdc2a9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 10:30:49.576146: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 147.24MiB (rounded to 154389504)requested by op TruncatedNormal\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-08-31 10:30:49.576340: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *__________________________******************************************_______________________________\n",
      "2022-08-31 10:30:49.576391: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at random_op.cc:74 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[50257,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"transformer\" (type TFGPT2MainLayer).\n\nOOM when allocating tensor with shape[50257,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:TruncatedNormal]\n\nCall arguments received by layer \"transformer\" (type TFGPT2MainLayer):\n  • input_ids=tf.Tensor(shape=(3, 5), dtype=int32)\n  • past=None\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=True\n  • output_attentions=False\n  • output_hidden_states=False\n  • return_dict=True\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# add the EOS token as PAD token to avoid warnings\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:2379\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2377\u001b[0m         model(model\u001b[38;5;241m.\u001b[39mdummy_inputs)  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2379\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdummy_inputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2381\u001b[0m \u001b[38;5;66;03m# 'by_name' allow us to do transfer learning by skipping/adding layers\u001b[39;00m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;66;03m# see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357\u001b[39;00m\n\u001b[1;32m   2383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:413\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    412\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:890\u001b[0m, in \u001b[0;36mTFGPT2LMHeadModel.call\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(GPT2_INPUTS_DOCSTRING)\n\u001b[1;32m    842\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    865\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFCausalLMOutputWithCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m        config.vocab_size - 1]`.\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    907\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(hidden_states, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:413\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    412\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:445\u001b[0m, in \u001b[0;36mTFGPT2MainLayer.call\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    442\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(position_ids, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shape_list(position_ids)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 445\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe, position_ids)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:2556\u001b[0m, in \u001b[0;36mTFSharedEmbeddings.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape):\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2553\u001b[0m \u001b[38;5;124;03m    Build shared token embedding layer Shared weights logic adapted from\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;124;03m    https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_initializer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializer_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mbuild(input_shape)\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"transformer\" (type TFGPT2MainLayer).\n\nOOM when allocating tensor with shape[50257,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:TruncatedNormal]\n\nCall arguments received by layer \"transformer\" (type TFGPT2MainLayer):\n  • input_ids=tf.Tensor(shape=(3, 5), dtype=int32)\n  • past=None\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=True\n  • output_attentions=False\n  • output_hidden_states=False\n  • return_dict=True\n  • training=False"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0232d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Greedy Search\n",
    "\n",
    "![greedy](../figs/entelecheia_greedy_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a31c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Greedy search simply selects the word with the highest probability as its next word: $w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$.\n",
    "\n",
    "![greedy](../figs/deepnlp_2_greedy_search.png)\n",
    "\n",
    "Starting from the word \"The\" the algorithm greedily chooses the next word of highest probability \"nice\" and so on, so\n",
    "that the final generated word sequence is (\"The\", \"nice\", \"woman\") having an overall probability of $0.5 \\times 0.4 = 0.2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7744784",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Generate word sequences using GPT2 on the context (\"I\",\"enjoy\",\"studying\",\"deep\",\"learning\",\"for\",\"natural\", \"language\", \"processing\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0dabdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, but I'm not sure how to apply it to real-world applications.\n",
      "\n",
      "I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not sure how to apply it to real-world applications. I'm not\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy studying deep learning for natural language processing',\n",
    "                             return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=100)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a03e74",
   "metadata": {},
   "source": [
    "- The generated words following the context are reasonable, but the model quickly starts repeating itself! This is a very common problem in language generation in general and seems to be even more so in greedy and beam search.\n",
    "\n",
    "- The major drawback of greedy search though is that it misses high probability words hidden behind a low probability word as can be seen in our sketch above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f06559",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Beam search\n",
    "\n",
    "![beam](../figs/entelecheia_beam_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a98286",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely `num_beams` of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. \n",
    "\n",
    "- Beam search with `num_beams=2`:\n",
    "\n",
    "![beam](../figs/deepnlp_2_greedy_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80c158-0eea-4f5b-99bf-21df2905efb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- At time step 1, besides the most likely hypothesis  (\"The\",\"nice\"), beam search also keeps track of the second\n",
    "most likely one (\"The\",\"dog\"). \n",
    "\n",
    "- At time step 2, beam search finds that the word sequence (\"The\",\"dog\",\"has\"),  has with $0.36$, a higher probability than (\"The\",\"nice\",\"woman\"), which has $0.2$. \n",
    "\n",
    "- It has found the most likely word sequence in our toy example\\!\n",
    "\n",
    "- Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163955d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Set `num_beams > 1` and `early_stopping=True` so that generation is finished when all beam hypotheses reached the EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4be3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "What is Deep Learning?\n",
      "\n",
      "Deep learning is a type of artificial intelligence (AI) that can be applied to real-world applications. Deep learning is a type of artificial intelligence (AI) that can be applied to real-world applications.\n",
      "\n",
      "Deep learning is a type of artificial intelligence (AI) that can be applied to real-world applications\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7f218",
   "metadata": {},
   "source": [
    "- While the result is arguably more fluent, the output still includes repetitions of the same word sequences.\n",
    "- A simple remedy is to introduce *n-grams* penalties as introduced by Paulus et al. (2017) and Klein et al. (2017). \n",
    "- The most common n-grams penalty makes sure that no *n-gram* appears twice by manually setting the probability of next words that could create an already seen *n-gram* to 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693530b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Set `no_repeat_ngram_size=2` so that no 2-gram appears twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3896798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, and I'm excited to see how it can be applied to real-world applications.\n",
      "\n",
      "In this post, I'll show you how you can use Deep Learning to build a neural network that can learn to read and write a sentence. In this article, you'll learn how to use the Deep Neural Network (DNN) to learn a language. You'll also learn about how the DNN works and what you need to do to get started\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54cfb2",
   "metadata": {},
   "source": [
    "- Looks much better! We can see that the repetition does not appear anymore. \n",
    "- Nevertheless, *n-gram* penalties have to be used with care. \n",
    "- An article generated about the city *New York* should not use a *2-gram* penalty or otherwise, the name of the city would only appear once in the whole text!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bfcaa9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- We can compare the top beams after generation and choose the generated beam that fits our purpose best.\n",
    "\n",
    "- Set the parameter `num_return_sequences` to the number of highest scoring beams that should be returned. \n",
    "- Make sure that `num_return_sequences <= num_beams`\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db782d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy studying deep learning for NLP, but I don't think it's the best way to do it.\n",
      "\n",
      "In this post, I'm going to show you how you can use Deep Learning to build a neural network that can be used in a variety of applications. The goal is to create a machine learning system that is able to learn from the input data, and then use that data to train the system on the output data. In this case, we'll be using a simple neural\n",
      "1: I enjoy studying deep learning for NLP, but I don't think it's the best way to do it.\n",
      "\n",
      "In this post, I'm going to show you how you can use Deep Learning to build a neural network that can be used in a variety of applications. The goal is to create a machine learning system that is able to learn from the input data, and then use that data to train the system on the output data. In this case, we'll be using a deep neural\n",
      "2: I enjoy studying deep learning for NLP, but I don't think it's the best way to do it.\n",
      "\n",
      "In this post, I'm going to show you how you can use Deep Learning to build a neural network that can be used in a variety of applications. The goal is to create a machine learning system that is able to learn from the input data, and then use that data to train the system on the output data. In this case, we'll be building a deep neural\n",
      "3: I enjoy studying deep learning for NLP, but I don't think it's the best way to do it.\n",
      "\n",
      "In this post, I'm going to show you how you can use Deep Learning to build a neural network that can be used in a variety of applications. The goal is to create a machine learning system that is able to learn from the input data, and then use that data to train the system on the output data. In this case, we'll be using the neural networks\n",
      "4: I enjoy studying deep learning for NLP, but I don't think it's the best way to do it.\n",
      "\n",
      "In this post, I'm going to show you how you can use Deep Learning to build a neural network that can be used in a variety of applications. The goal is to create a machine learning system that is able to learn from the input data, and then use that data to train the system on the output data. In this case, we'll be using the Deep Neural\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "    print(\"{}: {}\".format(\n",
    "        i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805dd0e",
   "metadata": {},
   "source": [
    "- The five beam hypotheses are only marginally different to each other - which should not be too surprising when using only 5 beams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eefe98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In open-ended generation, a couple of reasons have recently been brought forward why beam search might not be the best possible option:\n",
    "\n",
    "- Beam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization. \n",
    "- But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation.\n",
    "\n",
    "- We have seen that beam search heavily suffers from repetitive generation. \n",
    "- This is especially hard to control with *n-gram*- or other penalties in story generation since finding a good trade-off between forced \"no-repetition\" and repeating cycles of identical *n-grams* requires a lot of finetuning.\n",
    "\n",
    "- High quality human language does not follow a distribution of high probability next words. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b4eeb-9143-4677-8440-3b8a7f8a711b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- In other words, as humans, we want generated text to surprise us and not to be boring/predictable. [(Ari Holtzman et al., 2019)](https://arxiv.org/abs/1904.09751)\n",
    "\n",
    "![beam_vs_human](../figs/deepnlp_2_beam_vs_human.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b450fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Sampling\n",
    "\n",
    "Sampling means randomly picking the next word $w_t$ according to its conditional probability distribution:\n",
    "\n",
    "$$ w_t \\sim P(w|w_{1:t-1}) $$\n",
    "\n",
    "The following graphic visualizes language generation when sampling.\n",
    "\n",
    "![sampling](../figs/deepnlp_2_sampling_search.png)\n",
    "\n",
    "Language generation using sampling is not *deterministic* anymore. \n",
    "The word (\"car\") is sampled from the conditioned probability distribution $P(w | \\text{\"The\"})$, followed by sampling (\"drives\") from $P(w | \\text{\"The\"}, \\text{\"car\"})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd0ac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Set `do_sample=True` and deactivate *Top-K* sampling via `top_k=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19ef99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing in Silicon Valley.\n",
      "\n",
      "Schirmer also runs LLU's Deep Vision Laboratory, a network of advanced computer vision and computational neuroscience labs where he investigates AI system-level optimization.\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "Skye Anderson\n",
      "\n",
      "Ideal minutes\n",
      "\n",
      "Needing access to many deep learning subreddits and\n",
      "\n",
      "trigger warnings (this may require to actually connect your device with one of them to synchronize the\n",
      "\n",
      "connected)\n",
      "\n",
      "Junjuny\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8370c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- The text seems alright - but when taking a closer look, it is not very coherent. \n",
    "- Some words don't sound like they were written by a human. \n",
    "- That is the big problem when sampling word sequences: The models often generate incoherent gibberish.\n",
    "\n",
    "- A trick is to make the distribution $P(w|w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so-called `temperature` of the [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max).\n",
    "\n",
    "![temperature](../figs/deepnlp_2_sampling_search_with_temp.png)\n",
    "\n",
    "- The conditional next word distribution of step t=1 becomes much sharper leaving almost no chance for word (\"car\") to be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f82721",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Cool down the distribution in the library by setting `temperature=0.7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca49dff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, but I also like to explore the applications of them in the human community, especially in the fields of physics, biology, and neuroscience.\n",
      "\n",
      "SL: Your recent blog here at Advanced Deep Learning made me think about the importance of learning from experience and how it can be used to improve our understanding of machine learning. What do you consider to be the most important areas of expertise for Deep Learning that you'd like to see in the future?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=0,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fa1d1",
   "metadata": {},
   "source": [
    "- There are less weird n-grams and the output is a bit more coherent now. \n",
    "- While applying temperature can make a distribution less random, in its limit, when setting `temperature` $\\to 0$, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8db85c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Top-K Sampling\n",
    "\n",
    "\n",
    "![top_k](../figs/deepnlp_2_top_k_sampling.png)\n",
    "\n",
    "In *Top-K* sampling, the *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e94719",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Having set $K = 6$, in both sampling steps we limit our sampling pool to 6 words. \n",
    "- While the 6 most likely words, defined as $V_{\\text{top-K}}$ encompass only two-thirds of the whole\n",
    "probability mass in the first step, it includes almost all of the probability mass in the second step. \n",
    "- Nevertheless, we see that it successfully eliminates the rather weird candidates (\"not\", \"the\", \"small\", \"told\") in the second sampling step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70658b2-fb8a-4ab1-8a97-81e3d4f58364",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's see how *Top-K* can be used in the library by setting `top_k=50`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e639a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing: it is so simple yet powerful in so many ways.\n",
      "\n",
      "What is Deep Learning?\n",
      "\n",
      "Deep learning is an interesting concept; it allows us to analyze data quickly and reliably.\n",
      "\n",
      "So let's say I am interested in the information I want to access. What is the cost of making it happen?\n",
      "\n",
      "Most people assume that you have to know how to access information through your computer.\n",
      "\n",
      "But learning to read computer code is\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bea5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- The text is arguably the most *human-sounding* text so far. \n",
    "- One concern with *Top-K* sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w|w_{1:t-1})$. \n",
    "- This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above).\n",
    "\n",
    "\n",
    "- In step $t=1$, Top-K eliminates the possibility to sample (\"people\",\"big\",\"house\",\"cat\"), which seem like reasonable candidates. \n",
    "- On the other hand, in step $t=2$ the method includes the arguably ill-fitted words (\"down\",\"a\") in the sample pool of words. \n",
    "- Thus, limiting the sample pool to a fixed size $K$ could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution. \n",
    "- This intuition led Ari Holtzman et al. (2019) to create ***Top-p***- or ***nucleus***-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd5b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Top-p (nucleus) sampling\n",
    "\n",
    "![top_p](../figs/deepnlp_2_top_p_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3dae2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Instead of sampling only from the most likely *K* words, in *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. \n",
    "- The probability mass is then redistributed among this set of words. \n",
    "- This way, the size of the set of words (*a.k.a* the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. \n",
    "\n",
    "\n",
    "- Having set $p=0.92$, *Top-p* sampling picks the *minimum* number of words to exceed together $p=92$ of the probability mass, defined as $V_{\\text{top-p}}$. \n",
    "- In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%. \n",
    "- It can be seen that it keeps a wide range of words where the next word is arguably less predictable, *e.g.* $P(w | \\text{\"The''})$, and only a few words when the next word seems more predictable, *e.g.* $P(w | \\text{\"The\"}, \\text{\"car\"})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549d760-3940-4349-822b-88874f096e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Activate *Top-p* sampling by setting `0 < top_p < 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3b41fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy studying deep learning for natural language processing, and also found that we are able to now do natural language processing using external embedding systems. For this reason, I'm asking Google to establish an API and provide a seed of artificial intelligence engines that may be able to play a role in solving the most common questions people come up with in solving complex computers.\n",
      "\n",
      "The way we are processing deep learning, are we interpreting the internal world? Should we present it in purely textual form, or in\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_p=0.92,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e5ff5",
   "metadata": {},
   "source": [
    "Great, that sounds like it could have been written by a human. Well, maybe not quite yet.\n",
    "\n",
    "While in theory, *Top-p* seems more elegant than *Top-K*, both methods work well in practice. \n",
    "*Top-p* can also be used in combination with *Top-K*, which can avoid very low ranked words while allowing for some\n",
    "dynamic selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9bf73-78a9-44ee-9859-d45ba1977d55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Finally, to get multiple independently sampled outputs, we can *again* set the parameter `num_return_sequences > 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aafe1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy studying deep learning for natural language processing and the computational power of deep learning algorithms.\n",
      "\n",
      "For other topics, check out this site.\n",
      "1: I enjoy studying deep learning for natural language processing. I really enjoy learning programming languages that you can make into real-time programs. And I love being able to do some amazing things with the Raspberry Pi, and this also comes from my fascination with how computers work and communicate. I also love having a good time while I write and play on the internet as well. My favourite hobby so far is working on things with computers I can manipulate, but I really love making things out of materials.\n",
      "\n",
      "\n",
      "2: I enjoy studying deep learning for natural language processing. I enjoy writing. I enjoy learning. I like to learn new words for fun. I enjoy teaching. I enjoy learning. I want to have my words delivered in new ways. I want to learn new words for the next time I write.\n",
      "\n",
      "I like to write. I enjoy learning. I enjoy learning. I like to learn new words for the next time I write. I love reading books and listening to music. I like to read\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(\n",
    "        i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b7002",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Summary of decoding / search strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9fb0b",
   "metadata": {},
   "source": [
    "As *ad-hoc* decoding methods, *top-p* and *top-K* sampling seem to produce more fluent text than traditional *greedy* - and *beam* search on open-ended language generation. Recently, there has been more evidence though that the apparent flaws of *greedy* and *beam* search - mainly generating repetitive word sequences - are caused by the model (especially the way the model is trained), rather than the decoding method, *cf.* [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). Also, as demonstrated in [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492), it looks as *top-K* and *top-p* sampling also suffer from generating repetitive word sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010d4e7-3d9c-4980-80d3-95bea9cbba60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Greedy Search \n",
    "  - simply chooses the next word at each timestep t+1 that has the highest predicted probability of following the word at t. \n",
    "  - One of the main issues here is that greedy search will miss words with a high probability at t+1 if it is preceded by a word with a low probability at t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb28a24-ea98-4a1d-a70a-f445648e3d08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Beam Search \n",
    "  - keeps track of the n-th (num_beams) most likely word sequences and outputs the most likely sequence. \n",
    "  - Sounds great, but this method breaks down when the output length can be highly variable — as in the case of open-ended text generation. \n",
    "  - Both greedy and beam search also produce outputs whose distribution does not align very well with the way humans might perform the same task (i.e. both are liable to produce fairly repetitive, boring text).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b20ed6-1e2e-4eae-abaa-47da7af1d4d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Sampling With Top-k + Top-p\n",
    "  - a combination of three methods. \n",
    "  - By sampling, we mean that the next word is chosen randomly based on its conditional probability distribution (von Platen, 2020). \n",
    "  - In Top-k, we choose the k most likely words, and then redistribute the probability mass amongst them before the next draw. \n",
    "  - Top-p adds an additional constraint to top-k, in that we’re choosing from the smallest set of words whose cumulative probability exceed p.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a12f491",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Prompt Engineering: The Career of Future\n",
    "\n",
    "![prompt](../figs/deepnlp_2_prompt.png)\n",
    "(source: https://twitter.com/karpathy/status/1273788774422441984/photo/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a132fd0-c774-49f6-b671-43f8187bc4d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "> With the No-Code revolution around the corner, and the coming of new-age technologies like GPT-3 we may see a stark difference between the career of today and the careers of tomorrow…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4439c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "As a rule of thumb while designing the training prompt you should aim towards getting a zero-shot response from the model, if that isn’t possible move forward with few examples rather than providing it with an entire corpus. The standard flow for training prompt design should look like: Zero-Shot → Few Shots → Corpus-based Priming.\n",
    "\n",
    "- Step 1: Define the problem you are trying to solve and bucket it into one of the possible natural language tasks classification, Q & A, text generation, creative writing, etc.\n",
    "- Step 2: Ask yourself if there is a way to get a solution with zero-shot (i.e. without priming the GPT-3 model with any external training examples)\n",
    "- Step 3: If you think that you need external examples to prime the model for your use case, go back to step-2 and think really hard.\n",
    "- Step 4: Now think of how you might encounter the problem in a textual fashion given the “text-in, text-out” interface of GPT-3. Think about all the possible scenarios to represent your problem in textual form.\n",
    "- Step 5: If you end up using the external examples, use as few as possible and try to include variety in your examples without essentially overfitting the model or skewing the predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f869af7787e6a1c49e09e367fc6e1b81d93d1c6583b43249c80edc047bd13cb2"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
