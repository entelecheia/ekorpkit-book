
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Topic Models &#8212; eKorpkit Tutorials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/ekorpkit.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Deep Learning for NLP" href="../deep_nlp/index.html" />
    <link rel="prev" title="Topic Modeling" href="topic.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FN4GFT8HP8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FN4GFT8HP8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/ekorpkit.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">eKorpkit Tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    eKonomic Research Python Toolkit
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/index.html">
   Getting started
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/install.html">
     Installation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/usage.html">
     Usage
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basics/features/index.html">
   Key features
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/easy_config.html">
     Easy Configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/no_boiler.html">
     No Boilerplate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/workflows.html">
     Workflows
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/share.html">
     Sharable and Reproducible
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basics/features/plug.html">
     Pluggable Architece
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/config/index.html">
   Configuring ekorpkit
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/config/eKonf.html">
     Using eKonf class
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/datasets/index.html">
   Datasets
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/corpus.html">
     Build and Load Corpora
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/datasets/dataset.html">
     Build and Load Datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/corpora/pipeline.html">
     Corpus task pipelines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/models/index.html">
   Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/automl/index.html">
     Auto ML
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/embeddings/index.html">
     Word Embeddings
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/word2vec_basics.html">
       Word2Vec Basics
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/embeddings/eval_vectors.html">
       Evaluate pretrained embeddings
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/ngram/index.html">
     N-Grams
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram.html">
       N-Grams
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_lexicons.html">
       N-Gram model for ngram lexicon features
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/ngram/ngram_for_words.html">
       N-Gram model for unigram lexicon features
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/sentiment/index.html">
     Sentiment Analyers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/lbsa_en.html">
       Lexicon-based Sentiment Analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/financial_phrasebank_lm.html">
       Evaluate LM with financial phrasebank
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/sentiment/sentiment.html">
       LM Dictionary vs. finbert vs. T5
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/models/transformers/index.html">
     Transformers
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/models/art/index.html">
     AI Art (Text-to-Image)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/disco.html">
       Disco Diffusion
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/models/art/dalle-mini.html">
       DALL·E Mini
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/pipelines/index.html">
   Pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/pipelines/pipeline.html">
     Instantiating pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/preprocessors/index.html">
   Preprocessors
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/normalizer/index.html">
     Normalizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/normalizer/normalizer.html">
       Normalizers
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/segmenter/index.html">
     Segmenters
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/segmenter/segmenter.html">
       Segmenters
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/index.html">
     Tokenizers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
    <label for="toctree-checkbox-14">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/preprocessors/tokenizer/mecab.html">
       Mecab Tokenizer
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/visualizers/index.html">
   Visualizers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/visualizers/plot.html">
     Plots
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/workflow/index.html">
   Workflows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../corpus/ekorpkot_corpus.html">
   The eKorpkit Corpus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Use Cases
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/fomc/index.html">
   FOMC
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/02_FOMC_numerical_data.html">
     Preparing Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/03_FOMC_corpus.html">
     Preparing Textual Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/04_FOMC_EDA_numericals.html">
     EDA on Numerical Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/05_FOMC_features.html">
     Visualizing Features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/06_FOMC_AutoML.html">
     Checking Baseline with AutoML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/07_FOMC_sentiments.html">
     Predicting Sentiments of FOMC Corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/08_FOMC_EDA_sentiments.html">
     EDA on Sentiment Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/fomc/09_FOMC_AutoML_with_tones.html">
     Predicting the next decisions with tones
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../usecases/bok/index.html">
   Bank of Korea
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/edgar/index.html">
   EDGAR
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/edgar/predict_edgar.html">
     Prediciting Sentiments
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg/index.html">
   ESG
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/snorkel-polarity.html">
     Preparing polarity classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/rubrix.html">
     Improving classification datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/classifiers.html">
     Training Classifiers for ESG Ratings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/econ_news_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       econ_news_kr
      </span>
     </code>
     corpus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg/predict_news.html">
     Predicting ESG Categories and Polarities
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/esg-en/index.html">
   ESG (English)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/esg-en/esg_corpus.html">
     Building
     <code class="docutils literal notranslate">
      <span class="pre">
       JOCo
      </span>
     </code>
     corpus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../usecases/cointax/index.html">
   Taxation on Cryptocurrency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../usecases/cointax/coin_dataset.html">
     Improving classification datasets
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/analyst.html">
   ESG Topic Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../research/esg_topics/rethink_esg.html">
   Rethinking ESG
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Introduction to NLP
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="research.html">
     Research Applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="topic.html">
     Topic Modeling
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Topic Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../deep_nlp/index.html">
   Deep Learning for NLP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/ekorpkit.html">
     Getting started with ekorpkit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/zeroshot.html">
     Zero Shot, Prompt, and Search Strategies
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/bloom.html">
     What is BLOOM?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/bloom_apps.html">
     Bloom Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep_nlp/transformers.html">
     Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../aiart/index.html">
   AI Art (Text-to-Image)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
  <label for="toctree-checkbox-23">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/project-themes.html">
     Project Themes - A Brave New World
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../aiart/dalle1.html">
     DALL·E 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ds/index.html">
   Data Science for Economics and Finance
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/index.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../about/cite.html">
   Citation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/entelecheia/ekorpkit-book/main?urlpath=tree/ekorpkit-book/docs/lectures/intro_nlp/topic_models.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/entelecheia/ekorpkit-book/blob/main/ekorpkit-book/docs/lectures/intro_nlp/topic_models.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/entelecheia/ekorpkit-book/issues/new?title=Issue%20on%20page%20%2Fdocs/lectures/intro_nlp/topic_models.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/docs/lectures/intro_nlp/topic_models.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-tomotopy">
   What is tomotopy?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-started">
   Getting Started
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-of-tomotopy">
   Performance of tomotopy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-save-and-load">
   Model Save and Load
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#documents-in-the-model-and-out-of-the-model">
   Documents in the Model and out of the Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-for-unseen-documents">
   Inference for Unseen Documents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#corpus-and-transform">
   Corpus and transform
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parallel-sampling-algorithms">
   Parallel Sampling Algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pining-topics-using-word-priors">
   Pining Topics using Word Priors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples">
   Examples
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#install-or-upgrade-of-ekorpkit">
     Install or upgrade of ekorpkit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-a-dataset">
     Load a dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda-basics">
     LDA Basics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda-visualization">
     LDA Visualization
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Topic Models </h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-tomotopy">
   What is tomotopy?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-started">
   Getting Started
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-of-tomotopy">
   Performance of tomotopy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-save-and-load">
   Model Save and Load
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#documents-in-the-model-and-out-of-the-model">
   Documents in the Model and out of the Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-for-unseen-documents">
   Inference for Unseen Documents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#corpus-and-transform">
   Corpus and transform
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parallel-sampling-algorithms">
   Parallel Sampling Algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pining-topics-using-word-priors">
   Pining Topics using Word Priors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples">
   Examples
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#install-or-upgrade-of-ekorpkit">
     Install or upgrade of ekorpkit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-a-dataset">
     Load a dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda-basics">
     LDA Basics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda-visualization">
     LDA Visualization
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="topic-models-jupyter-book-badge">
<h1>Topic Models <a class="reference external" href="https://entelecheia.github.io/ekorpkit-book/"><img alt="Jupyter Book Badge" src="https://jupyterbook.org/badge.svg" /></a><a class="headerlink" href="#topic-models-jupyter-book-badge" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://github.com/bab2min/tomotopy"><img alt="tomoto" src="../../../_images/tomoto.png" /></a></p>
<p>Package tomotopy <span id="id1">[<a class="reference internal" href="../../about/index.html#id13" title="Minchul Lee. Bab2min/tomotopy: 0.12.3. July 2022. URL: https://doi.org/10.5281/zenodo.6868418, doi:10.5281/zenodo.6868418.">Lee, 2022</a>]</span></p>
<section id="what-is-tomotopy">
<h2>What is tomotopy?<a class="headerlink" href="#what-is-tomotopy" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> is a Python extension of <code class="docutils literal notranslate"><span class="pre">tomoto</span></code> (Topic Modeling Tool) which is a Gibbs-sampling based topic model library written in C++.
The current version of <code class="docutils literal notranslate"><span class="pre">tomoto</span></code> supports several major topic models including</p>
<ul class="simple">
<li><p>Latent Dirichlet Allocation (<code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel</span></code>)</p></li>
<li><p>Labeled LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.LLDAModel</span></code>)</p></li>
<li><p>Partially Labeled LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.PLDAModel</span></code>)</p></li>
<li><p>Supervised LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.SLDAModel</span></code>)</p></li>
<li><p>Dirichlet Multinomial Regression (<code class="docutils literal notranslate"><span class="pre">tomotopy.DMRModel</span></code>)</p></li>
<li><p>Generalized Dirichlet Multinomial Regression (<code class="docutils literal notranslate"><span class="pre">tomotopy.GDMRModel</span></code>)</p></li>
<li><p>Hierarchical Dirichlet Process (<code class="docutils literal notranslate"><span class="pre">tomotopy.HDPModel</span></code>)</p></li>
<li><p>Hierarchical LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.HLDAModel</span></code>)</p></li>
<li><p>Multi Grain LDA (<code class="docutils literal notranslate"><span class="pre">tomotopy.MGLDAModel</span></code>)</p></li>
<li><p>Pachinko Allocation (<code class="docutils literal notranslate"><span class="pre">tomotopy.PAModel</span></code>)</p></li>
<li><p>Hierarchical PA (<code class="docutils literal notranslate"><span class="pre">tomotopy.HPAModel</span></code>)</p></li>
<li><p>Correlated Topic Model (<code class="docutils literal notranslate"><span class="pre">tomotopy.CTModel</span></code>)</p></li>
<li><p>Dynamic Topic Model (<code class="docutils literal notranslate"><span class="pre">tomotopy.DTModel</span></code>)</p></li>
<li><p>Pseudo-document based Topic Model (<code class="docutils literal notranslate"><span class="pre">tomotopy.PTModel</span></code>).</p></li>
</ul>
</section>
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">#</a></h2>
<p>You can install tomotopy easily using pip. (<a class="reference external" href="https://pypi.org/project/tomotopy/">https://pypi.org/project/tomotopy/</a>)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install --upgrade pip
pip install tomotopy
</pre></div>
</div>
<p>The supported OS and Python versions are:</p>
<ul class="simple">
<li><p>Linux (x86-64) with Python &gt;= 3.6</p></li>
<li><p>macOS &gt;= 10.13 with Python &gt;= 3.6</p></li>
<li><p>Windows 7 or later (x86, x86-64) with Python &gt;= 3.6</p></li>
<li><p>Other OS with Python &gt;= 3.6: Compilation from source code required (with c++14 compatible compiler)</p></li>
</ul>
<p>After installing, you can start tomotopy by just importing.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tp</span><span class="o">.</span><span class="n">isa</span><span class="p">)</span> <span class="c1"># prints &#39;avx2&#39;, &#39;avx&#39;, &#39;sse2&#39; or &#39;none&#39;</span>
</pre></div>
</div>
<p>Currently, tomotopy can exploits AVX2, AVX or SSE2 SIMD instruction set for maximizing performance.
When the package is imported, it will check available instruction sets and select the best option.
If <code class="docutils literal notranslate"><span class="pre">tp.isa</span></code> tells <code class="docutils literal notranslate"><span class="pre">none</span></code>, iterations of training may take a long time.
But, since most of modern Intel or AMD CPUs provide SIMD instruction set, the SIMD acceleration could show a big improvement.</p>
<p>Here is a sample code for simple LDA training of texts from ‘sample.txt’ file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;sample.txt&#39;</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{}</span><span class="se">\t</span><span class="s1">Log-likelihood: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top 10 words of topic #</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="n">mdl</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="performance-of-tomotopy">
<h2>Performance of tomotopy<a class="headerlink" href="#performance-of-tomotopy" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> uses Collapsed Gibbs-Sampling(CGS) to infer the distribution of topics and the distribution of words.
Generally CGS converges more slowly than Variational Bayes(VB) that <a class="reference external" href="https://radimrehurek.com/gensim/models/ldamodel.html"><code class="docutils literal notranslate"><span class="pre">gensim's</span> <span class="pre">LdaModel</span></code></a> uses, but its iteration can be computed much faster.
In addition, <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> can take advantage of multicore CPUs with a SIMD instruction set, which can result in faster iterations.</p>
<p>Following chart shows the comparison of LDA model’s running time between <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> and <code class="docutils literal notranslate"><span class="pre">gensim</span></code>.
The input data consists of 1000 random documents from English Wikipedia with 1,506,966 words (about 10.1 MB).
<code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> trains 200 iterations and <code class="docutils literal notranslate"><span class="pre">gensim</span></code> trains 10 iterations.</p>
<p><img alt="" src="../../../_images/tmt_i5.png" /></p>
</section>
<section id="model-save-and-load">
<h2>Model Save and Load<a class="headerlink" href="#model-save-and-load" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> provides <code class="docutils literal notranslate"><span class="pre">save</span></code> and <code class="docutils literal notranslate"><span class="pre">load</span></code> method for each topic model class,
so you can save the model into the file whenever you want, and re-load it from the file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>

<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">HDPModel</span><span class="p">()</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;sample.txt&#39;</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{}</span><span class="se">\t</span><span class="s1">Log-likelihood: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>

<span class="c1"># save into file</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;sample_hdp_model.bin&#39;</span><span class="p">)</span>

<span class="c1"># load from file</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">HDPModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;sample_hdp_model.bin&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">mdl</span><span class="o">.</span><span class="n">is_live_topic</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="k">continue</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top 10 words of topic #</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># the saved model is HDP model, </span>
<span class="c1"># so when you load it by LDA model, it will raise an exception</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;sample_hdp_model.bin&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>When you load the model from a file, a model type in the file should match the class of methods.</p>
</section>
<section id="documents-in-the-model-and-out-of-the-model">
<h2>Documents in the Model and out of the Model<a class="headerlink" href="#documents-in-the-model-and-out-of-the-model" title="Permalink to this headline">#</a></h2>
<p>We can use Topic Model for two major purposes.
The basic one is to discover topics from a set of documents as a result of trained model,
and the more advanced one is to infer topic distributions for unseen documents by using trained model.</p>
<p>We named the document in the former purpose (used for model training) as <strong>document in the model</strong>,
and the document in the later purpose (unseen document during training) as <strong>document out of the model</strong>.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code>, these two different kinds of document are generated differently.
A <strong>document in the model</strong> can be created by <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.add_doc</span></code> method.
<code class="docutils literal notranslate"><span class="pre">add_doc</span></code> can be called before <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.train</span></code> starts.
In other words, after <code class="docutils literal notranslate"><span class="pre">train</span></code> called, <code class="docutils literal notranslate"><span class="pre">add_doc</span></code> cannot add a document into the model because the set of document used for training has become fixed.</p>
<p>To acquire the instance of the created document, you should use <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.docs</span></code> like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span> <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Failed to add doc&quot;</span><span class="p">)</span>
<span class="n">doc_inst</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="c1"># doc_inst is an instance of the added document</span>
</pre></div>
</div>
<p>A <strong>document out of the model</strong> is generated by <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.make_doc</span></code> method. <code class="docutils literal notranslate"><span class="pre">make_doc</span></code> can be called only after <code class="docutils literal notranslate"><span class="pre">train</span></code> starts.
If you use <code class="docutils literal notranslate"><span class="pre">make_doc</span></code> before the set of document used for training has become fixed, you may get wrong results.
Since <code class="docutils literal notranslate"><span class="pre">make_doc</span></code> returns the instance directly, you can use its return value for other manipulations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># add_doc ...</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">doc_inst</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">make_doc</span><span class="p">(</span><span class="n">unseen_doc</span><span class="p">)</span> <span class="c1"># doc_inst is an instance of the unseen document</span>
</pre></div>
</div>
</section>
<section id="inference-for-unseen-documents">
<h2>Inference for Unseen Documents<a class="headerlink" href="#inference-for-unseen-documents" title="Permalink to this headline">#</a></h2>
<p>If a new document is created by <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.make_doc</span></code>, its topic distribution can be inferred by the model.
Inference for unseen document should be performed using <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.infer</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># add_doc ...</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">doc_inst</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">make_doc</span><span class="p">(</span><span class="n">unseen_doc</span><span class="p">)</span>
<span class="n">topic_dist</span><span class="p">,</span> <span class="n">ll</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">doc_inst</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Topic Distribution for Unseen Docs: &quot;</span><span class="p">,</span> <span class="n">topic_dist</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Log-likelihood of inference: &quot;</span><span class="p">,</span> <span class="n">ll</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">infer</span></code> method can infer only one instance of <code class="docutils literal notranslate"><span class="pre">tomotopy.Document</span></code> or a <code class="docutils literal notranslate"><span class="pre">list</span></code> of instances of <code class="docutils literal notranslate"><span class="pre">tomotopy.Document</span></code>.
See more at <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.infer</span></code>.</p>
</section>
<section id="corpus-and-transform">
<h2>Corpus and transform<a class="headerlink" href="#corpus-and-transform" title="Permalink to this headline">#</a></h2>
<p>Every topic model in <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> has its own internal document type.
A document can be created and added into suitable for each model through each model’s <code class="docutils literal notranslate"><span class="pre">add_doc</span></code> method.
However, trying to add the same list of documents to different models becomes quite inconvenient,
because <code class="docutils literal notranslate"><span class="pre">add_doc</span></code> should be called for the same list of documents to each different model.
Thus, <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> provides <code class="docutils literal notranslate"><span class="pre">tomotopy.utils.Corpus</span></code> class that holds a list of documents.
<code class="docutils literal notranslate"><span class="pre">tomotopy.utils.Corpus</span></code> can be inserted into any model by passing as argument <code class="docutils literal notranslate"><span class="pre">corpus</span></code> to <code class="docutils literal notranslate"><span class="pre">__init__</span></code> or <code class="docutils literal notranslate"><span class="pre">add_corpus</span></code> method of each model.
So, inserting <code class="docutils literal notranslate"><span class="pre">tomotopy.utils.Corpus</span></code> just has the same effect to inserting documents the corpus holds.</p>
<p>Some topic models requires different data for its documents.
For example, <code class="docutils literal notranslate"><span class="pre">tomotopy.DMRModel</span></code> requires argument <code class="docutils literal notranslate"><span class="pre">metadata</span></code> in <code class="docutils literal notranslate"><span class="pre">str</span></code> type,
but <code class="docutils literal notranslate"><span class="pre">tomotopy.PLDAModel</span></code> requires argument <code class="docutils literal notranslate"><span class="pre">labels</span></code> in <code class="docutils literal notranslate"><span class="pre">List[str]</span></code> type.
Since <code class="docutils literal notranslate"><span class="pre">tomotopy.utils.Corpus</span></code> holds an independent set of documents rather than being tied to a specific topic model,
data types required by a topic model may be inconsistent when a corpus is added into that topic model.
In this case, miscellaneous data can be transformed to be fitted target topic model using argument <code class="docutils literal notranslate"><span class="pre">transform</span></code>.</p>
<p>See more details in the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tomotopy</span> <span class="kn">import</span> <span class="n">DMRModel</span>
<span class="kn">from</span> <span class="nn">tomotopy.utils</span> <span class="kn">import</span> <span class="n">Corpus</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">Corpus</span><span class="p">()</span>
<span class="n">corpus</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="s2">&quot;a b c d e&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">a_data</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">corpus</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="s2">&quot;e f g h i&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">a_data</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">corpus</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="s2">&quot;i j k l m&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">a_data</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DMRModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span> 
<span class="c1"># You lose `a_data` field in `corpus`, </span>
<span class="c1"># and `metadata` that `DMRModel` requires is filled with the default value, empty str.</span>

<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span>

<span class="k">def</span> <span class="nf">transform_a_data_to_metadata</span><span class="p">(</span><span class="n">misc</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;metadata&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">misc</span><span class="p">[</span><span class="s1">&#39;a_data&#39;</span><span class="p">])}</span>
<span class="c1"># this function transforms `a_data` to `metadata`</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DMRModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_a_data_to_metadata</span><span class="p">)</span>
<span class="c1"># Now docs in `model` has non-default `metadata`, that generated from `a_data` field.</span>

<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;2&#39;</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="o">==</span> <span class="s1">&#39;3&#39;</span>
</pre></div>
</div>
</section>
<section id="parallel-sampling-algorithms">
<h2>Parallel Sampling Algorithms<a class="headerlink" href="#parallel-sampling-algorithms" title="Permalink to this headline">#</a></h2>
<p>Since version 0.5.0, <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> allows you to choose a parallelism algorithm.
The algorithm provided in versions prior to 0.4.2 is <code class="docutils literal notranslate"><span class="pre">COPY_MERGE</span></code>, which is provided for all topic models.
The new algorithm <code class="docutils literal notranslate"><span class="pre">PARTITION</span></code>, available since 0.5.0, makes training generally faster and more memory-efficient, but it is available at not all topic models.</p>
<p>The following chart shows the speed difference between the two algorithms based on the number of topics.</p>
<p><img alt="" src="../../../_images/algo_comp.png" /></p>
</section>
<section id="pining-topics-using-word-priors">
<h2>Pining Topics using Word Priors<a class="headerlink" href="#pining-topics-using-word-priors" title="Permalink to this headline">#</a></h2>
<p>Since version 0.6.0, a new method <code class="docutils literal notranslate"><span class="pre">tomotopy.LDAModel.set_word_prior</span></code> has been added. It allows you to control word prior for each topic.
For example, we can set the weight of the word ‘church’ to 1.0 in topic 0, and the weight to 0.1 in the rest of the topics by following codes.
This means that the probability that the word ‘church’ is assigned to topic 0 is 10 times higher than the probability of being assigned to another topic.
Therefore, most of ‘church’ is assigned to topic 0, so topic 0 contains many words related to ‘church’.
This allows to manipulate some topics to be placed at a specific topic number.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># add documents into `mdl`</span>

<span class="c1"># setting word prior</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">set_word_prior</span><span class="p">(</span><span class="s1">&#39;church&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)])</span>
</pre></div>
</div>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">#</a></h2>
<section id="install-or-upgrade-of-ekorpkit">
<h3>Install or upgrade of ekorpkit<a class="headerlink" href="#install-or-upgrade-of-ekorpkit" title="Permalink to this headline">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Install ekorpkit package first.</p>
<p>Set logging level to Warning, if you don’t want to see verbose logging.</p>
<p>If you run this notebook in Colab, set Hardware accelerator to GPU.</p>
</div>
<div class="toggle docutils container">
<p>!pip install -U –pre ekorpkit[topic]</p>
<p>exit()</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ekorpkit</span> <span class="kn">import</span> <span class="n">eKonf</span>

<span class="n">eKonf</span><span class="o">.</span><span class="n">setLogger</span><span class="p">(</span><span class="s2">&quot;WARNING&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;version:&quot;</span><span class="p">,</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;is notebook?&quot;</span><span class="p">,</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">is_notebook</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;is colab?&quot;</span><span class="p">,</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">is_colab</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;environment variables:&quot;</span><span class="p">)</span>
<span class="n">eKonf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">eKonf</span><span class="o">.</span><span class="n">env</span><span class="p">()</span><span class="o">.</span><span class="n">dict</span><span class="p">())</span>

<span class="n">data_dir</span> <span class="o">=</span> <span class="s2">&quot;../data/topic_models&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>version: 0.1.39+5.g6b58da9
is notebook? True
is colab? False
environment variables:
{&#39;CUDA_DEVICE_ORDER&#39;: None,
 &#39;CUDA_VISIBLE_DEVICES&#39;: None,
 &#39;EKORPKIT_CONFIG_DIR&#39;: &#39;/workspace/projects/ekorpkit-book/config&#39;,
 &#39;EKORPKIT_DATA_DIR&#39;: None,
 &#39;EKORPKIT_LOG_LEVEL&#39;: &#39;WARNING&#39;,
 &#39;EKORPKIT_PROJECT&#39;: &#39;ekorpkit-book&#39;,
 &#39;EKORPKIT_WORKSPACE_ROOT&#39;: &#39;/workspace&#39;,
 &#39;KMP_DUPLICATE_LIB_OK&#39;: &#39;TRUE&#39;,
 &#39;NUM_WORKERS&#39;: 230}
</pre></div>
</div>
</div>
</div>
</section>
<section id="load-a-dataset">
<h3>Load a dataset<a class="headerlink" href="#load-a-dataset" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cfg</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="s1">&#39;path&#39;</span><span class="p">)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">uri</span> <span class="o">=</span> <span class="s1">&#39;https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/us_equities_news_sampled.zip&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;us_equities_news_sampled.parquet&quot;</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">cached_path</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Investing com Asian stock markets were broadly lower for a second day on Thursday as weak U S data on durable goods orders added to concerns over the global growth outlook while concerns over declining corporate profits also weighed During late Asian trade Hong Kong s Hang Seng Index tumbled 1 55 Australia s ASX 200 Index dipped 0 1 while Japan s Nikkei 225 Index shed 0 7 The Nikkei came further off a one year closing high hit earlier in the week as investors cashed in ahead of the Japanese fiscal year end March is the final month of Japan s fiscal year and market participants have expected many funds to lock in profits from a meteoric 19 rally in the January to March period after shedding more than 13 in April to December Exporters which have gained sharply in the first quarter on the back a weakening yen declined Automakers Toyota and Nissan slumped 1 65 and 1 8 respectively while consumer electronics giant Sony retreated 1 5 On the upside Sharp saw shares jump 6 7 extending the previous day s 15 rally following reports that Taiwan s Hon Hai Precision Industry is buying 10 of the Japanese electronics manufacturer for JPY66 91 billion with the two to form a tie up in liquid crystal display production Elsewhere shares in Hong Kong came under pressure amid lingering fears over a hard landing in China and worries over declining corporate profits PICC Property Casualty China s biggest non life insurer saw shares drop 4 after reporting 2011 net income rose to CNY8 03 billion missing expectations for income of CNY8 8 billion China Shipping Container Lines fell 1 55 after the nation s second largest container carrier reported a loss of CNY2 74 billion last year wider than the average estimate of CNY2 63 billion amid rising fuel costs and declining demand Port operator China Merchants Holdings declined 1 7 after saying annual profit dropped 5 2 due to costs from a harbor deal Hong Kong traded Chinese banks were also broadly weaker with Industrial and Commercial Bank of China and Bank of China down 2 4 and 2 3 respectively ahead of their 2011 earnings reports later on Thursday Raw material producers also contributed to losses amid concerns over the global growth outlook Copper mining giant Jiangxi Copper Company dropped 3 4 Aluminum Corporation of China or CHALCO slumped 2 4 while shares in oil majors PetroChina and CNOOC fell 2 1 and 3 8 respectively But shares in Australia outperformed regional equities for a second day remaining close to a four month high The index was weighed by a 7 decline in Leighton Holdings the country s largest construction company after saying its underlying profit in the year ending December 31 will be between AUD400 million and AUD450 million below market expectations citing increased costs due to wet weather and lower than expected productivity Meanwhile European stock markets were mildly lower after the open supported by hopes that euro zone leaders will increase the size of the European debt firewall to combat the fiscal crisis The EURO STOXX 50 shed 0 3 France s CAC 40 dipped 0 1 Germany s DAX fell 0 25 while London s FTSE 100 edged 0 15 lower Later in the day Germany was to publish official data on employment change while the U S was to release official data on initial jobless claims&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="lda-basics">
<h3>LDA Basics<a class="headerlink" href="#lda-basics" title="Permalink to this headline">#</a></h3>
<p>LDA class provides Latent Dirichlet Allocation(LDA) topic model and its implementation is based on following papers:</p>
<ul class="simple">
<li><p>Blei, D.M., Ng, A.Y., &amp;Jordan, M.I. (2003).Latent dirichlet allocation.Journal of machine Learning research, 3(Jan), 993 - 1022.</p></li>
<li><p>Newman, D., Asuncion, A., Smyth, P., &amp;Welling, M. (2009).Distributed algorithms for topic models.Journal of Machine Learning Research, 10(Aug), 1801 - 1828.</p></li>
</ul>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>

<span class="n">save_path</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">join_path</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s2">&quot;lda_basic.mdl&quot;</span><span class="p">)</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">tw</span><span class="o">=</span><span class="n">tp</span><span class="o">.</span><span class="n">TermWeight</span><span class="o">.</span><span class="n">ONE</span><span class="p">,</span> <span class="n">min_cf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rm_top</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">][:</span><span class="mi">1000</span><span class="p">]):</span>
    <span class="n">ch</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">ch</span><span class="p">)</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">burn_in</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Num docs:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">),</span> <span class="s1">&#39;, Vocab size:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">),</span> <span class="s1">&#39;, Num words:&#39;</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">num_words</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Removed top words:&#39;</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">removed_top_words</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{}</span><span class="se">\t</span><span class="s1">Log-likelihood: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>

<span class="n">mdl</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Num docs: 1000 , Vocab size: 5055 , Num words: 404158
Removed top words: [&#39;the&#39;, &#39;to&#39;, &#39;of&#39;, &#39;and&#39;, &#39;in&#39;, &#39;a&#39;, &#39;s&#39;, &#39;is&#39;, &#39;for&#39;, &#39;on&#39;]
Iteration: 0	Log-likelihood: -7.854219595886676
Iteration: 100	Log-likelihood: -7.794550892779028
Iteration: 200	Log-likelihood: -7.769356393354957
Iteration: 300	Log-likelihood: -7.760669986957033
Iteration: 400	Log-likelihood: -7.752600931523738
Iteration: 500	Log-likelihood: -7.747095299683212
Iteration: 600	Log-likelihood: -7.747758235485197
Iteration: 700	Log-likelihood: -7.752327772542281
Iteration: 800	Log-likelihood: -7.750339242796519
Iteration: 900	Log-likelihood: -7.746580096097232
&lt;Basic Info&gt;
| LDAModel (current version: 0.12.3)
| 1000 docs, 404158 words
| Total Vocabs: 27560, Used Vocabs: 5055
| Entropy of words: 7.37666
| Entropy of term-weighted words: 7.37666
| Removed Vocabs: the to of and in a s is for on
|
&lt;Training Info&gt;
| Iterations: 1000, Burn-in steps: 100
| Optimization Interval: 10
| Log-likelihood per word: -7.74658
|
&lt;Initial Parameters&gt;
| tw: TermWeight.ONE
| min_cf: 10 (minimum collection frequency of words)
| min_df: 0 (minimum document frequency of words)
| rm_top: 10 (the number of top words to be removed)
| k: 20 (the number of topics between 1 ~ 32767)
| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)
| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)
| seed: 967744972 (random seed)
| trained in version 0.12.3
|
&lt;Parameters&gt;
| alpha (Dirichlet prior on the per-document topic distributions)
|  [0.31919792 0.1450447  0.17351629 0.35764304 0.11996695 0.08289253
|   0.10895094 0.09602939 0.05753195 0.04877792 0.17098519 0.121597
|   0.25657448 0.1226459  0.72492486 0.1610593  0.5448861  0.22766687
|   0.29849392 0.23497261]
| eta (Dirichlet prior on the per-topic word distribution)
|  0.01
|
&lt;Topics&gt;
| #0 (20642) : with was he The as
| #1 (13412) : The I market stock that
| #2 (18491) : The rate U as Fed
| #3 (39164) : year quarter million 1 from
| #4 (8140) : China S trade U Trump
| #5 (6219) : oil Energy energy gas production
| #6 (6730) : bank banks financial gold its
| #7 (6979) : with The its vehicles GM
| #8 (6341) : with drug patients FDA healthcare
| #9 (7663) : or Zacks stocks investment are
| #10 (18393) : stock earnings has this that
| #11 (10174) : 1 week US The as
| #12 (38995) : Zacks Rank earnings company has
| #13 (10398) : you The stocks year dividend
| #14 (66535) : that are it have this
| #15 (12405) : NASDAQ Amazon its Apple Facebook
| #16 (44030) : its company has The growth
| #17 (18596) : 0 at The 1 NYSE
| #18 (29976) : percent S 1 after 2
| #19 (20875) : said that The S Reuters
|
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Topic #</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #0
		with	0.029238058254122734
		was	0.02469536103308201
		he	0.017301395535469055
		The	0.014595108106732368
		as	0.014111842028796673
		by	0.013725229538977146
		at	0.013628576882183552
		his	0.013290290720760822
		has	0.01324196346104145
		their	0.010439022444188595
Topic #1
		The	0.023696105927228928
		I	0.017530854791402817
		market	0.014708208851516247
		stock	0.012776925228536129
		that	0.010771362110972404
		over	0.010474241338670254
		this	0.009805720299482346
		my	0.009805720299482346
		at	0.009731439873576164
		but	0.009062918834388256
Topic #2
		The	0.021250110119581223
		rate	0.017366940155625343
		U	0.01618041656911373
		as	0.015479288063943386
		Fed	0.014508496038615704
		markets	0.014238830655813217
		year	0.014184897765517235
		economy	0.013160171918570995
		economic	0.012836574576795101
		by	0.012297244742512703
Topic #3
		year	0.05102723091840744
		quarter	0.03144266456365585
		million	0.03065214306116104
		1	0.021624881774187088
		from	0.019125809893012047
		billion	0.018156781792640686
		over	0.01670323871076107
		2	0.01596371829509735
		3	0.014535675756633282
		per	0.014025661163032055
Topic #4
		China	0.042855486273765564
		S	0.02759399637579918
		trade	0.02637307532131672
		U	0.024663789197802544
		Trump	0.019413836300373077
		with	0.0184371005743742
		Chinese	0.0184371005743742
		United	0.014652252197265625
		its	0.012454597279429436
		has	0.01233250554651022
Topic #5
		oil	0.04785192385315895
		Energy	0.03190181031823158
		energy	0.022491246461868286
		gas	0.021693741902709007
		production	0.018822723999619484
		by	0.018184719607234
		NYSE	0.016908710822463036
		The	0.01595170423388481
		prices	0.014994697645306587
		crude	0.013718688860535622
Topic #6
		bank	0.023450899869203568
		banks	0.022860977798700333
		financial	0.02005884423851967
		gold	0.02005884423851967
		its	0.0188790000975132
		Goldman	0.018436558544635773
		Bank	0.017846636474132538
		Financial	0.015781905502080917
		credit	0.01563442498445511
		investment	0.015339463949203491
Topic #7
		with	0.02247796766459942
		The	0.01735673099756241
		its	0.015507394447922707
		vehicles	0.013800314627587795
		GM	0.011808722279965878
		billion	0.0110974395647645
		cloud	0.0110974395647645
		Tesla	0.01095518283545971
		NASDAQ	0.01010164339095354
		car	0.01010164339095354
Topic #8
		with	0.025034615769982338
		drug	0.021279657259583473
		patients	0.013456830754876137
		FDA	0.012674547731876373
		healthcare	0.012205177918076515
		drugs	0.011422894895076752
		Inc	0.010327698662877083
		The	0.01017124205827713
		treatment	0.010014785453677177
		NASDAQ	0.009701872244477272
Topic #9
		or	0.03604177385568619
		Zacks	0.02048473060131073
		stocks	0.017373323440551758
		investment	0.017373323440551758
		are	0.014002632349729538
		that	0.01374334841966629
		Research	0.011798718012869358
		any	0.01153943408280611
		securities	0.010761582292616367
		1	0.010372656397521496
Topic #10
		stock	0.023640241473913193
		earnings	0.021146144717931747
		has	0.017513439059257507
		this	0.01659170724451542
		that	0.01594107411801815
		its	0.014477147720754147
		estimate	0.013934953138232231
		it	0.01344697643071413
		This	0.011332415975630283
		investors	0.011169757694005966
Topic #11
		1	0.040100544691085815
		week	0.024256324395537376
		US	0.02269146218895912
		The	0.01975734904408455
		as	0.01848589815199375
		with	0.015649588778614998
		at	0.015551784075796604
		price	0.013008885085582733
		Friday	0.013008885085582733
		0	0.012324259616434574
Topic #12
		Zacks	0.052503038197755814
		Rank	0.028172479942440987
		earnings	0.024561313912272453
		company	0.017595089972019196
		has	0.015955979004502296
		3	0.015418145805597305
		The	0.015341312624514103
		an	0.015136423520743847
		2	0.014624201692640781
		1	0.013676590286195278
Topic #13
		you	0.019429489970207214
		The	0.015888329595327377
		stocks	0.01502696517854929
		year	0.01493125781416893
		dividend	0.014644136652350426
		P	0.013495652005076408
		yield	0.013112824410200119
		5	0.0129214096814394
		2	0.012634288519620895
		fund	0.012634288519620895
Topic #14
		that	0.03056233748793602
		are	0.021641483530402184
		it	0.020590204745531082
		have	0.01630999520421028
		this	0.015604137443006039
		be	0.015453954227268696
		has	0.0148081686347723
		as	0.010918435640633106
		not	0.010347741656005383
		at	0.0099722845479846
Topic #15
		NASDAQ	0.027859870344400406
		Amazon	0.023444168269634247
		its	0.022882170975208282
		Apple	0.020875032991170883
		Facebook	0.013890192843973637
		billion	0.011722484603524208
		new	0.011722484603524208
		will	0.011160486377775669
		Google	0.011160486377775669
		The	0.011080200783908367
Topic #16
		its	0.02490917220711708
		company	0.022799396887421608
		has	0.01851179264485836
		The	0.01597099006175995
		growth	0.015199674293398857
		with	0.011978298425674438
		will	0.010957440361380577
		by	0.009981953538954258
		which	0.008802294731140137
		NYSE	0.008189779706299305
Topic #17
		0	0.03877446427941322
		at	0.029282091185450554
		The	0.02295384369790554
		1	0.021934889256954193
		NYSE	0.020701415836811066
		or	0.02011149562895298
		was	0.019253427162766457
		which	0.018127214163541794
		NASDAQ	0.017322774976491928
		Inc	0.016464708372950554
Topic #18
		percent	0.03280463442206383
		S	0.01968291401863098
		1	0.019216660410165787
		after	0.01641913503408432
		2	0.01631922461092472
		0	0.015819666907191277
		U	0.014987070113420486
		its	0.0138547383248806
		as	0.011756595224142075
		3	0.01045774482190609
Topic #19
		said	0.05505279451608658
		that	0.02700096182525158
		The	0.017777787521481514
		S	0.015675095841288567
		Reuters	0.015101633965969086
		it	0.015006056986749172
		U	0.0135246142745018
		by	0.012712210416793823
		would	0.012473268434405327
		have	0.011326344683766365
</pre></div>
</div>
</div>
</div>
</section>
<section id="lda-visualization">
<h3>LDA Visualization<a class="headerlink" href="#lda-visualization" title="Permalink to this headline">#</a></h3>
<p>This example shows how to perform a Latent Dirichlet Allocation using tomotopy and visualize the result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pyLDAvis</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">porter_stemmer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">PorterStemmer</span><span class="p">()</span><span class="o">.</span><span class="n">stem</span>
<span class="n">english_stops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">porter_stemmer</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
<span class="n">pat</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;^[a-z]{2,}$&#39;</span><span class="p">)</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">Corpus</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tp</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">SimpleTokenizer</span><span class="p">(</span><span class="n">porter_stemmer</span><span class="p">),</span> 
    <span class="n">stopwords</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">english_stops</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">pat</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">corpus</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">][:</span><span class="mi">1000</span><span class="p">])</span>
<span class="c1"># save preprocessed corpus for reuse</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">join_path</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s2">&quot;preprocessed_corpus.cps&quot;</span><span class="p">)</span>
<span class="n">corpus</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">rm_top</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">corpus</span><span class="o">=</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Num docs:</span><span class="si">{}</span><span class="s1">, Num Vocabs:</span><span class="si">{}</span><span class="s1">, Total Words:</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">),</span> <span class="n">mdl</span><span class="o">.</span><span class="n">num_words</span>
<span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Removed Top words: &#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">mdl</span><span class="o">.</span><span class="n">removed_top_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Num docs:1000, Num Vocabs:4058, Total Words:255382
Removed Top words:  year compani stock earn zack market quarter share expect said report nyse price million estim billion growth also rank trade
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s train the model</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{:04}</span><span class="s1">, LL per word: </span><span class="si">{:.4}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>
    <span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{:04}</span><span class="s1">, LL per word: </span><span class="si">{:.4}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">mdl</span><span class="o">.</span><span class="n">ll_per_word</span><span class="p">))</span>

<span class="n">mdl</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0000, LL per word: -11.91
Iteration: 0100, LL per word: -7.78
Iteration: 0200, LL per word: -7.713
Iteration: 0300, LL per word: -7.687
Iteration: 0400, LL per word: -7.668
Iteration: 1000, LL per word: -7.667
&lt;Basic Info&gt;
| LDAModel (current version: 0.12.3)
| 1000 docs, 255382 words
| Total Vocabs: 15059, Used Vocabs: 4058
| Entropy of words: 7.40001
| Entropy of term-weighted words: 7.40001
| Removed Vocabs: year compani stock earn zack market quarter share expect said report nyse price million estim billion growth also rank trade
|
&lt;Training Info&gt;
| Iterations: 500, Burn-in steps: 0
| Optimization Interval: 10
| Log-likelihood per word: -7.66656
|
&lt;Initial Parameters&gt;
| tw: TermWeight.ONE
| min_cf: 0 (minimum collection frequency of words)
| min_df: 5 (minimum document frequency of words)
| rm_top: 20 (the number of top words to be removed)
| k: 30 (the number of topics between 1 ~ 32767)
| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)
| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)
| seed: 2895547557 (random seed)
| trained in version 0.12.3
|
&lt;Parameters&gt;
| alpha (Dirichlet prior on the per-document topic distributions)
|  [0.48185164 0.38266072 0.08029101 0.2481648  0.38397688 0.10825986
|   0.41157666 0.05295729 0.09556314 0.1051754  0.27435258 0.09771502
|   0.09317087 0.0844915  0.19347201 0.34015027 0.13835451 0.15369639
|   0.0786861  0.1256792  0.08933923 0.07225117 0.14586316 0.12174093
|   0.04096875 0.12863763 0.13375707 0.09719449 0.11721694 0.61642367]
| eta (Dirichlet prior on the per-topic word distribution)
|  0.01
|
&lt;Topics&gt;
| #0 (22259) : industri inc current buy rate
| #1 (14453) : servic invest busi global base
| #2 (5167) : drug healthcar medic patient approv
| #3 (13907) : revenu oper increas per net
| #4 (15838) : one like get go make
| #5 (5639) : state court law investig case
| #6 (14796) : investor current buy chang time
| #7 (2845) : airlin boe air secur attack
| #8 (5199) : tax trump presid would hous
| #9 (4502) : nasdaq technolog cloud devic inc
| #10 (10485) : reuter new execut accord deal
| #11 (6315) : percent yen rose forecast japan
| #12 (4787) : amazon googl nasdaq facebook user
| #13 (3784) : appl iphon new game aapl
| #14 (14140) : rate econom fed data euro
| #15 (12862) : would use say govern includ
| #16 (7613) : week us support move day
| #17 (9658) : nasdaq close index fell point
| #18 (4513) : energi oil ga product rig
| #19 (5828) : china car vehicl chines product
| #20 (5389) : fund dividend yield etf invest
| #21 (5249) : invest secur inform research recommend
| #22 (9394) : consensu surpris posit esp beat
| #23 (5033) : retail sale store brand consum
| #24 (2568) : gold metal mine miner silver
| #25 (4971) : bank financi credit goldman rate
| #26 (4134) : second averag best first return
| #27 (6590) : valu score ratio investor industri
| #28 (4980) : revenu per analyst ep fiscal
| #29 (22484) : last month week sinc low
|
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">topic_term_dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_word_dist</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">)])</span>
<span class="n">doc_topic_dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">get_topic_dist</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">])</span>
<span class="n">doc_topic_dists</span> <span class="o">/=</span> <span class="n">doc_topic_dists</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">doc_lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">])</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">)</span>
<span class="n">term_frequency</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">used_vocab_freq</span>

<span class="n">prepared_data</span> <span class="o">=</span> <span class="n">pyLDAvis</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
    <span class="n">topic_term_dists</span><span class="p">,</span> 
    <span class="n">doc_topic_dists</span><span class="p">,</span> 
    <span class="n">doc_lengths</span><span class="p">,</span> 
    <span class="n">vocab</span><span class="p">,</span> 
    <span class="n">term_frequency</span><span class="p">,</span>
    <span class="n">start_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># tomotopy starts topic ids with 0, pyLDAvis with 1</span>
    <span class="n">sort_topics</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># IMPORTANT: otherwise the topic_ids between pyLDAvis and tomotopy are not matching!</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pyLDAvis</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">save_dir</span> <span class="o">=</span> <span class="s2">&quot;../../../assets/extra&quot;</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;lda_basic.html&quot;</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="n">eKonf</span><span class="o">.</span><span class="n">join_path</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
<span class="n">pyLDAvis</span><span class="o">.</span><span class="n">save_html</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>

<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&lt;a href=</span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2"> target=&#39;_blank&#39;&gt;</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">&lt;/a&gt;&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><a href=../../../assets/extra/lda_basic.html target='_blank'>lda_basic.html</a></div></div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/lectures/intro_nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="topic.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Topic Modeling</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../deep_nlp/index.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Deep Learning for NLP</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Young Joon Lee<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>