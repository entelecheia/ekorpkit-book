{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjgz1WTcPWee"
   },
   "source": [
    "# Lab 3: Training Tokenizers\n",
    "\n",
    "![](../figs/deep_nlp/lab/tokenize.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ekorpkit[wiki,fetch]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --pre ekorpkit[tokenize]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BzxPwsOVPWef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.utils.notebook:Google Colab not detected.\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_WORKSPACE_ROOT to /content/drive/MyDrive/workspace\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_PROJECT to ekorpkit-book\n",
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "version: 0.1.40.post0.dev17\n",
      "is colab? False\n",
      "project_dir: /content/drive/MyDrive/workspace/projects/ekorpkit-book\n",
      "time: 37.3 ms (started: 2022-11-13 04:47:09 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ekorpkit import eKonf\n",
    "\n",
    "eKonf.setLogger(\"INFO\")\n",
    "print(\"version:\", eKonf.__version__)\n",
    "\n",
    "is_colab = eKonf.is_colab()\n",
    "print(\"is colab?\", is_colab)\n",
    "if is_colab:\n",
    "    eKonf.mount_google_drive()\n",
    "workspace_dir = \"/content/drive/MyDrive/workspace\"\n",
    "project_name = \"ekorpkit-book\"\n",
    "project_dir = eKonf.set_workspace(workspace=workspace_dir, project=project_name)\n",
    "print(\"project_dir:\", project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it1Y0MVPXRsJ"
   },
   "source": [
    "### Load the saved corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.io.file:Processing [1] files from ['enko_filtered.parquet']\n",
      "INFO:ekorpkit.io.file:Loading 1 dataframes from ['/content/drive/MyDrive/workspace/projects/ekorpkit-book/data/enko_filtered.parquet']\n",
      "INFO:ekorpkit.io.file:Loading data from /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/enko_filtered.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>split</th>\n",
       "      <th>filename</th>\n",
       "      <th>corpus</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>avg_num_chars</th>\n",
       "      <th>avg_num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7644961</td>\n",
       "      <td>Anaissini is a tribe of click beetles in the f...</td>\n",
       "      <td>train</td>\n",
       "      <td>wiki_49</td>\n",
       "      <td>enwiki_sampled</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5.727273</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6658552</td>\n",
       "      <td>The Vicky Metcalf Award for Literature for You...</td>\n",
       "      <td>train</td>\n",
       "      <td>wiki_24</td>\n",
       "      <td>enwiki_sampled</td>\n",
       "      <td>479</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>5.841463</td>\n",
       "      <td>16.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11081255</td>\n",
       "      <td>Eylex Films Pvt is a chain of multiplex and si...</td>\n",
       "      <td>train</td>\n",
       "      <td>wiki_94</td>\n",
       "      <td>enwiki_sampled</td>\n",
       "      <td>1161</td>\n",
       "      <td>181</td>\n",
       "      <td>12</td>\n",
       "      <td>6.414365</td>\n",
       "      <td>15.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4706486</td>\n",
       "      <td>Željko Zečević (; born 21 October 1963) is a S...</td>\n",
       "      <td>train</td>\n",
       "      <td>wiki_02</td>\n",
       "      <td>enwiki_sampled</td>\n",
       "      <td>1151</td>\n",
       "      <td>201</td>\n",
       "      <td>15</td>\n",
       "      <td>5.726368</td>\n",
       "      <td>13.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2170359</td>\n",
       "      <td>Gilberto Nascimento Silva (born 9 June 1956) i...</td>\n",
       "      <td>train</td>\n",
       "      <td>wiki_57</td>\n",
       "      <td>enwiki_sampled</td>\n",
       "      <td>685</td>\n",
       "      <td>105</td>\n",
       "      <td>9</td>\n",
       "      <td>6.523810</td>\n",
       "      <td>11.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  split  \\\n",
       "1    7644961  Anaissini is a tribe of click beetles in the f...  train   \n",
       "2    6658552  The Vicky Metcalf Award for Literature for You...  train   \n",
       "4   11081255  Eylex Films Pvt is a chain of multiplex and si...  train   \n",
       "8    4706486  Željko Zečević (; born 21 October 1963) is a S...  train   \n",
       "12   2170359  Gilberto Nascimento Silva (born 9 June 1956) i...  train   \n",
       "\n",
       "   filename          corpus  num_chars  num_words  num_sents  avg_num_chars  \\\n",
       "1   wiki_49  enwiki_sampled         63         11          1       5.727273   \n",
       "2   wiki_24  enwiki_sampled        479         82          5       5.841463   \n",
       "4   wiki_94  enwiki_sampled       1161        181         12       6.414365   \n",
       "8   wiki_02  enwiki_sampled       1151        201         15       5.726368   \n",
       "12  wiki_57  enwiki_sampled        685        105          9       6.523810   \n",
       "\n",
       "    avg_num_words  \n",
       "1       11.000000  \n",
       "2       16.400000  \n",
       "4       15.083333  \n",
       "8       13.400000  \n",
       "12      11.666667  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.34 s (started: 2022-11-13 04:47:10 +00:00)\n"
     ]
    }
   ],
   "source": [
    "data = eKonf.load_data(\"enko_filtered.parquet\", project_dir + \"/data\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The London and South Western Railway T1 class was a class of fifty 0-4-4T steam tank locomotives designed for suburban passenger work by William Adams and built between 1888 and 1896.', 'History.', 'The class were numbered 1–20, 61–80 and 358–367. In typical London and South Western Railway fashion, they reused the numbers of retired or duplicated engines. The class remained largely intact until the 1930s, being scheduled to be withdrawn by 1940, however due to the Second World War a few remained in traffic (around eight examples) until the early British Railways years, the final one (30007) being withdrawn in May 1951.', 'Possible Revial.', \"No complete T1 locomotives were saved for preservation, however, a boiler and smokebox from a withdrawn locomotive was found in a factory in Essex back in the 1980s and was subsequently purchased for use on a 'new' T1 locomotive. Since September 2004, this boiler has been stored on the Avon Valley Railway.\"]\n",
      "['보이보디나 자치주()는 유고슬라비아 연방 인민 공화국의 세르비아 인민 공화국에 있던 두 개의 자치주 가운데 하나로, 수도는 노비사드였다.', '이 지역은 1945년부터 1963년까지 존재하였으며, 1963년에 보이보디나 사회주의 자치주로 개명하였다.']\n",
      "time: 146 ms (started: 2022-11-13 04:47:13 +00:00)\n"
     ]
    }
   ],
   "source": [
    "text_column = \"text\"\n",
    "\n",
    "text_en = (\n",
    "    data[data.corpus == \"enwiki_sampled\"][text_column].sample(1).values[0].split(\"\\n\")\n",
    ")\n",
    "text_ko = data[data.corpus == \"kowiki\"][text_column].sample(1).values[0].split(\"\\n\")\n",
    "\n",
    "print(text_en)\n",
    "print(text_ko)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### Covert pandas datafame to huggingface dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 603719\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 940 ms (started: 2022-11-13 03:06:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(data[[text_column]])\n",
    "raw_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 112 ms (started: 2022-11-13 03:07:01 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# shuffle the dataset\n",
    "\n",
    "raw_dataset = raw_dataset.shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into sentences for training\n",
    "\n",
    "The sentencepiece module comes with a python training API, which uses sentences in a file, one sentence per line. We will use the `sent_tokenize` function from the `nltk` package to split the text into sentences. The `sent_tokenize` function is a wrapper around the `punkt` tokenizer, which is a pre-trained sentence tokenizer. The `punkt` tokenizer is trained on the Penn Treebank corpus, which is a collection of Wall Street Journal articles. The `punkt` tokenizer is a good choice for plain English text, but it may not be the best choice for other languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.82 ms (started: 2022-11-12 06:40:36 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from ekorpkit.tokenizers.trainers.spm import export_sentence_chunk_files\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.tokenizers.spm:Writing sentence chunks to /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/tokenizers/enko_filtered_chunk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ebbb1b1a58498096c4db909dc50d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6393e3c31e4767912b265cf302c53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0000.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fe66a46197427ba6c666fc206d95f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0001.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c057403bef409a9c9e61bea947ff24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0002.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7579aa904b3e40ed92131be6d78e4421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0003.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cee6cb4a0047dea13757a0a75d1f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0004.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497f29f6bbca45cfa9fa1393012f9fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0005.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1317ae385c642c8a6f78331c473e986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0006.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486363dab1b74e6eb6a17f823e41fdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0007.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d6009136174737a4ff818e6d724c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0008.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2f50afa5ac467fa92e6876f1158e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0009.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1300392a0b4a4cb18c046cc7d20c7365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0010.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3268a2df14b74ceb959ca263bc784300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0011.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a27ed9113654169be6a9dbed5bb3836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0012.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d6341771d2417f97a5735179c8ee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0013.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4ff8a315de4aa4aab2101e094354a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0014.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3cbc5915f847d583811a30cd1f9219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0015.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a629be43c4a49309c26d9dc8fa8ad7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0016.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af75717dbc9448e9c75da815083295d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0017.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d621646ec5f45478373f45f754da45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0018.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bba6630437405781a7efb311b78646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0019.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3824cc7683c4bbd834fbb4c43c96732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0020.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953e34781070497e843b296e3b9ccffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0021.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf6cd36e9d643688d66fb27a1c516e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0022.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6237d87028447d941c5f2624aa7423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0023.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56754b1a995942d4a33374736c1467f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0024.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d86d8d2f62e402d8acfa68ff9949aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0025.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4471ec56f4a4a2695a0e21cfd2177d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0026.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5ac0cf428f4fe9a5ff21cce77f9afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0027.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae817a7e1a7647cba40ab08b7ddd92f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0028.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eda9a241fe74df2a582bce1bd04da04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0029.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92072fda4ac742ed86c11a2450cf98c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0030.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d3ca23620746a98aa73abd702b4478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0031.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9b08af8a6642d5ae8b1d5e46ec6c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0032.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ca5fc42bc741788dbaddd00e5740f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0033.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6453b8e02fea4fceb12622fafc279265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0034.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca60fc4d592454eb2075bb9f83cb7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0035.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9a1fc3f0fe453abac16bcb5c2f0686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0036.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210d7eb5a0084b3c830188bb82a9c6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0037.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7f3033590645419a2ca283ef3ad107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0038.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2051c0a19bd4459f8ce1ce28f6d8cf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0039.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faf54ba02cc41a393c0e4e6cd7a8a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0040.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2457b0f326146589daad0a41be7c17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0041.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0979246aaff4814b24f3cc9bba716e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0042.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14553ac08344b4d9ee7fee24e40258e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0043.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8b5e52aece404380e336d9195b5b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0044.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bf753237ee4dd789477701b011dd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0045.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc376d7c071444ee9d86ce1d36092d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0046.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27f8c1f7efe4dca82e4ac052382a276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0047.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9403a84983044f6bb7806bc71119df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0048.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb481928ec746a78c6965bc8a74ee2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0049.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac2760d07964aca8a025d54dbfd0391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0050.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a02b6eb2f1416daeaf8bf1937b09d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0051.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbb80a56de74c28b97b8432972eedeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0052.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2976ee0c6ab3473cbd817133e53b0e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0053.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f4da4373494d20b31f89fdd7f5ec9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0054.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4aea78ce10540d6abf62372b71c4a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0055.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc18f8b7a90b484c8f0b00dc1714aa6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0056.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ace2f9444c43e5b6bdd197a2214baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0057.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8e5508fe6040e7985b6ed8fd6276c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0058.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4be6bc48c3e4845b75b8e96d58539f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0059.txt:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df69545ff92548eeb6ee8a21033a770e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sentences to sent_chunk_0060.txt:   0%|          | 0/3719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 29s (started: 2022-11-12 06:40:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "output_dir = project_dir + \"/data/tokenizers/enko_filtered_chunk\"\n",
    "\n",
    "export_sentence_chunk_files(\n",
    "    raw_dataset,\n",
    "    output_dir=output_dir,\n",
    "    chunk_size=10000,\n",
    "    text_column=text_column,\n",
    "    sent_tokenize=sent_tokenize,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample sentences and combine them into a single file\n",
    "\n",
    "If your dataset is too large, you can sample a subset of the sentence files for training. The `sample` function from the `random` module can be used to sample a subset of the files.\n",
    "\n",
    "You can use `sample_and_combine` function to sample a subset of sentence files and combine them into a single file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.tokenizers.trainers.spm:sampled files: ['sent_chunk_0002.txt']\n",
      "INFO:ekorpkit.tokenizers.trainers.spm:number of lines sampled: 61,693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17460ebc71724050893c945f6a74053d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61693 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.tokenizers.trainers.spm:saved sampled sentences to /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/tokenizers/enko_filtered_samples/sampled_sentences.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 747 ms (started: 2022-11-13 03:08:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.tokenizers.trainers.spm import sample_and_combine\n",
    "\n",
    "input_dir = project_dir + \"/data/tokenizers/enko_filtered_chunk\"\n",
    "output_dir = project_dir + \"/data/tokenizers/enko_filtered_samples\"\n",
    "\n",
    "sampled_file = sample_and_combine(\n",
    "    input_dir=input_dir, output_dir=output_dir, sample_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizers with Hugging Face Tokenizers\n",
    "\n",
    "[Hugging Face's Tokenizers](https://huggingface.co/docs/tokenizers/quicktour) provides a wide range of tokenizers, including BPE, WordPiece, Unigram, SentencePiece, and ByteLevel. We will use the BPE and Unigram tokenizers in this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries and prepare functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.35 ms (started: 2022-11-12 06:51:08 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "from ekorpkit.tokenizers.trainers.spm import batch_chunks\n",
    "\n",
    "\n",
    "unk_token = \"<UNK>\"  # token for unknown words\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\", \"[MASK]\"]  # special tokens\n",
    "\n",
    "\n",
    "def prepare_tokenizer_trainer(algo):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if algo == \"BPE\":\n",
    "        tokenizer = Tokenizer(BPE(unk_token=unk_token))\n",
    "        trainer = BpeTrainer(special_tokens=spl_tokens)\n",
    "    elif algo == \"UNI\":\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(unk_token=unk_token, special_tokens=spl_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n",
    "        trainer = WordLevelTrainer(special_tokens=spl_tokens)\n",
    "\n",
    "    normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    return tokenizer, trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.2 ms (started: 2022-11-13 03:09:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def train_tokenizer(algo=\"BPE\"):\n",
    "    \"\"\"\n",
    "    Takes the files and trains the tokenizer.\n",
    "    \"\"\"\n",
    "    save_path = f\"{project_dir}/tokenizers/{algo}_tokenizer.json\"\n",
    "    tokenizer, trainer = prepare_tokenizer_trainer(algo)\n",
    "    tokenizer.train_from_iterator(\n",
    "        batch_chunks(raw_dataset, batch_size=1000, text_column=text_column),\n",
    "        trainer=trainer,\n",
    "    )\n",
    "    tokenizer.save(save_path)\n",
    "    tokenizer = Tokenizer.from_file(save_path)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BPE tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = train_tokenizer(\"BPE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> time: 15h 30min 58s (started: 2022-11-11 06:36:01 +00:00)\n",
    "\n",
    "To train a BPE tokenizer, it took 15 hours and 30 minutes for 603,719 wiki articles. The tokenizer was saved in the `{project_dir}/tokenizers` directory.\n",
    "\n",
    "> took 2m 15.5s\n",
    "\n",
    "With 256 processors, it took 2 minutes and 15 seconds to tokenize the 603,719 wiki articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train more efficiently with multiple processors, it is preferable to use CLI (command line interface) tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "ekorpkit \\\n",
    "    project.name=ekorpkit-book \\\n",
    "    dir.workspace=/content/drive/MyDrive/workspace \\\n",
    "    verbose=false \\\n",
    "    cmd=train_tokenizer \\\n",
    "    +tokenizer=train_hf \\\n",
    "    tokenizer.model_prefix=enko_wiki \\\n",
    "    tokenizer.model_type=bpe \\\n",
    "    tokenizer.vocab_size=30000 \\\n",
    "    tokenizer.input_files=sampled_sentences.txt \\\n",
    "    tokenizer.input_dir=data/tokenizers/enko_filtered_samples \\\n",
    "    tokenizer.output_dir=tokenizers/hf/enko_wiki\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "['The', 'London', 'and', 'South', 'Western', 'Railway', 'T', '1', 'class', 'was', 'a', 'class', 'of', 'fif', 'ty', '0', '-', '4', '-', '4', 'T', 'ste', 'am', 'tank', 'loc', 'om', 'ot', 'ives', 'designed', 'for', 'sub', 'urban', 'passenger', 'work', 'by', 'William', 'Adams', 'and', 'built', 'between', '1888', 'and', '1896', '.']\n",
      "['보이', '보', '디나', '자치', '주', '()', '는', '유고슬라비아', '연방', '인민', '공화국의', '세르비아', '인민', '공화국', '에', '있던', '두', '개의', '자치', '주', '가운데', '하나로', ',', '수도는', '노비', '사', '드', '였다', '.']\n",
      "time: 53.3 ms (started: 2022-11-13 04:48:08 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer_path = (\n",
    "    f\"{project_dir}/tokenizers/hf/enko_wiki/enko_wiki_bpe_huggingface_vocab_30000.json\"\n",
    ")\n",
    "\n",
    "bpe_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(f\"Vocab size: {bpe_tokenizer.get_vocab_size()}\")\n",
    "print(bpe_tokenizer.encode(text_en[0]).tokens)\n",
    "print(bpe_tokenizer.encode(text_ko[0]).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Unigram tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model_path = train_tokenizer(\"UNI\")\n",
    "```\n",
    "\n",
    "For a very large corpus, it may take a long time to train a Unigram tokenizer. It is recommended to use the following CLI command to train a Unigram tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "ekorpkit \\\n",
    "    project.name=ekorpkit-book \\\n",
    "    dir.workspace=/content/drive/MyDrive/workspace \\\n",
    "    verbose=false \\\n",
    "    cmd=train_tokenizer \\\n",
    "    +tokenizer=train_hf \\\n",
    "    tokenizer.model_prefix=enko_wiki \\\n",
    "    tokenizer.model_type=unigram \\\n",
    "    tokenizer.vocab_size=30000 \\\n",
    "    tokenizer.input_files=sampled_sentences.txt \\\n",
    "    tokenizer.input_dir=data/tokenizers/enko_filtered_samples \\\n",
    "    tokenizer.output_dir=tokenizers/hf/enko_wiki\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "['The', 'Lond', 'on', 'and', 'South', 'Wester', 'n', 'Railway', 'T', '1', 'class', 'was', 'a', 'class', 'of', 'fift', 'y', '0', '-', '4', '-', '4', 'T', 's', 'team', 'tank', 'lo', 'c', 'omotiv', 'es', 'design', 'ed', 'for', 'suburb', 'an', 'passenger', 'work', 'by', 'William', 'Adams', 'and', 'buil', 't', 'be', 'twe', 'en', '1888', 'and', '1896', '.']\n",
      "['보이', '보', '디', '나', '자치주', '()', '는', '유고슬라비아', '연방', '인민', '공화국', '의', '세르비아', '인민', '공화국', '에', '있던', '두', '개', '의', '자치주', '가운데', '하나로', ',', '수도', '는', '노비', '사', '드', '였다', '.']\n",
      "time: 56.3 ms (started: 2022-11-13 04:49:18 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer_path = (\n",
    "    f\"{project_dir}/tokenizers/hf/enko_wiki/enko_wiki_unigram_huggingface_vocab_30000.json\"\n",
    ")\n",
    "\n",
    "unigram_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(f\"Vocab size: {bpe_tokenizer.get_vocab_size()}\")\n",
    "print(unigram_tokenizer.encode(text_en[0]).tokens)\n",
    "print(unigram_tokenizer.encode(text_ko[0]).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizers with Google SentencePiece (SPM)\n",
    "\n",
    "### Install SentencePiece\n",
    "\n",
    "```bash\n",
    "pip install sentencepiece\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SentencePiece models\n",
    "\n",
    "You can use `train_spm` function to train a SentencePiece model. The `train_spm` function takes the following arguments:\n",
    "\n",
    "- `model_prefix`: The prefix of the model file. The model file will be saved as `{model_prefix}_{model_type}_vocab_{vocab_size}.model`.\n",
    "- `input`: The input file for training.\n",
    "- `output_dir`: The directory to save the model file.\n",
    "- `vocab_size`: The vocabulary size.\n",
    "- `model_type`: The model type. It can be `unigram` (default), `bpe`, `char`, or `word`.\n",
    "- `character_coverage`: The character coverage. It is only used for `unigram` and `bpe` model types. The default value is `1.0`.\n",
    "- `num_threads`: The number of threads to use for training. The default value is `1`. The max value is `128`.\n",
    "- `train_extremely_large_corpus`: Whether to train an extremely large corpus. The default value is `False`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Unigram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.tokenizers.spm:Training SentencePiece model enko_wiki_unigram_vocab_30000.model\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/tokenizers/enko_filtered_samples/sampled_sentences.txt\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 128\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /content/drive/MyDrive/workspace/projects/ekorpkit-book/data/tokenizers/enko_filtered_samples/sampled_sentences.txt\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 4000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 5000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 6000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 7000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 8000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 9000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 10000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 11000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 12000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 13000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 14000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 15000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 16000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 17000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 18000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 19000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 20000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 21000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 22000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 23000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 24000000 lines\n",
      "trainer_interface.cc(120) LOG(WARNING) Too many sentences are loaded! (24420004), which may slow down training.\n",
      "trainer_interface.cc(122) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(125) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 24420004 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=1528397615\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=3816\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 24419902 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) ExtracINFO:ekorpkit.tokenizers.spm:saved model to /content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/spm/enko_wiki_unigram_vocab_30000.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23min 6s (started: 2022-11-12 08:03:08 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 24419902\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 4696044\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 4696044 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=838106 obj=16.3019 num_tokens=14291441 num_tokens/piece=17.0521\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=688067 obj=14.038 num_tokens=14365800 num_tokens/piece=20.8785\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=516036 obj=13.9962 num_tokens=14484892 num_tokens/piece=28.0695\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=515998 obj=13.9855 num_tokens=14495195 num_tokens/piece=28.0916\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=386997 obj=14.0208 num_tokens=14763508 num_tokens/piece=38.1489\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=386994 obj=14.0098 num_tokens=14769200 num_tokens/piece=38.1639\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=290245 obj=14.1075 num_tokens=15208608 num_tokens/piece=52.3992\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=290245 obj=14.0799 num_tokens=15208857 num_tokens/piece=52.4001\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=217683 obj=14.2339 num_tokens=15712334 num_tokens/piece=72.1799\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=217683 obj=14.1986 num_tokens=15712378 num_tokens/piece=72.1801\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=163262 obj=14.3894 num_tokens=16252847 num_tokens/piece=99.5507\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=163262 obj=14.3501 num_tokens=16252560 num_tokens/piece=99.5489\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=122446 obj=14.5717 num_tokens=16812721 num_tokens/piece=137.307\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=122446 obj=14.5288 num_tokens=16812579 num_tokens/piece=137.306\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=91834 obj=14.7795 num_tokens=17416351 num_tokens/piece=189.65\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=91834 obj=14.7313 num_tokens=17416277 num_tokens/piece=189.65\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=68875 obj=15.012 num_tokens=18049647 num_tokens/piece=262.064\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=68875 obj=14.9565 num_tokens=18050042 num_tokens/piece=262.07\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=51656 obj=15.2728 num_tokens=18704298 num_tokens/piece=362.093\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=51656 obj=15.2075 num_tokens=18704520 num_tokens/piece=362.098\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=38742 obj=15.5638 num_tokens=19386320 num_tokens/piece=500.395\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=38742 obj=15.4915 num_tokens=19385990 num_tokens/piece=500.387\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=33000 obj=15.7017 num_tokens=19770798 num_tokens/piece=599.115\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=33000 obj=15.6585 num_tokens=19771014 num_tokens/piece=599.122\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.tokenizers.trainers.spm import train_spm\n",
    "\n",
    "uni_model_path = train_spm(\n",
    "    model_prefix=\"enko_wiki\",\n",
    "    input=sampled_file,\n",
    "    output_dir=project_dir + \"/tokenizers/spm\",\n",
    "    model_type=\"unigram\",\n",
    "    vocab_size=30000,\n",
    "    character_coverage=0.9995,\n",
    "    num_threads=128,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> time: 23min 6s (started: 2022-11-12 08:03:08 +00:00)\n",
    "\n",
    "It took 23 minutes to train a unigram model with a vocabulary size of 30,000. The model file was saved in the `{project_dir}/tokenizers` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "ekorpkit \\\n",
    "    project.name=ekorpkit-book \\\n",
    "    dir.workspace=/content/drive/MyDrive/workspace \\\n",
    "    verbose=false \\\n",
    "    cmd=train_tokenizer \\\n",
    "    +tokenizer=train_spm \\\n",
    "    tokenizer.model_prefix=enko_wiki \\\n",
    "    tokenizer.model_type=unigram \\\n",
    "    tokenizer.vocab_size=30000 \\\n",
    "    tokenizer.input_files=sampled_sentences.txt \\\n",
    "    tokenizer.input_dir=data/tokenizers/enko_filtered_samples \\\n",
    "    tokenizer.output_dir=tokenizers/spm/enko_wiki \\\n",
    "    tokenizer.character_coverage=0.9995 \\\n",
    "    tokenizer.num_workers=128\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "['▁The', '▁London', '▁and', '▁South', '▁Western', '▁Railway', '▁T', '1', '▁class', '▁was', '▁a', '▁class', '▁of', '▁fifty', '▁0', '-4', '-4', 'T', '▁steam', '▁tank', '▁locomotive', 's', '▁designed', '▁for', '▁suburb', 'an', '▁passenger', '▁work', '▁by', '▁William', '▁Adams', '▁and', '▁built', '▁between', '▁1888', '▁and', '▁1896', '.']\n",
      "['▁보이', '보', '디', '나', '▁자치주', '()', '는', '▁유고슬라비아', '▁연방', '▁인민', '▁공화국의', '▁세르비아', '▁인민', '▁공화국', '에', '▁있던', '▁두', '▁개의', '▁자치주', '▁가운데', '▁하나로', ',', '▁수도', '는', '▁노비', '사', '드', '였다', '.']\n",
      "time: 58.7 ms (started: 2022-11-13 04:52:09 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "model_file = \"tokenizers/spm/enko_wiki_unigram_vocab_30000.model\"\n",
    "model_file = project_dir + \"/\" + model_file\n",
    "uni_spm = spm.SentencePieceProcessor(model_file=model_file)\n",
    "print(f\"Vocab size: {uni_spm.vocab_size()}\")\n",
    "print(uni_spm.encode(text_en[0], out_type=str))\n",
    "print(uni_spm.encode(text_ko[0], out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train BPE model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "ekorpkit \\\n",
    "    project.name=ekorpkit-book \\\n",
    "    dir.workspace=/content/drive/MyDrive/workspace \\\n",
    "    verbose=false \\\n",
    "    cmd=train_tokenizer \\\n",
    "    +tokenizer=train_spm \\\n",
    "    tokenizer.model_prefix=enko_wiki \\\n",
    "    tokenizer.model_type=bpe \\\n",
    "    tokenizer.vocab_size=30000 \\\n",
    "    tokenizer.input_files=sampled_sentences.txt \\\n",
    "    tokenizer.input_dir=data/tokenizers/enko_filtered_samples \\\n",
    "    tokenizer.output_dir=tokenizers/spm/enko_wiki \\\n",
    "    tokenizer.character_coverage=0.9995 \\\n",
    "    tokenizer.num_workers=128\n",
    "```\n",
    "\n",
    "> took 15m 24.4s\n",
    "\n",
    "It took 15 minutes to train a BPE model with a vocabulary size of 30,000. The model file was saved in the `{project_dir}/tokenizers` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "['▁The', '▁London', '▁and', '▁South', '▁Western', '▁Railway', '▁T', '1', '▁class', '▁was', '▁a', '▁class', '▁of', '▁fif', 'ty', '▁0', '-4', '-4', 'T', '▁steam', '▁t', 'ank', '▁locomot', 'ives', '▁designed', '▁for', '▁sub', 'urban', '▁passenger', '▁work', '▁by', '▁William', '▁Adams', '▁and', '▁built', '▁between', '▁1888', '▁and', '▁189', '6.']\n",
      "['▁보이', '보', '디나', '▁자치', '주', '()', '는', '▁유고슬라비아', '▁연방', '▁인민', '▁공화국의', '▁세르비아', '▁인민', '▁공화국', '에', '▁있던', '▁두', '▁개의', '▁자치', '주', '▁가운데', '▁하나로', ',', '▁수', '도는', '▁노비', '사', '드', '였다', '.']\n",
      "time: 29.5 ms (started: 2022-11-13 04:53:59 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "model_file = \"tokenizers/spm/enko_wiki_bpe_vocab_30000.model\"\n",
    "model_file = project_dir + \"/\" + model_file\n",
    "bpe_spm = spm.SentencePieceProcessor(model_file=model_file)\n",
    "print(f\"Vocab size: {uni_spm.vocab_size()}\")\n",
    "print(bpe_spm.encode(text_en[0], out_type=str))\n",
    "print(bpe_spm.encode(text_ko[0], out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Tokenizers\n",
    "\n",
    "### Load the tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.1 ms (started: 2022-11-13 04:56:11 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tokenizers = {\n",
    "    \"BPE\": bpe_tokenizer,\n",
    "    \"UNI\": unigram_tokenizer,\n",
    "    \"UNI_SPM\": uni_spm,\n",
    "    \"BPE_SPM\": bpe_spm,\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize(tokenizer, text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using the tokenizer.\n",
    "    \"\"\"\n",
    "    if isinstance(tokenizer, spm.SentencePieceProcessor):\n",
    "        return tokenizer.encode(text, out_type=str)\n",
    "    return tokenizer.encode(text).tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the output of the tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: BPE\n",
      "['The', 'London', 'and', 'South', 'Western', 'Railway', 'T', '1', 'class', 'was', 'a', 'class', 'of', 'fif', 'ty', '0', '-', '4', '-', '4', 'T', 'ste', 'am', 'tank', 'loc', 'om', 'ot', 'ives', 'designed', 'for', 'sub', 'urban', 'passenger', 'work', 'by', 'William', 'Adams', 'and', 'built', 'between', '1888', 'and', '1896', '.']\n",
      "--------------------------------------------------\n",
      "Tokenizer: UNI\n",
      "['The', 'Lond', 'on', 'and', 'South', 'Wester', 'n', 'Railway', 'T', '1', 'class', 'was', 'a', 'class', 'of', 'fift', 'y', '0', '-', '4', '-', '4', 'T', 's', 'team', 'tank', 'lo', 'c', 'omotiv', 'es', 'design', 'ed', 'for', 'suburb', 'an', 'passenger', 'work', 'by', 'William', 'Adams', 'and', 'buil', 't', 'be', 'twe', 'en', '1888', 'and', '1896', '.']\n",
      "--------------------------------------------------\n",
      "Tokenizer: UNI_SPM\n",
      "['▁The', '▁London', '▁and', '▁South', '▁Western', '▁Railway', '▁T', '1', '▁class', '▁was', '▁a', '▁class', '▁of', '▁fifty', '▁0', '-4', '-4', 'T', '▁steam', '▁tank', '▁locomotive', 's', '▁designed', '▁for', '▁suburb', 'an', '▁passenger', '▁work', '▁by', '▁William', '▁Adams', '▁and', '▁built', '▁between', '▁1888', '▁and', '▁1896', '.']\n",
      "--------------------------------------------------\n",
      "Tokenizer: BPE_SPM\n",
      "['▁The', '▁London', '▁and', '▁South', '▁Western', '▁Railway', '▁T', '1', '▁class', '▁was', '▁a', '▁class', '▁of', '▁fif', 'ty', '▁0', '-4', '-4', 'T', '▁steam', '▁t', 'ank', '▁locomot', 'ives', '▁designed', '▁for', '▁sub', 'urban', '▁passenger', '▁work', '▁by', '▁William', '▁Adams', '▁and', '▁built', '▁between', '▁1888', '▁and', '▁189', '6.']\n",
      "--------------------------------------------------\n",
      "Tokenizer: BPE\n",
      "['보이', '보', '디나', '자치', '주', '()', '는', '유고슬라비아', '연방', '인민', '공화국의', '세르비아', '인민', '공화국', '에', '있던', '두', '개의', '자치', '주', '가운데', '하나로', ',', '수도는', '노비', '사', '드', '였다', '.']\n",
      "--------------------------------------------------\n",
      "Tokenizer: UNI\n",
      "['보이', '보', '디', '나', '자치주', '()', '는', '유고슬라비아', '연방', '인민', '공화국', '의', '세르비아', '인민', '공화국', '에', '있던', '두', '개', '의', '자치주', '가운데', '하나로', ',', '수도', '는', '노비', '사', '드', '였다', '.']\n",
      "--------------------------------------------------\n",
      "Tokenizer: UNI_SPM\n",
      "['▁보이', '보', '디', '나', '▁자치주', '()', '는', '▁유고슬라비아', '▁연방', '▁인민', '▁공화국의', '▁세르비아', '▁인민', '▁공화국', '에', '▁있던', '▁두', '▁개의', '▁자치주', '▁가운데', '▁하나로', ',', '▁수도', '는', '▁노비', '사', '드', '였다', '.']\n",
      "--------------------------------------------------\n",
      "Tokenizer: BPE_SPM\n",
      "['▁보이', '보', '디나', '▁자치', '주', '()', '는', '▁유고슬라비아', '▁연방', '▁인민', '▁공화국의', '▁세르비아', '▁인민', '▁공화국', '에', '▁있던', '▁두', '▁개의', '▁자치', '주', '▁가운데', '▁하나로', ',', '▁수', '도는', '▁노비', '사', '드', '였다', '.']\n",
      "--------------------------------------------------\n",
      "time: 20.6 ms (started: 2022-11-13 04:59:11 +00:00)\n"
     ]
    }
   ],
   "source": [
    "texts = [text_en[0] , text_ko[0]]\n",
    "tokens = {name: [] for name in tokenizers.keys()}\n",
    "\n",
    "\n",
    "# tokenize the texts with the tokenizers\n",
    "for text in texts:\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        print(f\"Tokenizer: {name}\")\n",
    "        tokens[name].append(tokenize(tokenizer, text))\n",
    "        print(tokens[name][-1])\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.9 ms (started: 2022-11-13 05:00:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compare_tokens(tokenizers, tokens, sample_num=0):\n",
    "\n",
    "    max_len = max(len(tokens[name][sample_num]) for name in tokenizers.keys())\n",
    "    diffs = {\n",
    "        name: max_len - len(tokens[name][sample_num]) for name in tokenizers.keys()\n",
    "    }\n",
    "\n",
    "    padded_tokens = {\n",
    "        name: tokens[name][sample_num] + [\"\"] * diffs[name]\n",
    "        for name in tokenizers.keys()\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(padded_tokens)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BPE</th>\n",
       "      <th>UNI</th>\n",
       "      <th>UNI_SPM</th>\n",
       "      <th>BPE_SPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>The</td>\n",
       "      <td>▁The</td>\n",
       "      <td>▁The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>London</td>\n",
       "      <td>Lond</td>\n",
       "      <td>▁London</td>\n",
       "      <td>▁London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>on</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South</td>\n",
       "      <td>and</td>\n",
       "      <td>▁South</td>\n",
       "      <td>▁South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Western</td>\n",
       "      <td>South</td>\n",
       "      <td>▁Western</td>\n",
       "      <td>▁Western</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Railway</td>\n",
       "      <td>Wester</td>\n",
       "      <td>▁Railway</td>\n",
       "      <td>▁Railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T</td>\n",
       "      <td>n</td>\n",
       "      <td>▁T</td>\n",
       "      <td>▁T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Railway</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>class</td>\n",
       "      <td>T</td>\n",
       "      <td>▁class</td>\n",
       "      <td>▁class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>was</td>\n",
       "      <td>1</td>\n",
       "      <td>▁was</td>\n",
       "      <td>▁was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a</td>\n",
       "      <td>class</td>\n",
       "      <td>▁a</td>\n",
       "      <td>▁a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>class</td>\n",
       "      <td>was</td>\n",
       "      <td>▁class</td>\n",
       "      <td>▁class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>of</td>\n",
       "      <td>a</td>\n",
       "      <td>▁of</td>\n",
       "      <td>▁of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fif</td>\n",
       "      <td>class</td>\n",
       "      <td>▁fifty</td>\n",
       "      <td>▁fif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ty</td>\n",
       "      <td>of</td>\n",
       "      <td>▁0</td>\n",
       "      <td>ty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>fift</td>\n",
       "      <td>-4</td>\n",
       "      <td>▁0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-</td>\n",
       "      <td>y</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>▁steam</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>▁tank</td>\n",
       "      <td>▁steam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>T</td>\n",
       "      <td>-</td>\n",
       "      <td>▁locomotive</td>\n",
       "      <td>▁t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ste</td>\n",
       "      <td>4</td>\n",
       "      <td>s</td>\n",
       "      <td>ank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>am</td>\n",
       "      <td>T</td>\n",
       "      <td>▁designed</td>\n",
       "      <td>▁locomot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tank</td>\n",
       "      <td>s</td>\n",
       "      <td>▁for</td>\n",
       "      <td>ives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>loc</td>\n",
       "      <td>team</td>\n",
       "      <td>▁suburb</td>\n",
       "      <td>▁designed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>om</td>\n",
       "      <td>tank</td>\n",
       "      <td>an</td>\n",
       "      <td>▁for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ot</td>\n",
       "      <td>lo</td>\n",
       "      <td>▁passenger</td>\n",
       "      <td>▁sub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ives</td>\n",
       "      <td>c</td>\n",
       "      <td>▁work</td>\n",
       "      <td>urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>designed</td>\n",
       "      <td>omotiv</td>\n",
       "      <td>▁by</td>\n",
       "      <td>▁passenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>for</td>\n",
       "      <td>es</td>\n",
       "      <td>▁William</td>\n",
       "      <td>▁work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sub</td>\n",
       "      <td>design</td>\n",
       "      <td>▁Adams</td>\n",
       "      <td>▁by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>urban</td>\n",
       "      <td>ed</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁William</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>passenger</td>\n",
       "      <td>for</td>\n",
       "      <td>▁built</td>\n",
       "      <td>▁Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>work</td>\n",
       "      <td>suburb</td>\n",
       "      <td>▁between</td>\n",
       "      <td>▁and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>by</td>\n",
       "      <td>an</td>\n",
       "      <td>▁1888</td>\n",
       "      <td>▁built</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>William</td>\n",
       "      <td>passenger</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁between</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Adams</td>\n",
       "      <td>work</td>\n",
       "      <td>▁1896</td>\n",
       "      <td>▁1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>and</td>\n",
       "      <td>by</td>\n",
       "      <td>.</td>\n",
       "      <td>▁and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>built</td>\n",
       "      <td>William</td>\n",
       "      <td></td>\n",
       "      <td>▁189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>between</td>\n",
       "      <td>Adams</td>\n",
       "      <td></td>\n",
       "      <td>6.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1888</td>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>and</td>\n",
       "      <td>buil</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1896</td>\n",
       "      <td>t</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>.</td>\n",
       "      <td>be</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td></td>\n",
       "      <td>twe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td></td>\n",
       "      <td>1888</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td></td>\n",
       "      <td>1896</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BPE        UNI      UNI_SPM     BPE_SPM\n",
       "0         The        The         ▁The        ▁The\n",
       "1      London       Lond      ▁London     ▁London\n",
       "2         and         on         ▁and        ▁and\n",
       "3       South        and       ▁South      ▁South\n",
       "4     Western      South     ▁Western    ▁Western\n",
       "5     Railway     Wester     ▁Railway    ▁Railway\n",
       "6           T          n           ▁T          ▁T\n",
       "7           1    Railway            1           1\n",
       "8       class          T       ▁class      ▁class\n",
       "9         was          1         ▁was        ▁was\n",
       "10          a      class           ▁a          ▁a\n",
       "11      class        was       ▁class      ▁class\n",
       "12         of          a          ▁of         ▁of\n",
       "13        fif      class       ▁fifty        ▁fif\n",
       "14         ty         of           ▁0          ty\n",
       "15          0       fift           -4          ▁0\n",
       "16          -          y           -4          -4\n",
       "17          4          0            T          -4\n",
       "18          -          -       ▁steam           T\n",
       "19          4          4        ▁tank      ▁steam\n",
       "20          T          -  ▁locomotive          ▁t\n",
       "21        ste          4            s         ank\n",
       "22         am          T    ▁designed    ▁locomot\n",
       "23       tank          s         ▁for        ives\n",
       "24        loc       team      ▁suburb   ▁designed\n",
       "25         om       tank           an        ▁for\n",
       "26         ot         lo   ▁passenger        ▁sub\n",
       "27       ives          c        ▁work       urban\n",
       "28   designed     omotiv          ▁by  ▁passenger\n",
       "29        for         es     ▁William       ▁work\n",
       "30        sub     design       ▁Adams         ▁by\n",
       "31      urban         ed         ▁and    ▁William\n",
       "32  passenger        for       ▁built      ▁Adams\n",
       "33       work     suburb     ▁between        ▁and\n",
       "34         by         an        ▁1888      ▁built\n",
       "35    William  passenger         ▁and    ▁between\n",
       "36      Adams       work        ▁1896       ▁1888\n",
       "37        and         by            .        ▁and\n",
       "38      built    William                     ▁189\n",
       "39    between      Adams                       6.\n",
       "40       1888        and                         \n",
       "41        and       buil                         \n",
       "42       1896          t                         \n",
       "43          .         be                         \n",
       "44                   twe                         \n",
       "45                    en                         \n",
       "46                  1888                         \n",
       "47                   and                         \n",
       "48                  1896                         \n",
       "49                     .                         "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27.5 ms (started: 2022-11-13 05:01:05 +00:00)\n"
     ]
    }
   ],
   "source": [
    "compare_tokens(tokenizers, tokens, sample_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BPE</th>\n",
       "      <th>UNI</th>\n",
       "      <th>UNI_SPM</th>\n",
       "      <th>BPE_SPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>보이</td>\n",
       "      <td>보이</td>\n",
       "      <td>▁보이</td>\n",
       "      <td>▁보이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>보</td>\n",
       "      <td>보</td>\n",
       "      <td>보</td>\n",
       "      <td>보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>디나</td>\n",
       "      <td>디</td>\n",
       "      <td>디</td>\n",
       "      <td>디나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>자치</td>\n",
       "      <td>나</td>\n",
       "      <td>나</td>\n",
       "      <td>▁자치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>주</td>\n",
       "      <td>자치주</td>\n",
       "      <td>▁자치주</td>\n",
       "      <td>주</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>는</td>\n",
       "      <td>는</td>\n",
       "      <td>는</td>\n",
       "      <td>는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>유고슬라비아</td>\n",
       "      <td>유고슬라비아</td>\n",
       "      <td>▁유고슬라비아</td>\n",
       "      <td>▁유고슬라비아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>연방</td>\n",
       "      <td>연방</td>\n",
       "      <td>▁연방</td>\n",
       "      <td>▁연방</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>인민</td>\n",
       "      <td>인민</td>\n",
       "      <td>▁인민</td>\n",
       "      <td>▁인민</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>공화국의</td>\n",
       "      <td>공화국</td>\n",
       "      <td>▁공화국의</td>\n",
       "      <td>▁공화국의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>세르비아</td>\n",
       "      <td>의</td>\n",
       "      <td>▁세르비아</td>\n",
       "      <td>▁세르비아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>인민</td>\n",
       "      <td>세르비아</td>\n",
       "      <td>▁인민</td>\n",
       "      <td>▁인민</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>공화국</td>\n",
       "      <td>인민</td>\n",
       "      <td>▁공화국</td>\n",
       "      <td>▁공화국</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>에</td>\n",
       "      <td>공화국</td>\n",
       "      <td>에</td>\n",
       "      <td>에</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>있던</td>\n",
       "      <td>에</td>\n",
       "      <td>▁있던</td>\n",
       "      <td>▁있던</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>두</td>\n",
       "      <td>있던</td>\n",
       "      <td>▁두</td>\n",
       "      <td>▁두</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>개의</td>\n",
       "      <td>두</td>\n",
       "      <td>▁개의</td>\n",
       "      <td>▁개의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>자치</td>\n",
       "      <td>개</td>\n",
       "      <td>▁자치주</td>\n",
       "      <td>▁자치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>주</td>\n",
       "      <td>의</td>\n",
       "      <td>▁가운데</td>\n",
       "      <td>주</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>가운데</td>\n",
       "      <td>자치주</td>\n",
       "      <td>▁하나로</td>\n",
       "      <td>▁가운데</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>하나로</td>\n",
       "      <td>가운데</td>\n",
       "      <td>,</td>\n",
       "      <td>▁하나로</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,</td>\n",
       "      <td>하나로</td>\n",
       "      <td>▁수도</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>수도는</td>\n",
       "      <td>,</td>\n",
       "      <td>는</td>\n",
       "      <td>▁수</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>노비</td>\n",
       "      <td>수도</td>\n",
       "      <td>▁노비</td>\n",
       "      <td>도는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>사</td>\n",
       "      <td>는</td>\n",
       "      <td>사</td>\n",
       "      <td>▁노비</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>드</td>\n",
       "      <td>노비</td>\n",
       "      <td>드</td>\n",
       "      <td>사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>였다</td>\n",
       "      <td>사</td>\n",
       "      <td>였다</td>\n",
       "      <td>드</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>.</td>\n",
       "      <td>드</td>\n",
       "      <td>.</td>\n",
       "      <td>였다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>였다</td>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       BPE     UNI  UNI_SPM  BPE_SPM\n",
       "0       보이      보이      ▁보이      ▁보이\n",
       "1        보       보        보        보\n",
       "2       디나       디        디       디나\n",
       "3       자치       나        나      ▁자치\n",
       "4        주     자치주     ▁자치주        주\n",
       "5       ()      ()       ()       ()\n",
       "6        는       는        는        는\n",
       "7   유고슬라비아  유고슬라비아  ▁유고슬라비아  ▁유고슬라비아\n",
       "8       연방      연방      ▁연방      ▁연방\n",
       "9       인민      인민      ▁인민      ▁인민\n",
       "10    공화국의     공화국    ▁공화국의    ▁공화국의\n",
       "11    세르비아       의    ▁세르비아    ▁세르비아\n",
       "12      인민    세르비아      ▁인민      ▁인민\n",
       "13     공화국      인민     ▁공화국     ▁공화국\n",
       "14       에     공화국        에        에\n",
       "15      있던       에      ▁있던      ▁있던\n",
       "16       두      있던       ▁두       ▁두\n",
       "17      개의       두      ▁개의      ▁개의\n",
       "18      자치       개     ▁자치주      ▁자치\n",
       "19       주       의     ▁가운데        주\n",
       "20     가운데     자치주     ▁하나로     ▁가운데\n",
       "21     하나로     가운데        ,     ▁하나로\n",
       "22       ,     하나로      ▁수도        ,\n",
       "23     수도는       ,        는       ▁수\n",
       "24      노비      수도      ▁노비       도는\n",
       "25       사       는        사      ▁노비\n",
       "26       드      노비        드        사\n",
       "27      였다       사       였다        드\n",
       "28       .       드        .       였다\n",
       "29              였다                 .\n",
       "30               .                  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.7 ms (started: 2022-11-13 05:01:13 +00:00)\n"
     ]
    }
   ],
   "source": [
    "compare_tokens(tokenizers, tokens, sample_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "corpus.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
