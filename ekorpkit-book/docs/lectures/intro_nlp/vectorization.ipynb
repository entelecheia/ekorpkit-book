{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Vector Semantics and Word Embeddings\n",
    "\n",
    "![](../figs/intro_nlp/vector/entelecheia_alphabet_letters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## Vector Semantics and Word Embeddings\n",
    "\n",
    "- Lexical semantics is the study of the meaning of words\n",
    "- Distributional hypothesis: words that occur in similar contexts have similar meanings\n",
    "- Sparse vectors: one-hot encoding\n",
    "- Dense vectors: word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f57ee",
   "metadata": {},
   "source": [
    "### What do words mean, and how do we represent that?\n",
    "\n",
    "> `cassoulet`\n",
    "\n",
    "Do we want to represent that ...\n",
    "\n",
    "- \"cassoulet\" is a French dish?\n",
    "- \"cassoulet\" contains meat and beans?\n",
    "- \"cassoulet\" is a stew?\n",
    "\n",
    "> `bar`\n",
    "\n",
    "Do we want to represent that ...\n",
    "\n",
    "- \"bar\" is a place where you can drink alcohol?\n",
    "- \"bar\" is a long rod?\n",
    "- \"bar\" is to prevent something from moving?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf9cd3",
   "metadata": {},
   "source": [
    "### Different approaches to lexical semantics\n",
    "\n",
    "NLP draws on two different approaches to lexical semantics:\n",
    "\n",
    "- **Lexical semantics**: \n",
    "  - The study of the meaning of words\n",
    "  - The lexicographic tradition aims to capture the information represented in lexical entries in dictionaries\n",
    "- **Distributional semantics**: \n",
    "  - The study of the meaning of words based on their distributional properties in large corpora\n",
    "  - The distributional hypothesis: words that occur in similar contexts have similar meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55701de7",
   "metadata": {},
   "source": [
    "#### Lexical semantics\n",
    "\n",
    "- Uses resources such as `lexicons`, `thesauri`, `ontologies` etc. that capture explicit knowledge about word meanings.\n",
    "- Assumes that words have `discrete word senses` that can be represented in a `lexicon`.\n",
    "  - bank 1 = a financial institution\n",
    "  - bank 2 = a river bank\n",
    "- May capture explicit knowledge about word meanings, but is limited in its ability to capture the meaning of words that are not in the lexicon.\n",
    "  -  `dog` is a `canine` (lexicon)\n",
    "  -  `cars` have `wheels` (lexicon)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d7870",
   "metadata": {},
   "source": [
    "#### Distributional semantics\n",
    "\n",
    "- Uses `large corpora of raw text` to learn the meaning of words from the contexts in which they occur.\n",
    "- Maps words to `vector representations` that capture the `distributional properties` of the words in the corpus.\n",
    "- Uses neural networks to learn the dense vector representations of words, `word embeddings`, from large corpora of raw text.\n",
    "- If each word is mapped to a single vector, this ignores the fact that words can have multiple meanings or parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c71ae",
   "metadata": {},
   "source": [
    "### How do we represent words to capture word similarities?\n",
    "\n",
    "- As `atomic symbols`\n",
    "  - in a traditional n-gram language model\n",
    "  - explicit features in a machine learning model\n",
    "  - this is equivalent to very high-dimensional one-hot vectors:\n",
    "    - aardvark = [1,0,...,0], bear = [0,1,...,0], ..., zebra = [0,0,...,1]\n",
    "    - height and tall are as different as aardvark and zebra\n",
    "- As very high-dimensional `sparse vectors`\n",
    "  - to capture the distributional properties of words\n",
    "- As low-dimensional `dense vectors`\n",
    "  - word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943297e",
   "metadata": {},
   "source": [
    "### What should word representations capture?\n",
    "\n",
    "- Vector representations of words were originally used to capture `lexical semantics` so that words with similar meanings would be represented by vectors that are close together in vector space.\n",
    "- These representations may also capture some `morphological` and `syntactic` information about words. (part of speech, inflections, stems, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04d66b",
   "metadata": {},
   "source": [
    "#### The Distributional Hypothesis\n",
    "\n",
    "Zellig Harris (1954):\n",
    "- Words that occur in similar contexts have similar meanings.\n",
    "- `oculist` and `eye doctor` occur in almost the same contexts\n",
    "- If A and B have almost the same environment, then A and B are synonymous.\n",
    "\n",
    "John Firth (1957):\n",
    "- You shall know a word by the company it keeps.\n",
    "\n",
    "> The `contexts` in which words occur tell us a lot about the meaning of words.\n",
    "> \n",
    "> Words that occur in similar contexts have similar meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad362e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
