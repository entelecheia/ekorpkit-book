{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Pretraining Language Models\n",
    "\n",
    "![](../figs/deep_nlp/lab/train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset and tokenizer, in this lab, we will train a language model on a large corpus of text from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "%pip install --pre ekorpkit[model]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.utils.notebook:Google Colab not detected.\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_WORKSPACE_ROOT to /content/drive/MyDrive/workspace\n",
      "INFO:ekorpkit.base:Setting EKORPKIT_PROJECT to ekorpkit-book\n",
      "INFO:ekorpkit.base:Loaded .env from /workspace/projects/ekorpkit-book/config/.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0.1.40.post0.dev22\n",
      "is colab? False\n",
      "project_dir: /content/drive/MyDrive/workspace/projects/ekorpkit-book\n",
      "time: 1.2 s (started: 2022-11-19 10:16:54 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ekorpkit import eKonf\n",
    "\n",
    "eKonf.setLogger(\"INFO\")\n",
    "print(\"version:\", eKonf.__version__)\n",
    "\n",
    "is_colab = eKonf.is_colab()\n",
    "print(\"is colab?\", is_colab)\n",
    "if is_colab:\n",
    "    eKonf.mount_google_drive()\n",
    "workspace_dir = \"/content/drive/MyDrive/workspace\"\n",
    "project_name = \"ekorpkit-book\"\n",
    "project_dir = eKonf.set_workspace(workspace=workspace_dir, project=project_name)\n",
    "print(\"project_dir:\", project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id 'entelecheia' will be used during this lab\n",
      "time: 866 ms (started: 2022-11-19 11:07:25 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "token = HfFolder.get_token()\n",
    "if token is None:\n",
    "    token = os.environ[\"HF_USER_ACCESS_TOKEN\"]\n",
    "\n",
    "if token is None:\n",
    "    raise ValueError(\"Please login to huggingface_hub\")\n",
    "\n",
    "user_id = HfApi().whoami(token)[\"name\"]\n",
    "\n",
    "print(f\"user id '{user_id}' will be used during this lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Normalization\n",
    "\n",
    "One little thing to note is that we will need to normalize our text before training our language model. This is because the same character can be represented in different ways. For example, the character \"é\" can be represented as \"e\" followed by a combining accent character, or as a single character.\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "Use `NFKC` normalization to normalize your text before training your language model.\n",
    "\n",
    "\n",
    "### Unicode Normalization Forms\n",
    "\n",
    "There are four normalization forms:\n",
    "\n",
    "- **NFC**: Normalization Form Canonical Composition\n",
    "- **NFD**: Normalization Form Canonical Decomposition\n",
    "- **NFKC**: Normalization Form Compatibility Composition\n",
    "- **NFKD**: Normalization Form Compatibility Decomposition\n",
    "\n",
    "In the above forms, \"C\" stands for \"Canonical\" and \"K\" stands for \"Compatibility\". The \"C\" forms are the most commonly used. The \"K\" forms are used when you need to convert characters to their compatibility representation. For example, the \"K\" forms will convert \"ﬁ\" to \"fi\".\n",
    "\n",
    "There two main differences between the two sets of forms:\n",
    "\n",
    "- The length of the string is changed or not: NFC and NFKC always produce a string of the same length or shorter, while NFD and NFKD may produce a string that is longer.\n",
    "- The original string is changed or not: NFC and NFD always produce a string that is identical to the original string, while NFKC and NFKD may produce a string that is different from the original string.\n",
    "\n",
    "### Unicode Normalization in Python\n",
    "\n",
    "In Python, you can use the `unicodedata` module to normalize your text. The `unicodedata.normalize` function takes two arguments:\n",
    "\n",
    "- `form`: The normalization form to use. This can be one of the following: `NFC`, `NFD`, `NFKC`, `NFKD`.\n",
    "- `unistr`: The string to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ａｂｃＡＢＣ１２３가나다…, 13\n",
      "NFC: ａｂｃＡＢＣ１２３가나다…, 13\n",
      "NFD: ａｂｃＡＢＣ１２３가나다…, 16\n",
      "NFKC: abcABC123가나다..., 15\n",
      "NFKD: abcABC123가나다..., 18\n",
      "time: 23.8 ms (started: 2022-11-19 10:16:57 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "text = \"ａｂｃＡＢＣ１２３가나다…\"\n",
    "print(f\"Original: {text}, {len(text)}\")\n",
    "for form in [\"NFC\", \"NFD\", \"NFKC\", \"NFKD\"]:\n",
    "    ntext = unicodedata.normalize(form, text)\n",
    "    print(f\"{form}: {ntext}, {len(ntext)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Pretraining\n",
    "\n",
    "In this lab, we will train a BERT-like model using masked-language modeling, one of the two pretraining tasks used in the original BERT paper.\n",
    "\n",
    "### What is BERT?\n",
    "\n",
    "BERT is a large-scale language model that was trained on the English Wikipedia using a masked-language modeling objective. The model was then fine-tuned on a variety of downstream tasks, including question answering, natural language inference, and sentiment analysis. BERT was the first large-scale language model to be pre-trained using a deep bidirectional architecture and outperformed previous language models on a variety of tasks.\n",
    "\n",
    "BERT was originally pre-trained on 1 Million Steps with a global batch size of 256.\n",
    "\n",
    "> \"We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus.\"\n",
    "\n",
    "For more information, see the lecture notes on BERT.\n",
    "\n",
    "### Masked-Language Modeling (MLM)\n",
    "\n",
    "Masked-language modeling is a pretraining task where we mask some of the input tokens and train the model to predict the original value of the masked tokens. For example, if we have the sentence \"The dog ate the apple\", we can mask the word \"ate\" and train the model to predict the original value of the masked token. The model will then learn to predict the original value of the masked tokens based on the context of the sentence.\n",
    "\n",
    "Example:\n",
    "\n",
    "> Input: \"The dog [MASK] the apple\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "Before training our language model, we need to preprocess our dataset. We will use our tokenizer to tokenize our dataset and then convert the tokens to their IDs. If we have a sentence that is longer than the maximum sequence length, we will truncate the sentence. If the sentence is shorter than the maximum sequence length, we will pad the sentence with the padding token.\n",
    "\n",
    "Unlike the original BERT paper, we will not use the WordPiece tokenization algorithm. Instead, we will use the `unigram` tokenization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "is_fast: True\n",
      "Vocab size: 30000\n",
      "{'input_ids': [1, 8, 14690, 10, 8, 968, 8, 6871, 8, 42, 8, 2777, 72, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 67 ms (started: 2022-11-19 10:55:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer_path = \"tokenizers/enko_wiki/enko_wiki_unigram_huggingface_vocab_30000.json\"\n",
    "tokenizer_path = project_dir + \"/\" + tokenizer_path\n",
    "context_length = 512\n",
    "\n",
    "unigram_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(f\"Vocab size: {unigram_tokenizer.get_vocab_size()}\")\n",
    "unigram_tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", unigram_tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", unigram_tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=unigram_tokenizer,\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_length=True,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "\n",
    "print(f\"is_fast: {tokenizer.is_fast}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(tokenizer(\"Hello, my dog is cute\"))\n",
    "tokenizer.save_pretrained(project_dir + \"/tokenizers/enko_wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 8, 14690, 10, 8, 235, 8, 202, 8, 15219, 489, 2, 8, 37, 8, 235, 8, 15219, 8, 11241, 8, 80, 8, 65, 9, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.9 ms (started: 2022-11-19 10:55:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeddc62f7db4f55a039916fbd57d972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-f34802e795f4ed05\n",
      "WARNING:datasets.builder:Reusing dataset text (/workspace/data/tbts/.cache/huggingface/datasets/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3618972\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.02 s (started: 2022-11-19 09:39:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_dir = project_dir + \"/data/tokenizers/enko_filtered_chunk\"\n",
    "\n",
    "dataset = load_dataset(\"text\", data_dir=data_dir, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.3 ms (started: 2022-11-19 09:55:12 +00:00)\n"
     ]
    }
   ],
   "source": [
    "text_column = \"text\"\n",
    "\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[text_column],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = 20\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=[text_column], num_proc=num_proc\n",
    ")\n",
    "tokenized_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= context_length:\n",
    "        total_length = (total_length // context_length) * context_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + context_length] for i in range(0, total_length, context_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=num_proc)\n",
    "\n",
    "# shuffle dataset\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=1234)\n",
    "\n",
    "print(f\"the dataset contains in total {len(tokenized_dataset)*context_length} tokens\")\n",
    "# the dataset contains in total 137,816,832 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.4 s (started: 2022-11-19 11:29:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "tk_path = project_dir + \"/tokenizers/enko_wiki\"\n",
    "\n",
    "# Load codeparrot tokenizer trained for Python code tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(tk_path)\n",
    "\n",
    "# Configuration\n",
    "config_kwargs = {\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    # \"torch_dtype\": \"float16\",\n",
    "}\n",
    "\n",
    "# # Load model with config and push to hub\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\", **config_kwargs)\n",
    "model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "model_path = project_dir + \"/models/enko_wiki_bert_base_uncased\"\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT size: 109.1M parameters\n",
      "time: 2.07 s (started: 2022-11-19 11:23:15 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"BERT size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 11:24:52.315972: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.71 s (started: 2022-11-19 11:24:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 268366\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 55.2 ms (started: 2022-11-19 11:27:08 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset_dir = project_dir + \"/data/tokenized_datasets/enko_filtered\"\n",
    "\n",
    "tokenized_dataset = Dataset.load_from_disk(dataset_dir)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 512])\n",
      "token_type_ids shape: torch.Size([5, 512])\n",
      "attention_mask shape: torch.Size([5, 512])\n",
      "labels shape: torch.Size([5, 512])\n",
      "time: 49.9 ms (started: 2022-11-19 11:27:34 +00:00)\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_dataset[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.43 s (started: 2022-11-19 11:34:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "time: 47.9 ms (started: 2022-11-19 11:34:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "acc_state = {str(k): str(v) for k, v in accelerator.state.__dict__.items()}\n",
    "device = accelerator.device\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "trainer = accelerator.prepare(trainer)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "took 6h 33m 0.0s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizerFast, AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    os.path.join(model_path, \"checkpoint-10000\")\n",
    ")\n",
    "tokenizer = AutoTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform predictions\n",
    "example = \"It is known that [MASK] is the capital of Germany\"\n",
    "for prediction in fill_mask(example):\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- [Unicode equivalence](https://en.wikipedia.org/wiki/Unicode_equivalence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
