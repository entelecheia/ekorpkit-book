{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899602f2-b9ee-458f-bb12-8da95bfb76d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# DALL·E 2\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20148f86",
   "metadata": {},
   "source": [
    "## CLIP\n",
    "\n",
    "- Paper: https://arxiv.org/abs/2103.00020\n",
    "- Blog post: https://openai.com/blog/clip/\n",
    "- Code: https://github.com/openai/CLIP (does not cover the training part)\n",
    "- Models: Available (on Apr 22, 2022, the last and the best ViT-L/14@336px model was published)\n",
    "- Alternative code: https://github.com/mlfoundations/open_clip (with training)\n",
    "- Alternative models (Multilingual): https://github.com/FreddeFrallan/Multilingual-CLIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d145cd0-2c51-4d1a-add2-1afee681c21a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- CLIP was originally a separate auxiliary model to rank the results from DALL·E. \n",
    "- An abbreviation for Contrastive Language-Image Pre-Training.\n",
    "- Take a large dataset of image-text pairs scraped from the Internet (400M such pairs). \n",
    "- Then train a contrastive model on such a dataset. \n",
    "- Contrastive models produce high scores (similarity) for an image and a text from the same pair (so they are similar) and a low score for mismatched texts and images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c974d-81c7-40a6-9f42-f38bed950931",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP: technical details\n",
    "\n",
    "- The model consists of two encoders: one for a text and another one for an image. \n",
    "- Encoders produce embeddings (a multidimensional vector representation of an object, say 512 bytes for each). \n",
    "- Then a dot product is calculated with two embeddings, and it results in a similarity score. \n",
    "- Embeddings are normalized, so this procedure outputs cosine similarity. \n",
    "- It is close to 1 for vectors pointing in the same direction (and consequently a small angle between them), 0 for orthogonal vectors, and -1 for opposite ones.\n",
    "- The model is trained to maximize the similarity score for the image-text pairs from the same dataset and minimize it for mismatched pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fc5e6-edd6-44a8-b452-c05786223e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Contrastive pre-training \n",
    "\n",
    "![objects](../figs/aiart/dalle2/contrastive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f62cb7-532a-4907-8d85-53134ec0135e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP image encoders\n",
    "\n",
    "- There are nine image encoders, five convolutional, and four transformer ones. \n",
    "- Convolutional encoders are ResNet-50, ResNet-101 and EfficientNet-like models called RN50x4, RN50x16, RN50x64 (the higher numbers, the better the model). \n",
    "- Transformer encoders are Vision Transformers (or ViT): ViT-B/32, ViT-B/16, ViT-L/14, and ViT-L/14@336. \n",
    "- The last one was fine-tuned on images with a resolution of 336×336 pixels, and others were trained on 224×224 pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a6be5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP text encoders\n",
    "\n",
    "- The text encoder is an ordinary transformer encoder but with masked attention. \n",
    "- It consists of 12 layers with 8 attention heads each, with 63M parameters in total. \n",
    "- The attention span is only 76 tokens (compared to the GPT-3 with 2048 tokens and a standard BERT with 512 tokens). \n",
    "- The text part of the model is suitable for pretty short texts only, and you can’t put a large paragraph into the model. \n",
    "- As DALL·E 2 uses mostly the same CLIP, it should have the same limitation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb82a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP applications\n",
    "\n",
    "- You can use such a model for ranking text-image pairs as was done in DALL·E to score multiple results and choose the best one. \n",
    "- You can use the CLIP features to train your custom classifiers on top of it.\n",
    "- You can use CLIP for zero-shot classification (when you didn't specifically train the model to work with these classes) with any number of classes. The classes can be adjustable without retraining a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d3aa10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "###  Zero-shot classification with CLIP\n",
    "\n",
    "![](../figs/aiart/dalle2/clip-zeroshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd22977",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CLIP prompt engineering: CLIPDraw\n",
    "\n",
    "- https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb\n",
    "- https://colab.research.google.com/github/pschaldenbrand/StyleCLIPDraw/blob/master/Style_ClipDraw.ipynb\n",
    "\n",
    "![](../figs/aiart/dalle2/clipdraw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0ced0",
   "metadata": {},
   "source": [
    "**CLIPDraw generation procedure**\n",
    "\n",
    "![](../figs/aiart/dalle2/clipdraw-procedure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eddd800",
   "metadata": {},
   "source": [
    "### CLIP prompt engineering: VQGAN-CLIP\n",
    "\n",
    "- https://github.com/nerdyrodent/VQGAN-CLIP\n",
    "- https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP(Updated).ipynb\n",
    "\n",
    "![](../figs/aiart/dalle2/vqgan-clip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e3be8c",
   "metadata": {},
   "source": [
    "**VQGAN-CLIP generation procedure**\n",
    "\n",
    "![](../figs/aiart/dalle2/vqgan-clip-procedure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d98fc",
   "metadata": {},
   "source": [
    "## GLIDE\n",
    "\n",
    "- Paper: https://arxiv.org/abs/2112.10741\n",
    "- Blog post: Strangely enough, OpenAI didn’t make a post on it\n",
    "- Code: https://github.com/openai/glide-text2im\n",
    "- Models: Available, but only a small model (300M instead of 3.5B parameters) trained on a filtered dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f374d",
   "metadata": {},
   "source": [
    "- GLIDE, which stands for `G`uided `L`anguage to `I`mage `D`iffusion for Generation and `E`diting, is a text-guided image generation model by OpenAI.\n",
    "- GLIDE generates images with a 256×256 pixel resolution.\n",
    "- GLIDE model with 3.5B parameters (but it seems the correct number is 5B parameters as there is a separate upsampling model with 1.5B parameters) is favored over 12B parameters DALL·E by human evaluators and also has beaten DALL·E by FID score.\n",
    "- GLIDE models can also be fine-tuned to perform image inpainting, enabling powerful text-driven image editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f78639",
   "metadata": {},
   "source": [
    "### GLIDE Examples\n",
    "\n",
    "![](../figs/aiart/dalle2/glide-examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5b044",
   "metadata": {},
   "source": [
    "### Text-conditional image inpainting\n",
    "\n",
    "\n",
    "![](../figs/aiart/dalle2/glide-inpainting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97a921",
   "metadata": {},
   "source": [
    "### Diffusion model\n",
    "\n",
    "- GLIDE resembles another kind of model called the diffusion model. \n",
    "- Diffusion models add random noise to input data through the chain of diffusion steps, and then they learn to reverse the diffusion process to construct images from the noise.\n",
    "\n",
    "![](../figs/aiart/dalle2/diffusion-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec8437",
   "metadata": {},
   "source": [
    "**Illustration of the process used to generate a new image with the diffusion model**\n",
    "\n",
    "![](../figs/aiart/dalle2/diffusion.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db94b8",
   "metadata": {},
   "source": [
    "**Diffusion models compared to the other classes of generative models**\n",
    "\n",
    "![](../figs/aiart/dalle2/diffusion-compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5c600",
   "metadata": {},
   "source": [
    "#### GLIDE technical details\n",
    "\n",
    "- First, the authors trained a 3.5B parameter diffusion model that uses a text encoder to condition on natural language descriptions. \n",
    "- Next, they compared two techniques for guiding diffusion models toward text prompts: CLIP guidance and classifier-free guidance.\n",
    "- Classifier guidance allows diffusion models to condition on a classifier’s labels, and gradients from the classifier are used to guide the sample towards the label.\n",
    "- The classifier-free guidance does not require a separate classifier model to be trained. is a form of guidance that interpolates between predictions from a diffusion model with and without labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27a84b",
   "metadata": {},
   "source": [
    "- Classifier-free guidance has two appealing properties\n",
    "- First, it allows a single model to leverage its own knowledge during guidance, rather than relying on the knowledge of a separate (and sometimes smaller) classification model. \n",
    "- Second, it simplifies guidance when conditioning on information that is difficult to predict with a classifier (such as text).\n",
    "- With CLIP guidance the classifier is replaced with a CLIP model. It uses the gradient of the dot product of the image and caption encodings with respect to the image.\n",
    "- The text-conditioned diffusion model is an augmented ADM model architecture that predicts an image for the next diffusion step based on a noised image xₜ and corresponding text caption c.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa022377",
   "metadata": {},
   "source": [
    "**GLIDE technical details: visual part**\n",
    "\n",
    "- The visual part is a modified U-Net architecture. \n",
    "- The U-Net model uses a stack of residual layers and down-sampling convolutions, followed by a stack of residual layers with up-sampling convolutions, with skip connections connecting the layers with the same spatial size.\n",
    "- There are different modifications to the original U-Net architecture regarding width, depth, and so on. \n",
    "- Global attention layers with several attention heads are added at the 8×8, 16×16, and 32×32 resolutions. \n",
    "- Also, a projection of the timestep embedding was added to each residual block..\n",
    "- For the classifier guidance, a classifier architecture is the down-sampling trunk of the U-Net model with an attention pool at the 8×8 layer to produce the final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28986026",
   "metadata": {},
   "source": [
    "**The original U-Net architecture**\n",
    "\n",
    "![](../figs/aiart/dalle2/u-net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ba26c",
   "metadata": {},
   "source": [
    "**GLIDE technical details: text part**\n",
    "\n",
    "- The text is encoded into a sequence of K (the maximum attention span is unclear) tokens that passed through a transformer model.\n",
    "- The output of this transformer is used in two ways: \n",
    "  - first, the final token embedding is used in place of a class embedding in the ADM model; \n",
    "  - second,the last layer of token embeddings (a sequence of K feature vectors) is separately projected to the dimensionality of each attention layer throughout the ADM model and then concatenated to the attention context at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21376be1",
   "metadata": {},
   "source": [
    "- The text transformer has 24 residual blocks of width 2048, resulting in roughly 1.2B parameters. \n",
    "- The visual part of the model trained for 64×64 resolution consists of 2.3B parameters. \n",
    "- In addition to the 3.5B parameters text-conditional diffusion model, the authors trained another 1.5B parameters text-conditional upsampling diffusion model to increase the resolution to 256×256 (this idea will be used in DALL·E as well). \n",
    "- The upsampling model is conditioned on text in the same way as the base model but uses a smaller text encoder with a width of 1024 instead of 2048. \n",
    "- For CLIP guidance, they also trained a noised 64×64 ViT-L CLIP model.\n",
    "- GLIDE was trained on the same dataset as DALL·E and the total training compute is roughly equal to that used to train DALL·E.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25caa6",
   "metadata": {},
   "source": [
    "**GLIDE is preferred by the human evaluators**\n",
    "\n",
    "![](../figs/aiart/dalle2/glide-eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a4554",
   "metadata": {},
   "source": [
    "#### GLIDE finetuning\n",
    "\n",
    "- The model was fine-tuned to support unconditional image generation. \n",
    "- This training procedure is exactly like pre-training, except 20% of text token sequences are replaced with the empty sequence. \n",
    "- This way, the model retained its ability to generate text-conditional outputs, but can also generate images unconditionally.\n",
    "- The model was also explicitly fine-tuned to perform inpainting. During fine-tuning, random regions of training examples are erased, and the remaining portions are fed into the model along with a mask channel as additional conditioning information.\n",
    "- GLIDE can be used iteratively to produce a complex scene using a zero-shot generation followed by a series of inpainting edits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f73553c",
   "metadata": {},
   "source": [
    "First, an image for the prompt “a cozy living room” is generated, then the shown inpainting masks are used and follow-up text prompts added a painting to the wall, a coffee table, and a vase of flowers on the coffee table, and finally to move the wall up to the couch.\n",
    "\n",
    "![](../figs/aiart/dalle2/glide-inpainting2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e332c9d8",
   "metadata": {},
   "source": [
    "## DALL·E 2/unCLIP\n",
    "\n",
    "- Paper: https://cdn.openai.com/papers/dall-e-2.pdf\n",
    "- Blog post: https://openai.com/dall-e-2/\n",
    "- Code: Not available\n",
    "- Models: Not available\n",
    "- Code (unofficial): https://github.com/lucidrains/DALLE2-pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80337b6b",
   "metadata": {},
   "source": [
    "DALL·E 1 vs. DALL·E 2\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle1-vs-dalle2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164229d",
   "metadata": {},
   "source": [
    "- OpenAI announced its DALL·E 2 system on April 6th, 2022. \n",
    "- The DALL·E 2 system significantly improves results over the original DALL·E. \n",
    "- It generates images with 4x greater resolution (compared to original DALL·E and GLIDE), now up to 1024×1024 pixels. \n",
    "- The model behind the DALL·E 2 system is called unCLIP.\n",
    "- The authors found that humans still slightly prefer GLIDE to unCLIP in terms of photorealism, but the gap is very small. \n",
    "- Even with similar photorealism, unCLIP is strongly preferred over GLIDE in terms of diversity, highlighting one of its benefits. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ba84c",
   "metadata": {},
   "source": [
    "DALL·E 2 can combine concepts, attributes, and styles:\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb26d9",
   "metadata": {},
   "source": [
    " Image editing based on text guidance\n",
    "\n",
    " ![Image editing based on text guidance](../figs/aiart/dalle2/dalle2-editing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d90cd91",
   "metadata": {},
   "source": [
    "Generating variations of an image\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-variations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c963a55",
   "metadata": {},
   "source": [
    "Some problems with DALL·E 2\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-problems.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0d359",
   "metadata": {},
   "source": [
    "unCLIP also struggles at producing coherent text\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e7f03",
   "metadata": {},
   "source": [
    "Producing details in complex scenes:\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-details.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb595a5",
   "metadata": {},
   "source": [
    "### DALL·E 2 technical details\n",
    "\n",
    "- It is a clever combination of CLIP and GLIDE, and the model itself (the full text-conditional image generation stack) is called unCLIP internally in the paper since it generates images by inverting the CLIP image encoder.\n",
    "- The CLIP model is trained separately. \n",
    "- Then the CLIP text encoder generates an embedding for the input text (caption). \n",
    "- Then a special prior model generates an image embedding based on the text embedding. \n",
    "- Then a diffusion decoder generates an image based on the image embedding. \n",
    "- The decoder essentially inverts image embeddings back into images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b1172",
   "metadata": {},
   "source": [
    "A high-level overview of DALL·E 2\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d0da9",
   "metadata": {},
   "source": [
    "### DALL·E 2 technical details: encoders\n",
    "\n",
    "- The CLIP model uses a ViT-H/16 image encoder that consumes 256×256 resolution images and has a width of 1280 with 32 Transformer blocks (it’s deeper than the largest ViT-L from the original CLIP work). \n",
    "- The text encoder is a Transformer with a causal attention mask, with a width of 1024 and 24 Transformer blocks (the original CLIP model had 12 transformer blocks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9007a9",
   "metadata": {},
   "source": [
    "### DALL·E 2 technical details: decoders\n",
    "\n",
    "- The diffusion decoder is a modified GLIDE with 3.5B parameters. \n",
    "- CLIP image embeddings are projected and added to the existing timestep embedding. \n",
    "- CLIP embeddings are also projected into four extra tokens of context that are concatenated to the sequence of outputs from the GLIDE text encoder. \n",
    "- The original GLIDE’s text conditioning pathway is retained because it could allow the diffusion model to learn aspects of natural language that CLIP fails to capture (yet, it helped little). \n",
    "- During training, the CLIP embeddings are randomly set to zero 10% of the time, and the text caption was randomly dropped 50% of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c275a2",
   "metadata": {},
   "source": [
    "- The decoder generates a 64×64 pixel image, and then two upsampling diffusion models subsequently generate 256×256 and 1024×1024 images, the former with 700M parameters, and the latter with 300M parameters. \n",
    "- To improve upsampling robustness, the conditioning images are slightly corrupted during training. \n",
    "- For the first upsampling stage, gaussian blur was used, and for the second, a more diverse BSR degradation is used, which includes JPEG compression artifacts, camera sensor noise, bilinear and bicubic interpolations, Gaussian noise. \n",
    "- The models are trained on random crops of images that are one-fourth the target size. Text conditioning is not used for the upsampling models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551385c",
   "metadata": {},
   "source": [
    "### DALL·E 2 technical details: the prior\n",
    "\n",
    "- The prior produces image embeddings from text descriptions. (Autoregressive (AR) prior and Diffusion prior with 1B parameters)\n",
    "- In the AR prior, the CLIP image embedding is converted into a sequence of discrete codes and predicted autoregressively conditioned on the caption. \n",
    "- In the diffusion prior the continuous embedding vector is directly modeled using a Gaussian diffusion model conditioned on the caption.\n",
    "- In addition to the caption, the prior model can be conditioned on the CLIP text embedding since it is a deterministic function of the caption. \n",
    "- To improve sample quality, the authors also enabled sampling using classifier-free guidance for both the AR and diffusion prior by randomly dropping this text conditioning information 10% of the time during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b400fc",
   "metadata": {},
   "source": [
    "AR prior\n",
    "\n",
    "- For the AR prior, the dimensionality of the CLIP image embeddings was reduced by Principal Component Analysis (PCA). \n",
    "- 319 principal components out of 1024 keep more than 99% information. \n",
    "- Each dimension is quantized into 1024 buckets. \n",
    "- Authors condition the AR prior on the text caption and the CLIP text embedding by encoding them as a prefix to the sequence. \n",
    "- Additionally, they prepend a token indicating the (quantized) dot product between the text embedding and image embedding. \n",
    "- This allowed conditioning of the model on a higher dot product since higher text-image dot products correspond to captions that better describe the image. \n",
    "- The dot product was sampled from the top half of the distribution. \n",
    "- The resulting sequence is predicted using a Transformer model with a causal attention mask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae11749c",
   "metadata": {},
   "source": [
    "Diffusion prior\n",
    "\n",
    "- For the diffusion prior, a decoder-only Transformer with a causal attention mask is trained on a sequence consisting of:\n",
    "  - the encoded text\n",
    "  - the CLIP text embedding\n",
    "  - an embedding for the diffusion timestep\n",
    "  - the noised CLIP image embedding\n",
    "  - a final embedding whose output from the Transformer is used to predict the unnoised CLIP image embedding.\n",
    "- A dot product was not used to condition the diffusion prior. \n",
    "- Instead, to improve quality during sampling time, two samples of an image embedding were generated, and the one with a higher dot product with a text embedding was selected.\n",
    "- Diffusion prior outperforms the AR prior for comparable model size and reduced training compute. The diffusion prior also performs better than the AR prior in pairwise comparisons against GLIDE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ddc81",
   "metadata": {},
   "source": [
    "AR vs Diffusion prior\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-ar-diffusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac25941",
   "metadata": {},
   "source": [
    "Using different conditioning signals\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-conditioning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ac1dd",
   "metadata": {},
   "source": [
    "### DALL·E 2 technical details: training\n",
    "\n",
    "- When training the encoder, authors sampled from the CLIP and DALL-E datasets (approximately 650M images in total) with equal probability. \n",
    "- When training the decoder, upsamplers, and prior, they used only the DALL-E dataset (approximately 250M images), as incorporating the noisier CLIP dataset while training the generative stack negatively impacted sample quality in their initial evaluations.\n",
    "- The total model size seems to be: 632M? parameters (CLIP ViT-H/16 image encoder) + 340M? (CLIP text encoder) + 1B (Diffusion prior) + 3.5B (diffusion decoder) + 1B (two diffusion upsamplers) =~ 6.5B parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23add75",
   "metadata": {},
   "source": [
    "unCLIP applications\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-applications.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8c039",
   "metadata": {},
   "source": [
    "- Each image x can be encoded into a bipartite latent representation (zi, xT) that is sufficient for the decoder to produce an accurate reconstruction. \n",
    "- The latent zi is a CLIP image embedding, and it describes the aspects of the image that are recognized by CLIP. \n",
    "- The latent xT is obtained by applying DDIM (denoising diffusion implicit model) inversion to x using the decoder while conditioning on zi. \n",
    "- In other words, it is a starting noise for the diffusion process when it generates the image x (or equivalently x0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d626d1c",
   "metadata": {},
   "source": [
    "Three interesting kinds of manipulations\n",
    "\n",
    "- First, you can create image variations for the given bipartite latent representation (zi, xT) by sampling in the decoder using DDIM with η > 0. \n",
    "- With η = 0, the decoder becomes deterministic and will reconstruct the given image x. \n",
    "- The larger the η parameter, the larger variations, and we can see what information was captured in the CLIP image embedding and present in all samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0da33",
   "metadata": {},
   "source": [
    "Creating image variations\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-variation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504c45f",
   "metadata": {},
   "source": [
    "- Second, you can make interpolations between images x1 and x2. \n",
    "- In order to do it, you have to take CLIP image embeddings zi1 and zi2, then apply slerp (Spherical Linear Interpolation) to obtain intermediate CLIP image representations. \n",
    "- There are two options for the corresponding intermediate DDIM latent xTi: \n",
    "  1. interpolate between xT1 and xT2 with slerp, \n",
    "  2. fix the DDIM latent to a randomly-sampled value for all interpolates in the\u000btrajectory (and you can generate an infinite number of trajectories this way). The following images were generated with the second option.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a8506",
   "metadata": {},
   "source": [
    "Images were generated with the second option\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-interpolation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e76a0f",
   "metadata": {},
   "source": [
    "- Finally, the third thing is language-guided image manipulations or text diffs. \n",
    "- In order to modify the image to reflect a new text description y, you first obtain its CLIP text embedding zt, as well as the CLIP text embedding zt0 of a caption describing the current image (which might be a dummy caption like “a photo” or an empty caption at all). \n",
    "- Then you compute a text diff vector zd = norm(zt − zt0). \n",
    "- Then you rotate between the image CLIP embedding zi and the text diff vector zd with slerp and generate images with the fixed base DDIM noise xT throughout the entire trajectory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d7edb",
   "metadata": {},
   "source": [
    "- “concept space” for text\n",
    "> “woman”+“king”−“man”,\n",
    "\n",
    "- perform arithmetic using both text and images\n",
    "> (image of victorian house)+“a modern house”−“a victorian house”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d470eac",
   "metadata": {},
   "source": [
    "Exploring text diffs\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-text-diffs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e13e67",
   "metadata": {},
   "source": [
    "The case with typographic attacks\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-typographic-attacks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12665dae",
   "metadata": {},
   "source": [
    "### The struggles of unCLIP \n",
    "\n",
    "- The struggles of unCLIP: attribute binding, text generation, and details in complex scenes.\n",
    "- The first two problems are probably due to CLIP embeddings properties.\n",
    "- The attribute binding problem might happen because the CLIP embedding itself does not explicitly bind attributes to objects, so the decoder mix up attributes and objects when generating an image.\n",
    "- The text generation problem is probably because the CLIP embedding does\u000bnot precisely encode the spelling information of a rendered text.\n",
    "- The low details problem might emerge due to the decoder hierarchy producing an image at a base resolution of 64×64 and then upsampling it. \n",
    "- So, with a higher base resolution, the problem might disappear (at the cost of additional training and inference compute).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another set of reconstructions for difficult binding problems\n",
    "\n",
    "![](../figs/aiart/dalle2/dalle2-difficult-binding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b60675",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
