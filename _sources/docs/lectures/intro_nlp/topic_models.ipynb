{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {},
   "source": [
    "# Topic Models [![Jupyter Book Badge](https://jupyterbook.org/badge.svg)](https://entelecheia.github.io/ekorpkit-book/) \n",
    "\n",
    "[![tomoto](../figs/intro_nlp/tomotopy/tomoto.png)](https://github.com/bab2min/tomotopy)\n",
    "\n",
    "Package tomotopy {cite}`tomotopy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {},
   "source": [
    "## What is tomotopy?\n",
    "\n",
    "`tomotopy` is a Python extension of `tomoto` (Topic Modeling Tool) which is a Gibbs-sampling based topic model library written in C++.\n",
    "It utilizes a vectorization of modern CPUs for maximizing speed. \n",
    "The current version of `tomoto` supports several major topic models including \n",
    "\n",
    "* Latent Dirichlet Allocation (`tomotopy.LDAModel`)\n",
    "* Labeled LDA (`tomotopy.LLDAModel`)\n",
    "* Partially Labeled LDA (`tomotopy.PLDAModel`)\n",
    "* Supervised LDA (`tomotopy.SLDAModel`)\n",
    "* Dirichlet Multinomial Regression (`tomotopy.DMRModel`)\n",
    "* Generalized Dirichlet Multinomial Regression (`tomotopy.GDMRModel`)\n",
    "* Hierarchical Dirichlet Process (`tomotopy.HDPModel`)\n",
    "* Hierarchical LDA (`tomotopy.HLDAModel`)\n",
    "* Multi Grain LDA (`tomotopy.MGLDAModel`) \n",
    "* Pachinko Allocation (`tomotopy.PAModel`)\n",
    "* Hierarchical PA (`tomotopy.HPAModel`)\n",
    "* Correlated Topic Model (`tomotopy.CTModel`)\n",
    "* Dynamic Topic Model (`tomotopy.DTModel`)\n",
    "* Pseudo-document based Topic Model (`tomotopy.PTModel`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5999eb3e",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "\n",
    "You can install tomotopy easily using pip. (https://pypi.org/project/tomotopy/)\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install tomotopy\n",
    "```\n",
    "\n",
    "The supported OS and Python versions are:\n",
    "\n",
    "* Linux (x86-64) with Python >= 3.6 \n",
    "* macOS >= 10.13 with Python >= 3.6\n",
    "* Windows 7 or later (x86, x86-64) with Python >= 3.6\n",
    "* Other OS with Python >= 3.6: Compilation from source code required (with c++14 compatible compiler)\n",
    "\n",
    "After installing, you can start tomotopy by just importing.\n",
    "\n",
    "```python\n",
    "import tomotopy as tp\n",
    "print(tp.isa) # prints 'avx2', 'avx', 'sse2' or 'none'\n",
    "```\n",
    "\n",
    "Currently, tomotopy can exploits AVX2, AVX or SSE2 SIMD instruction set for maximizing performance.\n",
    "When the package is imported, it will check available instruction sets and select the best option.\n",
    "If `tp.isa` tells `none`, iterations of training may take a long time. \n",
    "But, since most of modern Intel or AMD CPUs provide SIMD instruction set, the SIMD acceleration could show a big improvement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb2082",
   "metadata": {},
   "source": [
    "Here is a sample code for simple LDA training of texts from 'sample.txt' file.\n",
    "```python\n",
    "import tomotopy as tp\n",
    "mdl = tp.LDAModel(k=20)\n",
    "for line in open('sample.txt'):\n",
    "    mdl.add_doc(line.strip().split())\n",
    "\n",
    "for i in range(0, 100, 10):\n",
    "    mdl.train(10)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "\n",
    "for k in range(mdl.k):\n",
    "    print('Top 10 words of topic #{}'.format(k))\n",
    "    print(mdl.get_topic_words(k, top_n=10))\n",
    "\n",
    "mdl.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f42764",
   "metadata": {},
   "source": [
    "## Performance of tomotopy\n",
    "\n",
    "`tomotopy` uses Collapsed Gibbs-Sampling(CGS) to infer the distribution of topics and the distribution of words.\n",
    "Generally CGS converges more slowly than Variational Bayes(VB) that [`gensim's LdaModel`](https://radimrehurek.com/gensim/models/ldamodel.html) uses, but its iteration can be computed much faster.\n",
    "In addition, `tomotopy` can take advantage of multicore CPUs with a SIMD instruction set, which can result in faster iterations.\n",
    "\n",
    "Following chart shows the comparison of LDA model's running time between `tomotopy` and `gensim`. \n",
    "The input data consists of 1000 random documents from English Wikipedia with 1,506,966 words (about 10.1 MB).\n",
    "`tomotopy` trains 200 iterations and `gensim` trains 10 iterations.\n",
    "\n",
    "![](../figs/intro_nlp/tomotopy/tmt_i5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835004df",
   "metadata": {},
   "source": [
    "## Model Save and Load\n",
    "\n",
    "`tomotopy` provides `save` and `load` method for each topic model class, \n",
    "so you can save the model into the file whenever you want, and re-load it from the file.\n",
    "\n",
    "```python\n",
    "import tomotopy as tp\n",
    "\n",
    "mdl = tp.HDPModel()\n",
    "for line in open('sample.txt'):\n",
    "    mdl.add_doc(line.strip().split())\n",
    "\n",
    "for i in range(0, 100, 10):\n",
    "    mdl.train(10)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "\n",
    "# save into file\n",
    "mdl.save('sample_hdp_model.bin')\n",
    "\n",
    "# load from file\n",
    "mdl = tp.HDPModel.load('sample_hdp_model.bin')\n",
    "for k in range(mdl.k):\n",
    "    if not mdl.is_live_topic(k): continue\n",
    "    print('Top 10 words of topic #{}'.format(k))\n",
    "    print(mdl.get_topic_words(k, top_n=10))\n",
    "\n",
    "# the saved model is HDP model, \n",
    "# so when you load it by LDA model, it will raise an exception\n",
    "mdl = tp.LDAModel.load('sample_hdp_model.bin')\n",
    "```\n",
    "\n",
    "When you load the model from a file, a model type in the file should match the class of methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923cee3",
   "metadata": {},
   "source": [
    "## Documents in the Model and out of the Model\n",
    "\n",
    "We can use Topic Model for two major purposes. \n",
    "The basic one is to discover topics from a set of documents as a result of trained model,\n",
    "and the more advanced one is to infer topic distributions for unseen documents by using trained model.\n",
    "\n",
    "We named the document in the former purpose (used for model training) as **document in the model**,\n",
    "and the document in the later purpose (unseen document during training) as **document out of the model**.\n",
    "\n",
    "In `tomotopy`, these two different kinds of document are generated differently.\n",
    "A **document in the model** can be created by `tomotopy.LDAModel.add_doc` method.\n",
    "`add_doc` can be called before `tomotopy.LDAModel.train` starts. \n",
    "In other words, after `train` called, `add_doc` cannot add a document into the model because the set of document used for training has become fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab9370",
   "metadata": {},
   "source": [
    "To acquire the instance of the created document, you should use `tomotopy.LDAModel.docs` like:\n",
    "\n",
    "```python\n",
    "mdl = tp.LDAModel(k=20)\n",
    "idx = mdl.add_doc(words)\n",
    "if idx < 0: raise RuntimeError(\"Failed to add doc\")\n",
    "doc_inst = mdl.docs[idx]\n",
    "# doc_inst is an instance of the added document\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9f8ec",
   "metadata": {},
   "source": [
    "A **document out of the model** is generated by `tomotopy.LDAModel.make_doc` method. `make_doc` can be called only after `train` starts.\n",
    "If you use `make_doc` before the set of document used for training has become fixed, you may get wrong results.\n",
    "Since `make_doc` returns the instance directly, you can use its return value for other manipulations.\n",
    "\n",
    "```python\n",
    "mdl = tp.LDAModel(k=20)\n",
    "# add_doc ...\n",
    "mdl.train(100)\n",
    "doc_inst = mdl.make_doc(unseen_doc) # doc_inst is an instance of the unseen document\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3ae64",
   "metadata": {},
   "source": [
    "## Inference for Unseen Documents\n",
    "\n",
    "If a new document is created by `tomotopy.LDAModel.make_doc`, its topic distribution can be inferred by the model.\n",
    "Inference for unseen document should be performed using `tomotopy.LDAModel.infer` method.\n",
    "\n",
    "```python\n",
    "mdl = tp.LDAModel(k=20)\n",
    "# add_doc ...\n",
    "mdl.train(100)\n",
    "doc_inst = mdl.make_doc(unseen_doc)\n",
    "topic_dist, ll = mdl.infer(doc_inst)\n",
    "print(\"Topic Distribution for Unseen Docs: \", topic_dist)\n",
    "print(\"Log-likelihood of inference: \", ll)\n",
    "```\n",
    "The `infer` method can infer only one instance of `tomotopy.Document` or a `list` of instances of `tomotopy.Document`. \n",
    "See more at `tomotopy.LDAModel.infer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddee47",
   "metadata": {},
   "source": [
    "## Corpus and transform\n",
    "\n",
    "Every topic model in `tomotopy` has its own internal document type.\n",
    "A document can be created and added into suitable for each model through each model's `add_doc` method. \n",
    "However, trying to add the same list of documents to different models becomes quite inconvenient, \n",
    "because `add_doc` should be called for the same list of documents to each different model.\n",
    "Thus, `tomotopy` provides `tomotopy.utils.Corpus` class that holds a list of documents. \n",
    "`tomotopy.utils.Corpus` can be inserted into any model by passing as argument `corpus` to `__init__` or `add_corpus` method of each model. \n",
    "So, inserting `tomotopy.utils.Corpus` just has the same effect to inserting documents the corpus holds.\n",
    "\n",
    "Some topic models requires different data for its documents. \n",
    "For example, `tomotopy.DMRModel` requires argument `metadata` in `str` type, \n",
    "but `tomotopy.PLDAModel` requires argument `labels` in `List[str]` type. \n",
    "Since `tomotopy.utils.Corpus` holds an independent set of documents rather than being tied to a specific topic model, \n",
    "data types required by a topic model may be inconsistent when a corpus is added into that topic model. \n",
    "In this case, miscellaneous data can be transformed to be fitted target topic model using argument `transform`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c2eb4",
   "metadata": {},
   "source": [
    "See more details in the following code:\n",
    "\n",
    "```python\n",
    "from tomotopy import DMRModel\n",
    "from tomotopy.utils import Corpus\n",
    "\n",
    "corpus = Corpus()\n",
    "corpus.add_doc(\"a b c d e\".split(), a_data=1)\n",
    "corpus.add_doc(\"e f g h i\".split(), a_data=2)\n",
    "corpus.add_doc(\"i j k l m\".split(), a_data=3)\n",
    "\n",
    "model = DMRModel(k=10)\n",
    "model.add_corpus(corpus) \n",
    "# You lose `a_data` field in `corpus`, \n",
    "# and `metadata` that `DMRModel` requires is filled with the default value, empty str.\n",
    "\n",
    "assert model.docs[0].metadata == ''\n",
    "assert model.docs[1].metadata == ''\n",
    "assert model.docs[2].metadata == ''\n",
    "\n",
    "def transform_a_data_to_metadata(misc: dict):\n",
    "    return {'metadata': str(misc['a_data'])}\n",
    "# this function transforms `a_data` to `metadata`\n",
    "\n",
    "model = DMRModel(k=10)\n",
    "model.add_corpus(corpus, transform=transform_a_data_to_metadata)\n",
    "# Now docs in `model` has non-default `metadata`, that generated from `a_data` field.\n",
    "\n",
    "assert model.docs[0].metadata == '1'\n",
    "assert model.docs[1].metadata == '2'\n",
    "assert model.docs[2].metadata == '3'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a0e13d",
   "metadata": {},
   "source": [
    "## Parallel Sampling Algorithms\n",
    "\n",
    "Since version 0.5.0, `tomotopy` allows you to choose a parallelism algorithm. \n",
    "The algorithm provided in versions prior to 0.4.2 is `COPY_MERGE`, which is provided for all topic models.\n",
    "The new algorithm `PARTITION`, available since 0.5.0, makes training generally faster and more memory-efficient, but it is available at not all topic models.\n",
    "\n",
    "The following chart shows the speed difference between the two algorithms based on the number of topics.\n",
    "\n",
    "![](../figs/intro_nlp/tomotopy/algo_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943105a3",
   "metadata": {},
   "source": [
    "## Pining Topics using Word Priors\n",
    "\n",
    "Since version 0.6.0, a new method `tomotopy.LDAModel.set_word_prior` has been added. It allows you to control word prior for each topic.\n",
    "For example, we can set the weight of the word 'church' to 1.0 in topic 0, and the weight to 0.1 in the rest of the topics by following codes.\n",
    "This means that the probability that the word 'church' is assigned to topic 0 is 10 times higher than the probability of being assigned to another topic.\n",
    "Therefore, most of 'church' is assigned to topic 0, so topic 0 contains many words related to 'church'. \n",
    "This allows to manipulate some topics to be placed at a specific topic number.\n",
    "\n",
    "```python\n",
    "import tomotopy as tp\n",
    "mdl = tp.LDAModel(k=20)\n",
    "\n",
    "# add documents into `mdl`\n",
    "\n",
    "# setting word prior\n",
    "mdl.set_word_prior('church', [1.0 if k == 0 else 0.1 for k in range(20)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f426a4ee",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338377fb",
   "metadata": {},
   "source": [
    "## Install or upgrade of ekorpkit\n",
    "\n",
    "```{note}\n",
    "Install ekorpkit package first.\n",
    "\n",
    "Set logging level to Warning, if you don't want to see verbose logging.\n",
    "\n",
    "If you run this notebook in Colab, set Hardware accelerator to GPU.\n",
    "```\n",
    "```{toggle}\n",
    "!pip install -U --pre ekorpkit\n",
    "\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8de8f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0.1.39+5.g6b58da9\n",
      "is notebook? True\n",
      "is colab? False\n",
      "environment variables:\n",
      "{'CUDA_DEVICE_ORDER': None,\n",
      " 'CUDA_VISIBLE_DEVICES': None,\n",
      " 'EKORPKIT_CONFIG_DIR': '/workspace/projects/ekorpkit-book/config',\n",
      " 'EKORPKIT_DATA_DIR': None,\n",
      " 'EKORPKIT_LOG_LEVEL': 'WARNING',\n",
      " 'EKORPKIT_PROJECT': 'ekorpkit-book',\n",
      " 'EKORPKIT_WORKSPACE_ROOT': '/workspace',\n",
      " 'KMP_DUPLICATE_LIB_OK': 'TRUE',\n",
      " 'NUM_WORKERS': 230}\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "eKonf.setLogger(\"WARNING\")\n",
    "print(\"version:\", eKonf.__version__)\n",
    "print(\"is notebook?\", eKonf.is_notebook())\n",
    "print(\"is colab?\", eKonf.is_colab())\n",
    "print(\"environment variables:\")\n",
    "eKonf.print(eKonf.env().dict())\n",
    "\n",
    "data_dir = \"../data/topic_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e72c70e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ekorpkit.base:instantiating ekorpkit.pipelines.pipe.pipeline...\n",
      "INFO:ekorpkit.base:instantiating ekorpkit.pipelines.data.Data...\n",
      "INFO:ekorpkit.base:Applying pipe: functools.partial(<function sampling at 0x7efee81b75e0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                               text  split\n",
      "0  251155  Investing com Asian stock markets were broadly...  train\n",
      "1  270611  Solid execution product diversity and strong b...  train\n",
      "2  237917  Chip name Micron Technology Inc NASDAQ MU is h...  train\n",
      "3  406989  June is typically a boring month for gold and ...  train\n",
      "4  231535  A prudent investment decision involves buying ...  train\n"
     ]
    }
   ],
   "source": [
    "corpus_cfg = eKonf.compose('corpus', overrides=[\"project=esgml\"])\n",
    "corpus_cfg.name = 'us_equities_news'\n",
    "corpus_cfg.verbose = False\n",
    "# corpus = eKonf.instantiate(corpus_cfg)\n",
    "cfg = eKonf.compose(\"pipeline\", overrides=[\"project=esgml\"])\n",
    "cfg.data.corpus = corpus_cfg\n",
    "cfg._pipeline_ = [\"sampling\"]\n",
    "cfg.sampling.sample_size_per_group = 0.1\n",
    "cfg.sampling.output_dir = data_dir\n",
    "cfg.sampling.output_file = \"us_equities_news_sampled.parquet\"\n",
    "data = eKonf.instantiate(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc39953",
   "metadata": {},
   "source": [
    "### Load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dcc08f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>251155</td>\n",
       "      <td>Investing com Asian stock markets were broadly...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>270611</td>\n",
       "      <td>Solid execution product diversity and strong b...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>237917</td>\n",
       "      <td>Chip name Micron Technology Inc NASDAQ MU is h...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>406989</td>\n",
       "      <td>June is typically a boring month for gold and ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231535</td>\n",
       "      <td>A prudent investment decision involves buying ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  split\n",
       "0  251155  Investing com Asian stock markets were broadly...  train\n",
       "1  270611  Solid execution product diversity and strong b...  train\n",
       "2  237917  Chip name Micron Technology Inc NASDAQ MU is h...  train\n",
       "3  406989  June is typically a boring month for gold and ...  train\n",
       "4  231535  A prudent investment decision involves buying ...  train"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = eKonf.compose('path')\n",
    "cfg.cache.uri = 'https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/us_equities_news_sampled.zip'\n",
    "data = eKonf.load_data(\"us_equities_news_sampled.parquet\", cfg.cached_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca993c9",
   "metadata": {},
   "source": [
    "### LDA Basics\n",
    "\n",
    "LDA class provides Latent Dirichlet Allocation(LDA) topic model and its implementation is based on following papers:\n",
    "\n",
    "- Blei, D.M., Ng, A.Y., &Jordan, M.I. (2003).Latent dirichlet allocation.Journal of machine Learning research, 3(Jan), 993 - 1022.\n",
    "- Newman, D., Asuncion, A., Smyth, P., &Welling, M. (2009).Distributed algorithms for topic models.Journal of Machine Learning Research, 10(Aug), 1801 - 1828.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f645737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 22098 , Vocab size: 57681 , Num words: 11216361\n",
      "Removed top words: ['the', 'to', 'of', 'and', 'in']\n",
      "Iteration: 0\tLog-likelihood: -8.310562901610025\n",
      "Iteration: 100\tLog-likelihood: -8.266379166823173\n",
      "Iteration: 200\tLog-likelihood: -8.249079422671398\n",
      "Iteration: 300\tLog-likelihood: -8.234771446920387\n",
      "Iteration: 400\tLog-likelihood: -8.229122603357679\n",
      "Iteration: 500\tLog-likelihood: -8.223622848932184\n",
      "Iteration: 600\tLog-likelihood: -8.22001981586702\n",
      "Iteration: 700\tLog-likelihood: -8.220164232504388\n",
      "Iteration: 800\tLog-likelihood: -8.214119530347634\n",
      "Iteration: 900\tLog-likelihood: -8.207522882978928\n",
      "<Basic Info>\n",
      "| LDAModel (current version: 0.12.3)\n",
      "| 22098 docs, 11216361 words\n",
      "| Total Vocabs: 128352, Used Vocabs: 57681\n",
      "| Entropy of words: 7.85406\n",
      "| Entropy of term-weighted words: 7.85406\n",
      "| Removed Vocabs: the to of and in\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 1000, Burn-in steps: 100\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -8.20752\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.ONE\n",
      "| min_cf: 3 (minimum collection frequency of words)\n",
      "| min_df: 0 (minimum document frequency of words)\n",
      "| rm_top: 5 (the number of top words to be removed)\n",
      "| k: 20 (the number of topics between 1 ~ 32767)\n",
      "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| seed: 1495571789 (random seed)\n",
      "| trained in version 0.12.3\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (Dirichlet prior on the per-document topic distributions)\n",
      "|  [0.0458566  0.1386956  0.05798944 0.09173854 0.11660527 0.01820273\n",
      "|   0.0602277  0.11931805 0.13841131 0.01932807 0.21369949 0.04940439\n",
      "|   0.0776258  0.07703414 0.06907631 0.05780613 0.14706294 0.06013599\n",
      "|   0.1130297  0.07722862]\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "|\n",
      "<Topics>\n",
      "| #0 (468266) : earnings a Zacks is quarter\n",
      "| #1 (705522) : a said s its for\n",
      "| #2 (343384) : a is The on for\n",
      "| #3 (416515) : a is for its s\n",
      "| #4 (772445) : S on a 0 1\n",
      "| #5 (124743) : a for is with options\n",
      "| #6 (279768) : 0 at or 1 was\n",
      "| #7 (748744) : a said on that s\n",
      "| #8 (964464) : year quarter million 1 from\n",
      "| #9 (286321) : is or a Zacks for\n",
      "| #10 (1472295) : a that is it for\n",
      "| #11 (355143) : a for with is s\n",
      "| #12 (325693) : Apple a s NASDAQ its\n",
      "| #13 (370363) : s sales its a company\n",
      "| #14 (399654) : a is The dividend has\n",
      "| #15 (483989) : a is has for stock\n",
      "| #16 (1169834) : Zacks a has company s\n",
      "| #17 (353188) : oil a is for The\n",
      "| #18 (501193) : a China S U for\n",
      "| #19 (674837) : a The is for on\n",
      "|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "save_path = eKonf.join_path(data_dir, \"lda_basic.mdl\")\n",
    "mdl = tp.LDAModel(tw=tp.TermWeight.ONE, min_cf=3, rm_top=5, k=20)\n",
    "for n, line in enumerate(data['text']):\n",
    "    ch = line.strip().split()\n",
    "    mdl.add_doc(ch)\n",
    "mdl.burn_in = 100\n",
    "mdl.train(0)\n",
    "print('Num docs:', len(mdl.docs), ', Vocab size:', len(mdl.used_vocabs), ', Num words:', mdl.num_words)\n",
    "print('Removed top words:', mdl.removed_top_words)\n",
    "for i in range(0, 1000, 100):\n",
    "    mdl.train(100)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "\n",
    "mdl.summary()\n",
    "mdl.save(save_path, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef45a6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\n",
      "\t\tearnings\t0.03124929964542389\n",
      "\t\ta\t0.028039269149303436\n",
      "\t\tZacks\t0.02219722606241703\n",
      "\t\tis\t0.020621005445718765\n",
      "\t\tquarter\t0.018191618844866753\n",
      "\t\tfor\t0.017067573964595795\n",
      "\t\tEarnings\t0.015026379376649857\n",
      "\t\tESP\t0.014128423295915127\n",
      "\t\tan\t0.013452290557324886\n",
      "\t\tthat\t0.012859341688454151\n",
      "Topic #1\n",
      "\t\ta\t0.026198897510766983\n",
      "\t\tsaid\t0.025242939591407776\n",
      "\t\ts\t0.019541189074516296\n",
      "\t\tits\t0.015248588286340237\n",
      "\t\tfor\t0.011897781863808632\n",
      "\t\ton\t0.011563551612198353\n",
      "\t\tit\t0.010953155346214771\n",
      "\t\tpercent\t0.00960773415863514\n",
      "\t\tas\t0.008685766719281673\n",
      "\t\tcompany\t0.008452088572084904\n",
      "Topic #2\n",
      "\t\ta\t0.030532576143741608\n",
      "\t\tis\t0.016013480722904205\n",
      "\t\tThe\t0.015097678638994694\n",
      "\t\ton\t0.012399696744978428\n",
      "\t\tfor\t0.011318178847432137\n",
      "\t\tat\t0.010512854903936386\n",
      "\t\tit\t0.008428314700722694\n",
      "\t\tday\t0.007102582603693008\n",
      "\t\tTesla\t0.007090953178703785\n",
      "\t\tstock\t0.006995012052357197\n",
      "Topic #3\n",
      "\t\ta\t0.01793372444808483\n",
      "\t\tis\t0.016452036798000336\n",
      "\t\tfor\t0.012529638595879078\n",
      "\t\tits\t0.012028549797832966\n",
      "\t\ts\t0.011740843765437603\n",
      "\t\twith\t0.010846556164324284\n",
      "\t\ttechnology\t0.009976244531571865\n",
      "\t\tNASDAQ\t0.009379253722727299\n",
      "\t\twill\t0.009110728278756142\n",
      "\t\tThe\t0.008410641923546791\n",
      "Topic #4\n",
      "\t\tS\t0.019718732684850693\n",
      "\t\ton\t0.019661812111735344\n",
      "\t\ta\t0.01943672075867653\n",
      "\t\t0\t0.017082324251532555\n",
      "\t\t1\t0.016551939770579338\n",
      "\t\tpercent\t0.014398054219782352\n",
      "\t\ts\t0.013101842254400253\n",
      "\t\tThe\t0.012660715728998184\n",
      "\t\tU\t0.01251065544784069\n",
      "\t\tafter\t0.010465435683727264\n",
      "Topic #5\n",
      "\t\ta\t0.028191950172185898\n",
      "\t\tfor\t0.021664651110768318\n",
      "\t\tis\t0.014714432880282402\n",
      "\t\twith\t0.011219375766813755\n",
      "\t\toptions\t0.010716661810874939\n",
      "\t\this\t0.008673887699842453\n",
      "\t\tthat\t0.008546214550733566\n",
      "\t\tvolatility\t0.007253522053360939\n",
      "\t\ton\t0.007054032292217016\n",
      "\t\tas\t0.006343849468976259\n",
      "Topic #6\n",
      "\t\t0\t0.03876658156514168\n",
      "\t\tat\t0.02934960648417473\n",
      "\t\tor\t0.02234751358628273\n",
      "\t\t1\t0.021844562143087387\n",
      "\t\twas\t0.01921565644443035\n",
      "\t\twhich\t0.015937551856040955\n",
      "\t\tNASDAQ\t0.014646284282207489\n",
      "\t\tThe\t0.01412549801170826\n",
      "\t\ton\t0.014075559563934803\n",
      "\t\tInc\t0.013850836083292961\n",
      "Topic #7\n",
      "\t\ta\t0.026682842522859573\n",
      "\t\tsaid\t0.01741178147494793\n",
      "\t\ton\t0.015312546864151955\n",
      "\t\tthat\t0.013975335285067558\n",
      "\t\ts\t0.012738214805722237\n",
      "\t\tThe\t0.011103400960564613\n",
      "\t\tfor\t0.010098491795361042\n",
      "\t\twas\t0.00858111772686243\n",
      "\t\tby\t0.008440990000963211\n",
      "\t\tU\t0.008267499506473541\n",
      "Topic #8\n",
      "\t\tyear\t0.03777458146214485\n",
      "\t\tquarter\t0.02896769717335701\n",
      "\t\tmillion\t0.028481706976890564\n",
      "\t\t1\t0.01698167622089386\n",
      "\t\tfrom\t0.015365163795650005\n",
      "\t\t2\t0.012678230181336403\n",
      "\t\tbillion\t0.012417101301252842\n",
      "\t\ta\t0.012012973427772522\n",
      "\t\t3\t0.011970488354563713\n",
      "\t\tover\t0.011747700162231922\n",
      "Topic #9\n",
      "\t\tis\t0.026326481252908707\n",
      "\t\tor\t0.02442336454987526\n",
      "\t\ta\t0.022607386112213135\n",
      "\t\tZacks\t0.020143095403909683\n",
      "\t\tfor\t0.015291192568838596\n",
      "\t\tstocks\t0.014799728989601135\n",
      "\t\tthat\t0.013360192067921162\n",
      "\t\tare\t0.012704906985163689\n",
      "\t\tinvestment\t0.011826545000076294\n",
      "\t\tResearch\t0.010195302776992321\n",
      "Topic #10\n",
      "\t\ta\t0.026791885495185852\n",
      "\t\tthat\t0.023886676877737045\n",
      "\t\tis\t0.020946161821484566\n",
      "\t\tit\t0.012942749075591564\n",
      "\t\tfor\t0.011327537707984447\n",
      "\t\twe\t0.011231127195060253\n",
      "\t\ts\t0.011211437173187733\n",
      "\t\tare\t0.011083795689046383\n",
      "\t\tI\t0.010176045820116997\n",
      "\t\tbe\t0.009823672473430634\n",
      "Topic #11\n",
      "\t\ta\t0.020864764228463173\n",
      "\t\tfor\t0.020766371861100197\n",
      "\t\twith\t0.015346375294029713\n",
      "\t\tis\t0.013524716719985008\n",
      "\t\ts\t0.011478163301944733\n",
      "\t\tThe\t0.010997447185218334\n",
      "\t\tfrom\t0.007882636971771717\n",
      "\t\ton\t0.007497502025216818\n",
      "\t\tthat\t0.006727233063429594\n",
      "\t\tBoeing\t0.006626029498875141\n",
      "Topic #12\n",
      "\t\tApple\t0.02030530944466591\n",
      "\t\ta\t0.01906707137823105\n",
      "\t\ts\t0.017792053520679474\n",
      "\t\tNASDAQ\t0.015772251412272453\n",
      "\t\tits\t0.01572321355342865\n",
      "\t\tis\t0.013062838464975357\n",
      "\t\tfor\t0.012008496560156345\n",
      "\t\ton\t0.01025841198861599\n",
      "\t\tcompany\t0.009939656592905521\n",
      "\t\tThe\t0.009550408460199833\n",
      "Topic #13\n",
      "\t\ts\t0.021421290934085846\n",
      "\t\tsales\t0.01631803810596466\n",
      "\t\tits\t0.01597566343843937\n",
      "\t\ta\t0.015387967228889465\n",
      "\t\tcompany\t0.014013081789016724\n",
      "\t\ton\t0.0105489082634449\n",
      "\t\tis\t0.010017824359238148\n",
      "\t\thas\t0.0078099193051457405\n",
      "\t\tfor\t0.007408237084746361\n",
      "\t\tThe\t0.0072761401534080505\n",
      "Topic #14\n",
      "\t\ta\t0.023136673495173454\n",
      "\t\tis\t0.012710190378129482\n",
      "\t\tThe\t0.012222971767187119\n",
      "\t\tdividend\t0.010886242613196373\n",
      "\t\thas\t0.010031736455857754\n",
      "\t\tfund\t0.008607557974755764\n",
      "\t\tas\t0.008360200561583042\n",
      "\t\twith\t0.008185301907360554\n",
      "\t\tits\t0.007902964949607849\n",
      "\t\tETF\t0.007413247134536505\n",
      "Topic #15\n",
      "\t\ta\t0.0332772321999073\n",
      "\t\tis\t0.022882360965013504\n",
      "\t\thas\t0.016356931999325752\n",
      "\t\tfor\t0.01582656055688858\n",
      "\t\tstock\t0.015669718384742737\n",
      "\t\tthis\t0.012685603462159634\n",
      "\t\tthat\t0.012254289351403713\n",
      "\t\ts\t0.012019027024507523\n",
      "\t\tZacks\t0.011170845478773117\n",
      "\t\tearnings\t0.010324726812541485\n",
      "Topic #16\n",
      "\t\tZacks\t0.022936400026082993\n",
      "\t\ta\t0.020220259204506874\n",
      "\t\thas\t0.016317356377840042\n",
      "\t\tcompany\t0.016230208799242973\n",
      "\t\ts\t0.013876333832740784\n",
      "\t\tThe\t0.013500397093594074\n",
      "\t\tfor\t0.01269469689577818\n",
      "\t\tRank\t0.01241616252809763\n",
      "\t\tis\t0.010008460842072964\n",
      "\t\tgrowth\t0.009430031292140484\n",
      "Topic #17\n",
      "\t\toil\t0.019801318645477295\n",
      "\t\ta\t0.018382295966148376\n",
      "\t\tis\t0.01412523165345192\n",
      "\t\tfor\t0.012624233961105347\n",
      "\t\tThe\t0.011699890717864037\n",
      "\t\tproduction\t0.010812296532094479\n",
      "\t\tgas\t0.010303483344614506\n",
      "\t\tEnergy\t0.010145186446607113\n",
      "\t\tenergy\t0.008825100027024746\n",
      "\t\tfrom\t0.008525466546416283\n",
      "Topic #18\n",
      "\t\ta\t0.019231945276260376\n",
      "\t\tChina\t0.018903110176324844\n",
      "\t\tS\t0.015108540654182434\n",
      "\t\tU\t0.013544078916311264\n",
      "\t\tfor\t0.011517252773046494\n",
      "\t\ts\t0.01038326695561409\n",
      "\t\tis\t0.009811291471123695\n",
      "\t\ton\t0.00963989831507206\n",
      "\t\tas\t0.00934892799705267\n",
      "\t\tThe\t0.008886564522981644\n",
      "Topic #19\n",
      "\t\ta\t0.02060071751475334\n",
      "\t\tThe\t0.01461031660437584\n",
      "\t\tis\t0.01301425788551569\n",
      "\t\tfor\t0.011943803168833256\n",
      "\t\ton\t0.011200555600225925\n",
      "\t\ts\t0.010091605596244335\n",
      "\t\tas\t0.009640030562877655\n",
      "\t\t1\t0.009573404677212238\n",
      "\t\tat\t0.008796103298664093\n",
      "\t\tthat\t0.008279383182525635\n"
     ]
    }
   ],
   "source": [
    "for k in range(mdl.k):\n",
    "    print('Topic #{}'.format(k))\n",
    "    for word, prob in mdl.get_topic_words(k):\n",
    "        print('\\t', word, prob, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e566ca",
   "metadata": {},
   "source": [
    "### LDA Visualization\n",
    "\n",
    "This example shows how to perform a Latent Dirichlet Allocation using tomotopy and visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e922a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pyLDAvis\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28ffcaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = nltk.PorterStemmer().stem\n",
    "english_stops = set(porter_stemmer(w) for w in stopwords.words('english'))\n",
    "pat = re.compile('^[a-z]{2,}$')\n",
    "corpus = tp.utils.Corpus(\n",
    "    tokenizer=tp.utils.SimpleTokenizer(porter_stemmer), \n",
    "    stopwords=lambda x: x in english_stops or not pat.match(x)\n",
    ")\n",
    "\n",
    "corpus.process(d.lower() for d in data['text'])\n",
    "# save preprocessed corpus for reuse\n",
    "save_path = eKonf.join_path(data_dir, \"preprocessed_corpus.cps\")\n",
    "corpus.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2aaeb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs:22098, Num Vocabs:20231, Total Words:5861553\n",
      "Removed Top words:  year compani stock earn zack quarter market share expect million report said nyse estim growth price billion rank also trade nasdaq revenu invest investor new inc last rate industri like sale buy per increas month time current one consensu percent\n"
     ]
    }
   ],
   "source": [
    "mdl = tp.LDAModel(min_df=5, rm_top=40, k=30, corpus=corpus)\n",
    "mdl.train(0)\n",
    "\n",
    "print('Num docs:{}, Num Vocabs:{}, Total Words:{}'.format(\n",
    "    len(mdl.docs), len(mdl.used_vocabs), mdl.num_words\n",
    "))\n",
    "print('Removed Top words: ', *mdl.removed_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b1cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0000, LL per word: -11.76\n",
      "Iteration: 0020, LL per word: -8.53\n",
      "Iteration: 0040, LL per word: -8.273\n",
      "Iteration: 0060, LL per word: -8.196\n",
      "Iteration: 0080, LL per word: -8.154\n",
      "Iteration: 0100, LL per word: -8.133\n",
      "Iteration: 0120, LL per word: -8.117\n",
      "Iteration: 0140, LL per word: -8.105\n",
      "Iteration: 0160, LL per word: -8.097\n"
     ]
    }
   ],
   "source": [
    "# Let's train the model\n",
    "for i in range(0, 1000, 100):\n",
    "    print('Iteration: {:04}, LL per word: {:.4}'.format(i, mdl.ll_per_word))\n",
    "    mdl.train(100)\n",
    "print('Iteration: {:04}, LL per word: {:.4}'.format(1000, mdl.ll_per_word))\n",
    "\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f479ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_dists = np.stack([mdl.get_topic_word_dist(k) for k in range(mdl.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])\n",
    "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "doc_lengths = np.array([len(doc.words) for doc in mdl.docs])\n",
    "vocab = list(mdl.used_vocabs)\n",
    "term_frequency = mdl.used_vocab_freq\n",
    "\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency,\n",
    "    start_index=0, # tomotopy starts topic ids with 0, pyLDAvis with 1\n",
    "    sort_topics=False # IMPORTANT: otherwise the topic_ids between pyLDAvis and tomotopy are not matching!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8647f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(prepared_data)\n",
    "pyLDAvis.save_html(prepared_data, eKonf.join_path(data_dir, \"lda_basic.html\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
